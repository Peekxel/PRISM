{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a30c09d-2469-48af-96d5-7c0cae4d0517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env Rscript\n",
    "# Fixed GSE236519 Harmonization Script for Jupyter Environment\n",
    "# This script downloads, harmonizes, and converts the dataset to h5ad format\n",
    "\n",
    "# Load required packages\n",
    "suppressPackageStartupMessages({\n",
    "  library(Seurat)\n",
    "  library(tidyverse)\n",
    "  library(R.utils)\n",
    "  library(Matrix)\n",
    "})\n",
    "\n",
    "# Define directories and files\n",
    "accession <- \"GSE236519\"\n",
    "\n",
    "# Fix for Jupyter environment - use a simple, clean path instead of kernel path\n",
    "# Check if we're in a Jupyter kernel path and set data directory accordingly\n",
    "current_dir <- getwd()\n",
    "if (grepl(\"kernel-\", current_dir) || grepl(\"\\\\.json\", current_dir)) {\n",
    "  # We're likely in a Jupyter environment with a problematic path\n",
    "  # Use a simpler path that doesn't have special characters or nested .json\n",
    "  root_dir <- \"/tmp\"\n",
    "  message(\"Detected Jupyter environment. Using /tmp directory for data storage.\")\n",
    "} else {\n",
    "  root_dir <- current_dir\n",
    "}\n",
    "\n",
    "dataset_dir <- file.path(root_dir, accession)\n",
    "output_rds <- file.path(dataset_dir, paste0(accession, \"_harmonized.rds\"))\n",
    "\n",
    "# Create the dataset directory if it doesn't exist\n",
    "dir.create(dataset_dir, showWarnings = FALSE, recursive = TRUE)\n",
    "message(\"Working with dataset directory: \", dataset_dir)\n",
    "\n",
    "# File URLs and their local paths\n",
    "file_urls <- c(\n",
    "  \"https://ftp.ncbi.nlm.nih.gov/geo/series/GSE236nnn/GSE236519/suppl/GSE236519_arrayed_CRISPRi_hSyn.rds.gz\",\n",
    "  \"https://ftp.ncbi.nlm.nih.gov/geo/series/GSE236nnn/GSE236519/suppl/GSE236519_arrayed_screen_hSyn.rds.gz\",\n",
    "  \"https://ftp.ncbi.nlm.nih.gov/geo/series/GSE236nnn/GSE236519/suppl/GSE236519_lgdel_model.rds.gz\",\n",
    "  \"https://ftp.ncbi.nlm.nih.gov/geo/series/GSE236nnn/GSE236519/suppl/GSE236519_pooled_screen_CBh.rds.gz\"\n",
    ")\n",
    "\n",
    "# Function to safely download and extract files\n",
    "download_and_extract <- function(file_url, dest_dir) {\n",
    "  file_name <- basename(file_url)\n",
    "  download_path <- file.path(dest_dir, file_name)\n",
    "  extracted_path <- sub(\"\\\\.gz$\", \"\", download_path)\n",
    "\n",
    "  # Create a temporary path for download if needed\n",
    "  temp_download <- FALSE\n",
    "  if (!file.exists(extracted_path)) {\n",
    "    message(\"Downloading \", file_name, \"...\")\n",
    "\n",
    "    # First try direct download\n",
    "    download_success <- tryCatch({\n",
    "      download.file(file_url, download_path, mode = \"wb\", quiet = FALSE)\n",
    "      TRUE\n",
    "    }, error = function(e) {\n",
    "      message(\"Direct download failed: \", e$message)\n",
    "      FALSE\n",
    "    })\n",
    "\n",
    "    # If direct download fails, try system commands as fallback\n",
    "    if (!download_success) {\n",
    "      message(\"Trying system commands for download...\")\n",
    "\n",
    "      # Try wget first\n",
    "      if (Sys.which(\"wget\") != \"\") {\n",
    "        message(\"Using wget...\")\n",
    "        download_success <- system(paste(\"wget -q\", shQuote(file_url), \"-O\", shQuote(download_path))) == 0\n",
    "      }\n",
    "\n",
    "      # If wget fails, try curl\n",
    "      if (!download_success && Sys.which(\"curl\") != \"\") {\n",
    "        message(\"Using curl...\")\n",
    "        download_success <- system(paste(\"curl -L\", shQuote(file_url), \"-o\", shQuote(download_path))) == 0\n",
    "      }\n",
    "    }\n",
    "\n",
    "    # Check if download was successful\n",
    "    if (!download_success || !file.exists(download_path) || file.info(download_path)$size == 0) {\n",
    "      message(\"Download failed for \", file_name)\n",
    "      return(NULL)\n",
    "    }\n",
    "\n",
    "    message(\"Download complete.\")\n",
    "\n",
    "    # Extract the file\n",
    "    message(\"Extracting \", file_name, \"...\")\n",
    "\n",
    "    # Try multiple extraction methods\n",
    "    extract_success <- tryCatch({\n",
    "      R.utils::gunzip(download_path, extracted_path, remove = FALSE)\n",
    "      TRUE\n",
    "    }, error = function(e) {\n",
    "      message(\"R.utils::gunzip failed: \", e$message)\n",
    "      FALSE\n",
    "    })\n",
    "\n",
    "    if (!extract_success) {\n",
    "      message(\"Trying system gunzip...\")\n",
    "      extract_success <- system(paste(\"gunzip -c\", shQuote(download_path), \">\", shQuote(extracted_path))) == 0\n",
    "    }\n",
    "\n",
    "    if (!extract_success || !file.exists(extracted_path) || file.info(extracted_path)$size == 0) {\n",
    "      message(\"Extraction failed for \", file_name)\n",
    "      return(NULL)\n",
    "    }\n",
    "\n",
    "    message(\"Extraction complete.\")\n",
    "  } else {\n",
    "    message(\"Found existing extracted file: \", extracted_path)\n",
    "  }\n",
    "\n",
    "  return(extracted_path)\n",
    "}\n",
    "\n",
    "# Download and extract all files\n",
    "all_files <- sapply(file_urls, download_and_extract, dest_dir = dataset_dir)\n",
    "all_files <- all_files[!is.null(all_files)]\n",
    "\n",
    "# Function to harmonize metadata for a Seurat object\n",
    "harmonize_metadata <- function(seu_obj, dataset_name) {\n",
    "  message(\"Harmonizing metadata for: \", dataset_name)\n",
    "\n",
    "  # Create a base metadata dataframe\n",
    "  meta_data <- seu_obj@meta.data\n",
    "  meta_cols <- colnames(meta_data)\n",
    "\n",
    "  # Initialize harmonized metadata with cell barcodes\n",
    "  harmonized_meta <- data.frame(\n",
    "    cell_barcode = rownames(meta_data),\n",
    "    row.names = rownames(meta_data)\n",
    "  )\n",
    "\n",
    "  # Standard fields required for harmonization\n",
    "  harmonized_meta$organism <- \"Mus musculus\"\n",
    "\n",
    "  # Determine cell type based on the specific dataset\n",
    "  if (\"cell_types\" %in% meta_cols) {\n",
    "    harmonized_meta$cell_type <- meta_data$cell_types\n",
    "  } else if (\"cell_types_broad\" %in% meta_cols) {\n",
    "    harmonized_meta$cell_type <- meta_data$cell_types_broad\n",
    "  } else {\n",
    "    # Default cell type for this dataset (from the GEO description)\n",
    "    harmonized_meta$cell_type <- \"Prefrontal cortex neurons\"\n",
    "  }\n",
    "\n",
    "  # Set CRISPR type based on the dataset name\n",
    "  if (grepl(\"CRISPRi\", dataset_name)) {\n",
    "    harmonized_meta$crispr_type <- \"CRISPRi\"\n",
    "  } else {\n",
    "    harmonized_meta$crispr_type <- \"CRISPR KO\"\n",
    "  }\n",
    "\n",
    "  # Set cancer type (non-cancer in this case)\n",
    "  harmonized_meta$cancer_type <- \"Non-Cancer\"\n",
    "\n",
    "  # Determine condition (control vs test)\n",
    "  # For the pooled screen dataset with gRNA information\n",
    "  if (\"gRNAs\" %in% meta_cols) {\n",
    "    perturbation_col <- \"gRNAs\"\n",
    "    perturbations <- meta_data[[perturbation_col]]\n",
    "\n",
    "    # Identify controls using common naming patterns\n",
    "    is_control <- grepl(\"control|ctrl|non-target|nt|neg|scramble\",\n",
    "                        perturbations, ignore.case = TRUE)\n",
    "\n",
    "    harmonized_meta$condition <- \"test\"\n",
    "    harmonized_meta$condition[is_control] <- \"control\"\n",
    "\n",
    "    # Set perturbation names\n",
    "    harmonized_meta$perturbation_name <- perturbations\n",
    "    harmonized_meta$perturbation_name[is_control] <- \"non-targeting\"\n",
    "\n",
    "  } else if (\"per_gene\" %in% meta_cols) {\n",
    "    # For the arrayed screens that use per_gene for perturbation info\n",
    "    perturbation_col <- \"per_gene\"\n",
    "    perturbations <- meta_data[[perturbation_col]]\n",
    "\n",
    "    # Identify controls for this specific dataset\n",
    "    is_control <- grepl(\"Safe|Control|Ctrl\", perturbations, ignore.case = TRUE)\n",
    "\n",
    "    harmonized_meta$condition <- \"test\"\n",
    "    harmonized_meta$condition[is_control] <- \"control\"\n",
    "\n",
    "    # Clean up perturbation names\n",
    "    harmonized_meta$perturbation_name <- perturbations\n",
    "    harmonized_meta$perturbation_name[is_control] <- \"non-targeting\"\n",
    "\n",
    "  } else if (\"condition\" %in% meta_cols) {\n",
    "    # For the lgdel_model that uses condition\n",
    "    perturbation_col <- \"condition\"\n",
    "    perturbations <- meta_data[[perturbation_col]]\n",
    "\n",
    "    # This dataset compares WT (control) vs. deletion model\n",
    "    is_control <- grepl(\"wt|WT|control\", perturbations, ignore.case = TRUE)\n",
    "\n",
    "    harmonized_meta$condition <- \"test\"\n",
    "    harmonized_meta$condition[is_control] <- \"control\"\n",
    "\n",
    "    # Set perturbation names\n",
    "    harmonized_meta$perturbation_name <- ifelse(is_control, \"non-targeting\", \"22q11.2-deletion\")\n",
    "\n",
    "  } else {\n",
    "    # Default if no perturbation information is found\n",
    "    harmonized_meta$condition <- \"unknown\"\n",
    "    harmonized_meta$perturbation_name <- \"unknown\"\n",
    "  }\n",
    "\n",
    "  # Preserve original quality metrics\n",
    "  quality_cols <- c(\"nCount_RNA\", \"nFeature_RNA\", \"percent.mt\", \"orig.ident\")\n",
    "  for (col in quality_cols) {\n",
    "    if (col %in% meta_cols) {\n",
    "      harmonized_meta[[col]] <- meta_data[[col]]\n",
    "    }\n",
    "  }\n",
    "\n",
    "  # Preserve all other metadata with orig_ prefix\n",
    "  for (col in meta_cols) {\n",
    "    if (!col %in% colnames(harmonized_meta) &&\n",
    "        !col %in% c(quality_cols, perturbation_col)) {\n",
    "      new_col <- paste0(\"orig_\", col)\n",
    "      harmonized_meta[[new_col]] <- meta_data[[col]]\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return(harmonized_meta)\n",
    "}\n",
    "\n",
    "# Function to merge all datasets into a single Seurat object\n",
    "merge_and_harmonize <- function(file_paths) {\n",
    "  if (length(file_paths) == 0) {\n",
    "    message(\"No files found for merging.\")\n",
    "    return(NULL)\n",
    "  }\n",
    "\n",
    "  # Create a list to hold Seurat objects\n",
    "  seurat_list <- list()\n",
    "  dataset_names <- c()\n",
    "\n",
    "  # Process each file\n",
    "  for (i in seq_along(file_paths)) {\n",
    "    file_path <- file_paths[i]\n",
    "    dataset_name <- sub(\"\\\\.rds$\", \"\", basename(file_path))\n",
    "    dataset_names <- c(dataset_names, dataset_name)\n",
    "\n",
    "    message(\"Processing dataset: \", dataset_name)\n",
    "\n",
    "    # Load the Seurat object\n",
    "    tryCatch({\n",
    "      seu_obj <- readRDS(file_path)\n",
    "\n",
    "      # Harmonize metadata\n",
    "      harmonized_meta <- harmonize_metadata(seu_obj, dataset_name)\n",
    "\n",
    "      # Add dataset identifier\n",
    "      harmonized_meta$dataset <- dataset_name\n",
    "\n",
    "      # Create a new Seurat object with raw counts and harmonized metadata\n",
    "      harmonized_obj <- CreateSeuratObject(\n",
    "        counts = seu_obj@assays$RNA@counts,\n",
    "        meta.data = harmonized_meta,\n",
    "        project = accession\n",
    "      )\n",
    "\n",
    "      # Store in list\n",
    "      seurat_list[[i]] <- harmonized_obj\n",
    "\n",
    "      # Report statistics\n",
    "      message(\"Dataset: \", dataset_name)\n",
    "      message(\"Cells: \", ncol(harmonized_obj))\n",
    "      message(\"Genes: \", nrow(harmonized_obj))\n",
    "      message(\"\")\n",
    "    }, error = function(e) {\n",
    "      message(\"Error processing \", dataset_name, \": \", e$message)\n",
    "    })\n",
    "  }\n",
    "\n",
    "  if (length(seurat_list) == 0) {\n",
    "    message(\"No datasets were successfully processed.\")\n",
    "    return(NULL)\n",
    "  }\n",
    "\n",
    "  # Merge all Seurat objects\n",
    "  message(\"Merging all datasets...\")\n",
    "  if (length(seurat_list) > 1) {\n",
    "    merged_obj <- merge(seurat_list[[1]], seurat_list[-1], add.cell.ids = dataset_names)\n",
    "  } else {\n",
    "    merged_obj <- seurat_list[[1]]\n",
    "  }\n",
    "\n",
    "  message(\"Merged object created with \", ncol(merged_obj), \" cells and \", nrow(merged_obj), \" genes.\")\n",
    "\n",
    "  return(merged_obj)\n",
    "}\n",
    "\n",
    "# Function to write conversion script\n",
    "write_conversion_script <- function(dataset_dir, input_rds) {\n",
    "  python_script <- file.path(dataset_dir, \"convert_to_h5ad.py\")\n",
    "  output_h5ad <- sub(\"\\\\.rds$\", \".h5ad\", input_rds)\n",
    "\n",
    "  script_content <- paste(\n",
    "    \"#!/usr/bin/env python3\",\n",
    "    \"# Convert Seurat RDS to h5ad format for GSE236519\",\n",
    "    \"\",\n",
    "    \"import os\",\n",
    "    \"import sys\",\n",
    "    \"import numpy as np\",\n",
    "    \"import pandas as pd\",\n",
    "    \"import scipy.sparse\",\n",
    "    \"import scipy.io\",\n",
    "    \"import anndata\",\n",
    "    \"import subprocess\",\n",
    "    \"\",\n",
    "    \"# First install required packages if needed\",\n",
    "    \"def install_package(package):\",\n",
    "    \"    try:\",\n",
    "    \"        __import__(package)\",\n",
    "    \"    except ImportError:\",\n",
    "    \"        print(f'Installing {package}...')\",\n",
    "    \"        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\",\n",
    "    \"\",\n",
    "    \"install_package('rpy2')\",\n",
    "    \"install_package('anndata')\",\n",
    "    \"\",\n",
    "    \"# Import rpy2 modules\",\n",
    "    \"from rpy2.robjects import pandas2ri, r, numpy2ri\",\n",
    "    \"from rpy2.robjects.conversion import localconverter\",\n",
    "    \"import rpy2.robjects as ro\",\n",
    "    \"\",\n",
    "    \"# Initialize R converters\",\n",
    "    \"pandas2ri.activate()\",\n",
    "    \"numpy2ri.activate()\",\n",
    "    \"\",\n",
    "    \"# File paths\",\n",
    "    paste0(\"input_rds = '\", input_rds, \"'\"),\n",
    "    paste0(\"output_h5ad = '\", output_h5ad, \"'\"),\n",
    "    \"dataset_dir = os.path.dirname(input_rds)\",\n",
    "    \"\",\n",
    "    \"print(f'Converting {input_rds} to {output_h5ad}')\",\n",
    "    \"\",\n",
    "    \"# Load required R packages\",\n",
    "    \"print('Loading R packages...')\",\n",
    "    \"r(\\\"library(Seurat)\\\")\",\n",
    "    \"r(\\\"library(Matrix)\\\")\",\n",
    "    \"\",\n",
    "    \"# Load the Seurat object\",\n",
    "    \"print('Loading Seurat object...')\",\n",
    "    paste0(\"r(f\\\"seu <- readRDS('\", input_rds, \"')\\\")\"),\n",
    "    \"\",\n",
    "    \"# Use intermediate file approach to avoid conversion issues\",\n",
    "    \"print('Converting via temporary sparse matrix file...')\",\n",
    "    \"temp_mtx = os.path.join(dataset_dir, 'temp_counts.mtx')\",\n",
    "    \"r(f\\\"\\\"\\\"\",\n",
    "    \"# Get counts matrix\",\n",
    "    \"counts <- GetAssayData(seu, slot='counts', assay='RNA')\",\n",
    "    \"# Write sparse matrix to file\",\n",
    "    \"writeMM(counts, '{temp_mtx}')\",\n",
    "    \"# Save gene and cell names\",\n",
    "    \"write.table(rownames(counts), '{dataset_dir}/temp_genes.txt', row.names=FALSE, col.names=FALSE, quote=FALSE)\",\n",
    "    \"write.table(colnames(counts), '{dataset_dir}/temp_cells.txt', row.names=FALSE, col.names=FALSE, quote=FALSE)\",\n",
    "    \"\\\"\\\"\\\")\",\n",
    "    \"\",\n",
    "    \"# Read the sparse matrix\",\n",
    "    \"print('Reading sparse matrix from file...')\",\n",
    "    \"counts_sparse = scipy.io.mmread(temp_mtx).tocsr()\",\n",
    "    \"\",\n",
    "    \"# Read gene and cell names\",\n",
    "    \"genes = pd.read_csv(f\\\"{dataset_dir}/temp_genes.txt\\\", header=None)[0].values\",\n",
    "    \"cells = pd.read_csv(f\\\"{dataset_dir}/temp_cells.txt\\\", header=None)[0].values\",\n",
    "    \"\",\n",
    "    \"# Get metadata\",\n",
    "    \"print('Extracting metadata...')\",\n",
    "    \"with localconverter(ro.default_converter + pandas2ri.converter):\",\n",
    "    \"    meta_df = r('seu@meta.data')\",\n",
    "    \"    meta_df = pd.DataFrame(meta_df)\",\n",
    "    \"\",\n",
    "    \"# Create AnnData object - note we transpose the matrix to get cells x genes\",\n",
    "    \"print('Creating AnnData object...')\",\n",
    "    \"adata = anndata.AnnData(\",\n",
    "    \"    X=counts_sparse.T,  # Transpose to cells x genes\",\n",
    "    \"    obs=meta_df,\",\n",
    "    \"    var=pd.DataFrame(index=genes)\",\n",
    "    \")\",\n",
    "    \"\",\n",
    "    \"# Ensure required fields are present\",\n",
    "    \"required_fields = ['organism', 'cell_type', 'crispr_type', 'cancer_type', 'condition', 'perturbation_name']\",\n",
    "    \"for field in required_fields:\",\n",
    "    \"    if field not in adata.obs_keys():\",\n",
    "    \"        print(f'WARNING: Required field {field} missing from metadata')\",\n",
    "    \"\",\n",
    "    \"# Save as h5ad\",\n",
    "    \"print('Saving h5ad file...')\",\n",
    "    \"adata.write(output_h5ad)\",\n",
    "    \"\",\n",
    "    \"# Clean up temporary files\",\n",
    "    \"print('Cleaning up temporary files...')\",\n",
    "    \"os.remove(temp_mtx)\",\n",
    "    \"os.remove(f\\\"{dataset_dir}/temp_genes.txt\\\")\",\n",
    "    \"os.remove(f\\\"{dataset_dir}/temp_cells.txt\\\")\",\n",
    "    \"\",\n",
    "    \"print(f'Conversion complete! File saved to {output_h5ad}')\",\n",
    "    sep = \"\\n\"\n",
    "  )\n",
    "\n",
    "  # Write the Python script to file\n",
    "  writeLines(script_content, python_script)\n",
    "\n",
    "  # Make the script executable\n",
    "  Sys.chmod(python_script, mode = \"0755\")\n",
    "\n",
    "  message(\"Python conversion script created: \", python_script)\n",
    "  return(python_script)\n",
    "}\n",
    "\n",
    "# Main execution flow\n",
    "message(\"=== Starting GSE236519 harmonization process ===\")\n",
    "\n",
    "# Merge and harmonize datasets\n",
    "harmonized_seurat <- merge_and_harmonize(all_files)\n",
    "\n",
    "if (!is.null(harmonized_seurat)) {\n",
    "  # Save as RDS\n",
    "  message(\"Saving harmonized Seurat object to RDS...\")\n",
    "  saveRDS(harmonized_seurat, output_rds)\n",
    "  message(\"Saved to: \", output_rds)\n",
    "\n",
    "  # Create Python conversion script\n",
    "  python_script <- write_conversion_script(dataset_dir, output_rds)\n",
    "\n",
    "  # Print instructions for conversion\n",
    "  message(\"\\n=== Conversion Instructions ===\")\n",
    "  message(\"To convert the RDS file to h5ad format, run the following command:\")\n",
    "  message(\"python \", python_script, \"\\n\")\n",
    "\n",
    "  # Print dataset summary\n",
    "  message(\"\\n=== Dataset Summary ===\")\n",
    "  message(\"Accession: \", accession)\n",
    "  message(\"Number of cells: \", ncol(harmonized_seurat))\n",
    "  message(\"Number of genes: \", nrow(harmonized_seurat))\n",
    "} else {\n",
    "  message(\"Failed to create harmonized Seurat object.\")\n",
    "}\n",
    "\n",
    "message(\"\\nHarmonization process complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "703ca59a-be42-409e-b477-a3a0fbea9dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Data Type Fixed Harmonized Converter for GSE236519\n",
    "# This script handles mixed data types in the metadata\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "import scipy.io\n",
    "import subprocess\n",
    "import glob\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# First install required packages if needed\n",
    "def install_package(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        print(f'Installing {package}...')\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "\n",
    "install_package('rpy2')\n",
    "install_package('anndata')\n",
    "install_package('scanpy')\n",
    "\n",
    "# Import packages after installation\n",
    "from rpy2.robjects import pandas2ri, r, numpy2ri\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "import rpy2.robjects as ro\n",
    "import anndata\n",
    "import scanpy as sc\n",
    "\n",
    "# Initialize R converters\n",
    "pandas2ri.activate()\n",
    "numpy2ri.activate()\n",
    "\n",
    "# File paths\n",
    "data_dir = '/content/GSE236519'\n",
    "harmonized_rds = os.path.join(data_dir, 'GSE236519_harmonized.rds')\n",
    "output_h5ad = os.path.join(data_dir, 'GSE236519_harmonized.h5ad')\n",
    "\n",
    "# Load required R packages\n",
    "print('Loading R packages...')\n",
    "r(\"suppressPackageStartupMessages(library(Seurat))\")\n",
    "r(\"suppressPackageStartupMessages(library(Matrix))\")\n",
    "r(\"suppressPackageStartupMessages(library(dplyr))\")\n",
    "\n",
    "# Check Seurat version\n",
    "r(\"seurat_version <- packageVersion('Seurat')\")\n",
    "seurat_version = r(\"as.character(seurat_version)\")\n",
    "print(f\"Detected Seurat version: {seurat_version[0]}\")\n",
    "\n",
    "# Function to clean and standardize metadata\n",
    "def clean_metadata(meta_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean and standardize metadata to ensure all data types are compatible with h5ad format.\n",
    "    \n",
    "    Args:\n",
    "        meta_df: Input metadata DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned metadata DataFrame\n",
    "    \"\"\"\n",
    "    print(\"Cleaning and standardizing metadata...\")\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    meta = meta_df.copy()\n",
    "    \n",
    "    # Convert problematic columns to string\n",
    "    # These columns are commonly causing issues in conversion\n",
    "    str_columns = ['cell_barcode', 'organism', 'cell_type', 'crispr_type', \n",
    "                   'cancer_type', 'condition', 'perturbation_name', 'dataset']\n",
    "    \n",
    "    for col in str_columns:\n",
    "        if col in meta.columns:\n",
    "            meta[col] = meta[col].astype(str)\n",
    "    \n",
    "    # Handle NA values and convert mixed columns to string\n",
    "    for col in meta.columns:\n",
    "        # Skip numeric columns\n",
    "        if pd.api.types.is_numeric_dtype(meta[col]):\n",
    "            # Replace NaN in numeric columns with 0\n",
    "            meta[col] = meta[col].fillna(0)\n",
    "            continue\n",
    "            \n",
    "        # For mixed or problematic columns, convert to string\n",
    "        try:\n",
    "            # First try to convert to string\n",
    "            meta[col] = meta[col].fillna('NA').astype(str)\n",
    "        except:\n",
    "            print(f\"Warning: Could not convert column {col} to string. Dropping column.\")\n",
    "            meta = meta.drop(columns=[col])\n",
    "    \n",
    "    # Check for any remaining NaN values\n",
    "    if meta.isna().any().any():\n",
    "        print(\"Warning: NaN values remain in metadata. Converting to string 'NA'\")\n",
    "        meta = meta.fillna('NA')\n",
    "    \n",
    "    return meta\n",
    "\n",
    "# First extract the harmonized metadata directly using R\n",
    "print(\"Extracting harmonized metadata directly from R...\")\n",
    "r(f\"\"\"\n",
    "# Load the harmonized Seurat object\n",
    "harm_obj <- readRDS('{harmonized_rds}')\n",
    "\n",
    "# Function to safely convert to character\n",
    "safe_char <- function(x) {{\n",
    "  tryCatch({{\n",
    "    as.character(x)\n",
    "  }}, error = function(e) {{\n",
    "    rep(\"NA\", length(x))\n",
    "  }})\n",
    "}}\n",
    "\n",
    "# Extract the harmonized metadata and convert problematic columns to character\n",
    "harm_meta <- harm_obj@meta.data\n",
    "harm_meta$cell_barcode <- row.names(harm_meta)\n",
    "harm_meta$organism <- safe_char(harm_meta$organism)\n",
    "harm_meta$cell_type <- safe_char(harm_meta$cell_type)\n",
    "harm_meta$crispr_type <- safe_char(harm_meta$crispr_type)\n",
    "harm_meta$cancer_type <- safe_char(harm_meta$cancer_type)\n",
    "harm_meta$condition <- safe_char(harm_meta$condition)\n",
    "harm_meta$perturbation_name <- safe_char(harm_meta$perturbation_name)\n",
    "harm_meta$dataset <- safe_char(harm_meta$dataset)\n",
    "\n",
    "# Get indices per dataset\n",
    "dataset_indices <- list()\n",
    "for (ds in unique(harm_meta$dataset)) {{\n",
    "  dataset_indices[[ds]] <- which(harm_meta$dataset == ds)\n",
    "}}\n",
    "\n",
    "# Store info for reference\n",
    "datasets <- names(dataset_indices)\n",
    "dataset_sizes <- sapply(dataset_indices, length)\n",
    "print(paste(\"Found datasets:\", paste(datasets, collapse=\", \")))\n",
    "print(paste(\"Dataset sizes:\", paste(dataset_sizes, collapse=\", \")))\n",
    "\"\"\")\n",
    "\n",
    "# Function to process an individual RDS file with harmonized metadata\n",
    "def process_rds_file(rds_path: str, dataset_name: str) -> Tuple[str, Optional[anndata.AnnData]]:\n",
    "    \"\"\"\n",
    "    Process a single RDS file and create an h5ad file with harmonized metadata.\n",
    "    \n",
    "    Args:\n",
    "        rds_path: Path to the RDS file\n",
    "        dataset_name: Name of the dataset\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (output file path, AnnData object if successful)\n",
    "    \"\"\"\n",
    "    print(f\"Processing file: {rds_path} as {dataset_name}\")\n",
    "    output_file = os.path.join(data_dir, f\"{dataset_name}_harmonized.h5ad\")\n",
    "    \n",
    "    # Load the Seurat object\n",
    "    print(f'Loading {dataset_name}...')\n",
    "    r(f\"seu <- readRDS('{rds_path}')\")\n",
    "    \n",
    "    # Use intermediate file approach to avoid conversion issues\n",
    "    print('Converting to sparse matrix...')\n",
    "    temp_mtx = os.path.join(data_dir, f'temp_{dataset_name}.mtx')\n",
    "    temp_genes = os.path.join(data_dir, f'temp_{dataset_name}_genes.txt')\n",
    "    temp_cells = os.path.join(data_dir, f'temp_{dataset_name}_cells.txt')\n",
    "\n",
    "    # Extract data based on Seurat version\n",
    "    r(f\"\"\"\n",
    "    # Different approach for v5 vs older versions\n",
    "    seurat_v5 <- as.numeric(substr(packageVersion('Seurat'), 1, 1)) >= 5\n",
    "\n",
    "    if (seurat_v5) {{\n",
    "      # Seurat v5 approach\n",
    "      print(\"Using Seurat v5 approach with layers...\")\n",
    "      \n",
    "      # Get the first assay name\n",
    "      assay_name <- DefaultAssay(seu)\n",
    "      \n",
    "      # Get available layers\n",
    "      available_layers <- Layers(seu[[assay_name]])\n",
    "      print(paste(\"Available layers:\", paste(available_layers, collapse=\", \")))\n",
    "      \n",
    "      # If 'counts' exists as a layer, use it, otherwise use the first layer\n",
    "      layer_to_use <- if (\"counts\" %in% available_layers) \"counts\" else available_layers[1]\n",
    "      print(paste(\"Using layer:\", layer_to_use))\n",
    "      \n",
    "      # Access the layer\n",
    "      counts <- seu[[assay_name]][layer_to_use]\n",
    "    }} else {{\n",
    "      # Seurat v4 and older approach\n",
    "      print(\"Using traditional GetAssayData approach...\")\n",
    "      counts <- GetAssayData(seu, slot='counts', assay='RNA')\n",
    "    }}\n",
    "\n",
    "    # Write sparse matrix to file\n",
    "    print(\"Writing matrix to file...\")\n",
    "    writeMM(counts, '{temp_mtx}')\n",
    "\n",
    "    # Save gene and cell names\n",
    "    write.table(rownames(counts), '{temp_genes}', row.names=FALSE, col.names=FALSE, quote=FALSE)\n",
    "    write.table(colnames(counts), '{temp_cells}', row.names=FALSE, col.names=FALSE, quote=FALSE)\n",
    "    \"\"\")\n",
    "\n",
    "    # Read the sparse matrix\n",
    "    print('Reading sparse matrix from file...')\n",
    "    counts_sparse = scipy.io.mmread(temp_mtx).tocsr()\n",
    "    \n",
    "    # Read gene and cell names\n",
    "    genes = pd.read_csv(temp_genes, header=None)[0].values\n",
    "    cells = pd.read_csv(temp_cells, header=None)[0].values\n",
    "    \n",
    "    print(f'Extracting harmonized metadata for {dataset_name}...')\n",
    "    # Get dataset-specific metadata directly from R\n",
    "    with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "        # Use R to get the metadata for this specific dataset\n",
    "        r(f\"\"\"\n",
    "        # Create a metadata subset for just this dataset\n",
    "        dataset_cells <- which(harm_meta$dataset == '{dataset_name}')\n",
    "        dataset_meta <- harm_meta[dataset_cells, ]\n",
    "        \n",
    "        # Check for cell name alignment\n",
    "        original_cell_names <- readLines('{temp_cells}')\n",
    "        meta_cells <- rownames(dataset_meta)\n",
    "        \n",
    "        # If cell names don't match exactly, attempt to realign\n",
    "        if (!all(original_cell_names %in% meta_cells)) {{\n",
    "            # If we have same number of cells, assume same order\n",
    "            if (length(original_cell_names) == nrow(dataset_meta)) {{\n",
    "                rownames(dataset_meta) <- original_cell_names\n",
    "                print(\"Used positional matching for metadata\")\n",
    "            }} else {{\n",
    "                print(\"WARNING: Cell count mismatch, using basic metadata\")\n",
    "                dataset_meta <- data.frame(\n",
    "                    dataset = rep('{dataset_name}', length(original_cell_names)),\n",
    "                    cell_barcode = original_cell_names,\n",
    "                    row.names = original_cell_names\n",
    "                )\n",
    "                # Add required harmonized fields\n",
    "                dataset_meta$organism <- \"Mus musculus\"\n",
    "                dataset_meta$cell_type <- \"Prefrontal cortex neurons\"\n",
    "                dataset_meta$crispr_type <- if (grepl(\"CRISPRi\", '{dataset_name}')) \"CRISPRi\" else \"CRISPR KO\"\n",
    "                dataset_meta$cancer_type <- \"Non-Cancer\"\n",
    "                dataset_meta$condition <- \"unknown\"\n",
    "                dataset_meta$perturbation_name <- \"unknown\"\n",
    "            }}\n",
    "        }}\n",
    "        \n",
    "        # Write to CSV for Python to read\n",
    "        write.csv(dataset_meta, '{data_dir}/temp_{dataset_name}_meta.csv', row.names=TRUE)\n",
    "        \"\"\")\n",
    "    \n",
    "    # Read the metadata from CSV - this avoids serialization issues from R to Python\n",
    "    dataset_metadata = pd.read_csv(f\"{data_dir}/temp_{dataset_name}_meta.csv\", \n",
    "                                   index_col=0, low_memory=False)\n",
    "    \n",
    "    # Clean and standardize the metadata\n",
    "    dataset_metadata = clean_metadata(dataset_metadata)\n",
    "    \n",
    "    # Create AnnData object\n",
    "    print('Creating AnnData object...')\n",
    "    try:\n",
    "        adata = anndata.AnnData(\n",
    "            X=counts_sparse.T,  # Transpose to cells x genes\n",
    "            obs=dataset_metadata,\n",
    "            var=pd.DataFrame(index=genes)\n",
    "        )\n",
    "        \n",
    "        # Ensure required fields are present\n",
    "        required_fields = ['organism', 'cell_type', 'crispr_type', 'cancer_type', 'condition', 'perturbation_name']\n",
    "        for field in required_fields:\n",
    "            if field not in adata.obs.columns:\n",
    "                print(f'WARNING: Required field {field} missing from metadata')\n",
    "        \n",
    "        # Save as h5ad\n",
    "        print(f'Saving {dataset_name} to h5ad with harmonized metadata...')\n",
    "        adata.write(output_file)\n",
    "        \n",
    "        # Clean up temporary files\n",
    "        for file in [temp_mtx, temp_genes, temp_cells]:\n",
    "            try:\n",
    "                os.remove(file)\n",
    "            except:\n",
    "                print(f\"Warning: Could not remove temporary file {file}\")\n",
    "        \n",
    "        print(f'Completed processing {dataset_name}')\n",
    "        return output_file, adata\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating AnnData object: {e}\")\n",
    "        return output_file, None\n",
    "\n",
    "# Process original RDS files with harmonized metadata\n",
    "original_rds_files = [\n",
    "    (os.path.join(data_dir, 'GSE236519_arrayed_CRISPRi_hSyn.rds'), 'GSE236519_arrayed_CRISPRi_hSyn'),\n",
    "    (os.path.join(data_dir, 'GSE236519_arrayed_screen_hSyn.rds'), 'GSE236519_arrayed_screen_hSyn'),\n",
    "    (os.path.join(data_dir, 'GSE236519_lgdel_model.rds'), 'GSE236519_lgdel_model'),\n",
    "    (os.path.join(data_dir, 'GSE236519_pooled_screen_CBh.rds'), 'GSE236519_pooled_screen_CBh')\n",
    "]\n",
    "\n",
    "# Process each file individually\n",
    "h5ad_files = []\n",
    "adatas = []\n",
    "\n",
    "for rds_file, dataset_name in original_rds_files:\n",
    "    if os.path.exists(rds_file):\n",
    "        h5ad_file, adata = process_rds_file(rds_file, dataset_name)\n",
    "        if adata is not None:\n",
    "            h5ad_files.append(h5ad_file)\n",
    "            adatas.append(adata)\n",
    "    else:\n",
    "        print(f\"Warning: File {rds_file} not found\")\n",
    "\n",
    "# Now combine the datasets\n",
    "if len(adatas) > 0:\n",
    "    print(f\"\\nCombining {len(adatas)} datasets...\")\n",
    "    \n",
    "    try:\n",
    "        # Use scanpy's concatenate function\n",
    "        print(\"Using scanpy to combine datasets...\")\n",
    "        combined = sc.concat(\n",
    "            adatas, \n",
    "            join='outer',  # Use outer join to keep all genes\n",
    "            label='dataset_batch'  # This creates a new column with the dataset name\n",
    "        )\n",
    "        \n",
    "        # Ensure all required fields are present in final object\n",
    "        required_fields = ['organism', 'cell_type', 'crispr_type', 'cancer_type', 'condition', 'perturbation_name']\n",
    "        for field in required_fields:\n",
    "            if field not in combined.obs.columns:\n",
    "                print(f'WARNING: Required field {field} missing from combined metadata')\n",
    "                # Add a default value if missing\n",
    "                combined.obs[field] = \"unknown\"\n",
    "        \n",
    "        # Save as h5ad\n",
    "        print(f'Saving final harmonized h5ad to {output_h5ad}...')\n",
    "        combined.write(output_h5ad)\n",
    "        \n",
    "        print(f\"\\nConversion complete! Final harmonized file saved to {output_h5ad}\")\n",
    "        print(f\"Individual harmonized dataset files: {', '.join(h5ad_files)}\")\n",
    "        \n",
    "        # Print metadata stats for verification\n",
    "        print(\"\\nHarmonized metadata field summary:\")\n",
    "        for field in required_fields:\n",
    "            nunique = combined.obs[field].nunique()\n",
    "            print(f\"  {field}: {nunique} unique values\")\n",
    "            \n",
    "            # Print top values for categorical fields\n",
    "            if nunique < 20:  # Only for fields with few unique values\n",
    "                value_counts = combined.obs[field].value_counts().head(5)\n",
    "                print(f\"    Top values: {', '.join([f'{k} ({v})' for k, v in value_counts.items()])}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining datasets: {e}\")\n",
    "        print(\"Trying alternative approach...\")\n",
    "        \n",
    "        # Alternative approach: manually create combined h5ad\n",
    "        print(\"Creating combined h5ad by concatenating files manually...\")\n",
    "        \n",
    "        # Write a script to join them using command line tools\n",
    "        concatenation_script = os.path.join(data_dir, \"concatenate_h5ad.py\")\n",
    "        \n",
    "        with open(concatenation_script, \"w\") as f:\n",
    "            f.write(\"\"\"\n",
    "import scanpy as sc\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the list of files to concatenate\n",
    "files = sys.argv[1:-1]\n",
    "output_file = sys.argv[-1]\n",
    "\n",
    "print(f\"Concatenating {len(files)} h5ad files to {output_file}\")\n",
    "\n",
    "# Load the files\n",
    "adatas = []\n",
    "for file in files:\n",
    "    print(f\"Loading {file}...\")\n",
    "    try:\n",
    "        adata = sc.read_h5ad(file)\n",
    "        adatas.append(adata)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file}: {e}\")\n",
    "\n",
    "if len(adatas) > 0:\n",
    "    # Concatenate the files\n",
    "    print(\"Concatenating datasets...\")\n",
    "    combined = sc.concat(\n",
    "        adatas, \n",
    "        join='outer',\n",
    "        label='dataset'\n",
    "    )\n",
    "    \n",
    "    # Save the combined file\n",
    "    print(f\"Saving to {output_file}...\")\n",
    "    combined.write(output_file)\n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    print(\"No valid files to concatenate.\")\n",
    "\"\"\")\n",
    "        \n",
    "        # Make the script executable\n",
    "        os.chmod(concatenation_script, 0o755)\n",
    "        \n",
    "        # Run the script\n",
    "        cmd = [\"python\", concatenation_script] + h5ad_files + [output_h5ad]\n",
    "        print(\"Running command:\", \" \".join(cmd))\n",
    "        subprocess.call(cmd)\n",
    "else:\n",
    "    print(\"No datasets were successfully processed.\")\n",
    "\n",
    "# Clean up temporary files\n",
    "for temp_file in glob.glob(f\"{data_dir}/temp_*\"):\n",
    "    try:\n",
    "        os.remove(temp_file)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3b38bd-1bdb-4f0a-801c-082ce9d71f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the concatenated file\n",
    "adata = sc.read(\"/content/GSE236519/GSE236519_harmonized.h5ad\")\n",
    "\n",
    "# Check for reserved column names and fix them\n",
    "if '_index' in adata.obs.columns:\n",
    "    # Rename the '_index' column to 'orig_index'\n",
    "    adata.obs['orig_index'] = adata.obs['_index']\n",
    "    del adata.obs['_index']\n",
    "\n",
    "# Add the required harmonization fields if missing\n",
    "if 'organism' not in adata.obs.columns:\n",
    "    adata.obs['organism'] = \"Mus musculus\"\n",
    "\n",
    "if 'cell_type' not in adata.obs.columns:\n",
    "    # Use cell_types or cell_types_broad if available\n",
    "    if 'cell_types' in adata.obs.columns:\n",
    "        adata.obs['cell_type'] = adata.obs['cell_types']\n",
    "    elif 'cell_types_broad' in adata.obs.columns:\n",
    "        adata.obs['cell_type'] = adata.obs['cell_types_broad']\n",
    "    else:\n",
    "        adata.obs['cell_type'] = \"Prefrontal cortex neurons\"\n",
    "\n",
    "if 'crispr_type' not in adata.obs.columns:\n",
    "    # Set based on dataset\n",
    "    adata.obs['crispr_type'] = \"CRISPR KO\"\n",
    "    if 'dataset' in adata.obs.columns:\n",
    "        mask = adata.obs['dataset'].astype(str).str.contains('CRISPRi')\n",
    "        adata.obs.loc[mask, 'crispr_type'] = \"CRISPRi\"\n",
    "    elif 'dataset_name' in adata.obs.columns:\n",
    "        mask = adata.obs['dataset_name'].astype(str).str.contains('CRISPRi')\n",
    "        adata.obs.loc[mask, 'crispr_type'] = \"CRISPRi\"\n",
    "\n",
    "if 'cancer_type' not in adata.obs.columns:\n",
    "    adata.obs['cancer_type'] = \"Non-Cancer\"\n",
    "\n",
    "# Handle perturbation_name with the specific improvements requested\n",
    "# First, convert categorical to string to avoid category issues\n",
    "if 'perturbation_name' in adata.obs.columns:\n",
    "    # Convert to string to avoid categorical data issues\n",
    "    adata.obs['perturbation_name'] = adata.obs['perturbation_name'].astype(str)\n",
    "else:\n",
    "    # Use gRNAs if available, otherwise per_gene\n",
    "    if 'gRNAs' in adata.obs.columns:\n",
    "        adata.obs['perturbation_name'] = adata.obs['gRNAs'].astype(str)\n",
    "    elif 'per_gene' in adata.obs.columns:\n",
    "        adata.obs['perturbation_name'] = adata.obs['per_gene'].astype(str)\n",
    "    else:\n",
    "        adata.obs['perturbation_name'] = \"unknown\"\n",
    "\n",
    "# Clean up perturbation names\n",
    "# 1. Set \"nan\" values to np.nan for proper filtering\n",
    "adata.obs.loc[adata.obs['perturbation_name'] == 'nan', 'perturbation_name'] = np.nan\n",
    "\n",
    "# 2. Convert \"non-targeting\" to \"Non-targeting\" for consistency\n",
    "adata.obs.loc[adata.obs['perturbation_name'].str.lower() == 'non-targeting', 'perturbation_name'] = \"Non-targeting\"\n",
    "\n",
    "# 3. Remove numeric suffixes from gene names (e.g., Txnrd2_2 â†’ Txnrd2)\n",
    "# First, create a helper function to clean gene names\n",
    "def clean_gene_name(name):\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    name = str(name)\n",
    "    if '_' in name:\n",
    "        # Split by underscore and check if last part is numeric\n",
    "        parts = name.split('_')\n",
    "        if parts[-1].isdigit():\n",
    "            # Return all parts except the last numeric one\n",
    "            return '_'.join(parts[:-1])\n",
    "    return name\n",
    "\n",
    "# Apply the cleaning function to all perturbation names\n",
    "adata.obs['perturbation_name'] = adata.obs['perturbation_name'].apply(clean_gene_name)\n",
    "\n",
    "# Set controls consistently\n",
    "control_patterns = ['Safe_H', 'Safe', 'Control', 'Ctrl', 'control', 'ctrl', 'non-target', 'NT']\n",
    "for pattern in control_patterns:\n",
    "    mask = adata.obs['perturbation_name'].str.contains(pattern, case=False, na=False)\n",
    "    adata.obs.loc[mask, 'perturbation_name'] = \"Non-targeting\"\n",
    "\n",
    "# Ensure condition is present and consistent with perturbation names\n",
    "adata.obs['condition'] = \"test\"\n",
    "mask = adata.obs['perturbation_name'] == \"Non-targeting\"\n",
    "adata.obs.loc[mask, 'condition'] = \"control\"\n",
    "\n",
    "# Handle wt_ prefixes separately\n",
    "wt_mask = adata.obs['perturbation_name'].str.startswith('wt_', na=False)\n",
    "if wt_mask.any():\n",
    "    adata.obs.loc[wt_mask, 'condition'] = \"control\"\n",
    "\n",
    "# Remove cells with NaN perturbation names\n",
    "print(f\"Cell count before filtering: {adata.shape[0]}\")\n",
    "adata = adata[~adata.obs['perturbation_name'].isna()].copy()\n",
    "print(f\"Cell count after filtering: {adata.shape[0]}\")\n",
    "\n",
    "# Save the updated file\n",
    "adata.write(\"/content/GSE236519/GSE236519_harmonized_fixed.h5ad\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n======== Harmonized Metadata Fields ========\")\n",
    "required_fields = ['organism', 'cell_type', 'crispr_type', 'cancer_type', 'condition', 'perturbation_name']\n",
    "for field in required_fields:\n",
    "    n_unique = adata.obs[field].nunique()\n",
    "    print(f\"{field}: {n_unique} unique values\")\n",
    "    # Show the most common values\n",
    "    if n_unique < 20:\n",
    "        top_values = adata.obs[field].value_counts().head(5)\n",
    "        print(f\"  Top values: {', '.join([f'{k} ({v})' for k, v in top_values.items()])}\")\n",
    "    else:\n",
    "        print(f\"  (Many values)\")\n",
    "\n",
    "# Show updated perturbation name counts\n",
    "print(\"\\n======== Top Perturbation Targets ========\")\n",
    "perturbation_counts = adata.obs[\"perturbation_name\"].value_counts()\n",
    "print(perturbation_counts[:20])\n",
    "\n",
    "print(\"\\nFixed harmonized file saved to: /content/GSE236519/GSE236519_harmonized_fixed.h5ad\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
