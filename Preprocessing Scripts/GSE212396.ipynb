{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a30c09d-2469-48af-96d5-7c0cae4d0517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # GSE212396 Full Pipeline: Auto-Download, Process, Filter Unknown, Combine\n",
    "#\n",
    "# This updated script includes a snippet that sets `condition = \"Control\"`\n",
    "# for any `perturbation_name` in [\"negative_control\", \"safe_targeting\", \"Non-targeting\"]\n",
    "# and `condition = \"Test\"` otherwise.\n",
    "\n",
    "# %% [code]\n",
    "import os\n",
    "import logging\n",
    "import urllib.request\n",
    "import gzip\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import anndata as ad\n",
    "from scipy.io import mmread\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# FTP Source Info\n",
    "# ------------------------------------------------------------------------\n",
    "GEO_ACCESSION = \"GSE212396\"\n",
    "BASE_URL = \"https://ftp.ncbi.nlm.nih.gov/geo/series/GSE212nnn/GSE212396/suppl\"\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Datasets to process\n",
    "# ------------------------------------------------------------------------\n",
    "DATASETS = [\n",
    "    \"50gLib_Eahy926_16K_scRNAseq\",\n",
    "    \"50gLib_Eahy926_150K_scRNAseq\",\n",
    "    \"50gLib_TeloHAEC_150K_scRNAseq\",\n",
    "    \"200gLib_TeloHAEC_scRNAseq_1\",\n",
    "    \"200gLib_TeloHAEC_scRNAseq_2\",\n",
    "]\n",
    "\n",
    "# Guide files to process\n",
    "GUIDE_FILES = [\n",
    "    \"50gLib_targets_n_guides.txt\",\n",
    "    \"200gLib_targets_n_guides.txt\",\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Calls-file column name candidates\n",
    "# ------------------------------------------------------------------------\n",
    "CELL_BARCODE_COLS = [\"cell_barcode\", \"barcode\", \"cell\", \"cbc_10x\", \"CBC_10x\", \"CBC\"]\n",
    "GUIDE_COLS = [\"guide\", \"guide_id\", \"sgRNA_ID\", \"Guide\"]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Helper: Download with progress bar\n",
    "# ------------------------------------------------------------------------\n",
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "def download_file_if_needed(url: str, out_path: str) -> str:\n",
    "    \"\"\"Download out_path if it doesn't exist.\"\"\"\n",
    "    if os.path.exists(out_path):\n",
    "        logging.info(f\"File already exists: {out_path}\")\n",
    "        return out_path\n",
    "    \n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    logging.info(f\"Downloading {url} to {out_path}\")\n",
    "    with DownloadProgressBar(\n",
    "        unit='B', unit_scale=True, miniters=1, desc=os.path.basename(url)\n",
    "    ) as t:\n",
    "        urllib.request.urlretrieve(url, out_path, reporthook=t.update_to)\n",
    "    return out_path\n",
    "\n",
    "def gunzip_file_if_needed(gz_path: str) -> str:\n",
    "    \"\"\"Gunzip gz_path -> gz_path.replace('.gz','') if needed.\"\"\"\n",
    "    if not os.path.exists(gz_path):\n",
    "        raise FileNotFoundError(f\"Cannot gunzip. File not found: {gz_path}\")\n",
    "    \n",
    "    out_path = gz_path.replace(\".gz\", \"\")\n",
    "    if os.path.exists(out_path):\n",
    "        logging.info(f\"Unzipped file already exists: {out_path}\")\n",
    "        return out_path\n",
    "    \n",
    "    logging.info(f\"Gunzipping {gz_path} -> {out_path}\")\n",
    "    with gzip.open(gz_path, 'rb') as f_in, open(out_path, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "    return out_path\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Step 1: Download or confirm guide files\n",
    "# ------------------------------------------------------------------------\n",
    "def get_guide_files(root_dir: str, debug=False):\n",
    "    \"\"\"\n",
    "    Make sure each guide file is present & unzipped:\n",
    "      GSE212396_50gLib_targets_n_guides.txt\n",
    "      GSE212396_200gLib_targets_n_guides.txt\n",
    "    returns paths to the unzipped .txt files\n",
    "    \"\"\"\n",
    "    guide_dir = os.path.join(root_dir, \"guides\")\n",
    "    os.makedirs(guide_dir, exist_ok=True)\n",
    "\n",
    "    final_paths = []\n",
    "    for gf in GUIDE_FILES:\n",
    "        gz_name = f\"{GEO_ACCESSION}_{gf}.gz\"  # e.g. GSE212396_50gLib_targets_n_guides.txt.gz\n",
    "        gz_url  = f\"{BASE_URL}/{gz_name}\"\n",
    "        gz_path = os.path.join(guide_dir, gz_name)\n",
    "\n",
    "        # Download if needed\n",
    "        download_file_if_needed(gz_url, gz_path)\n",
    "        # Unzip if needed\n",
    "        out_path = gunzip_file_if_needed(gz_path)\n",
    "        final_paths.append(out_path)\n",
    "\n",
    "    return final_paths\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Step 2: Load guide info into a dictionary\n",
    "# ------------------------------------------------------------------------\n",
    "def load_guide_info(guide_txt_paths, debug=False):\n",
    "    \"\"\"\n",
    "    Build a dict:\n",
    "      guide_info[<GuideSequence>] = { target_gene: <>, library: <> }\n",
    "    from the 50gLib & 200gLib .txt files.\n",
    "    \n",
    "    If 'TargetGene'/'Gene' is missing, \n",
    "    we also check 'guideSet' as a fallback for the target gene.\n",
    "    \"\"\"\n",
    "    guide_info = {}\n",
    "    \n",
    "    for p in guide_txt_paths:\n",
    "        df = pd.read_csv(p, sep='\\t')\n",
    "        logging.info(f\"Reading guide info from {p}: {df.shape[0]} rows.\")\n",
    "        if debug:\n",
    "            print(\"[DEBUG] Columns:\", df.columns.tolist())\n",
    "            print(df.head(5))\n",
    "\n",
    "        # We look for 'GuideSequence', plus potential target columns\n",
    "        possible_guide_seq_cols = [\"GuideSequence\"]\n",
    "        possible_target_cols    = [\"TargetGene\", \"Gene\"]\n",
    "        fallback_col = \"guideSet\"  # We can treat guideSet as the \"target gene\" if needed\n",
    "\n",
    "        # find the guide-sequence col\n",
    "        guide_seq_col = None\n",
    "        for c in possible_guide_seq_cols:\n",
    "            if c in df.columns:\n",
    "                guide_seq_col = c\n",
    "                break\n",
    "\n",
    "        if not guide_seq_col:\n",
    "            logging.warning(f\"No 'GuideSequence' column in {p}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # find the target col\n",
    "        target_col = None\n",
    "        for c in possible_target_cols:\n",
    "            if c in df.columns:\n",
    "                target_col = c\n",
    "                break\n",
    "\n",
    "        if not target_col:\n",
    "            # see if fallback is present\n",
    "            if fallback_col in df.columns:\n",
    "                target_col = fallback_col\n",
    "                logging.info(f\"Using '{fallback_col}' as target gene column for {p}.\")\n",
    "            else:\n",
    "                logging.warning(f\"No 'TargetGene'/'Gene'/'guideSet' in {p}; using 'Unknown'.\")\n",
    "                df[\"UnknownGene\"] = \"Unknown\"\n",
    "                target_col = \"UnknownGene\"\n",
    "\n",
    "        library_name = os.path.basename(p)\n",
    "        for _, row in df.iterrows():\n",
    "            g_seq = str(row[guide_seq_col])\n",
    "            t_gene = str(row[target_col]) if pd.notnull(row[target_col]) else \"Unknown\"\n",
    "            guide_info[g_seq] = {\n",
    "                \"target_gene\": t_gene,\n",
    "                \"library\": library_name\n",
    "            }\n",
    "\n",
    "    logging.info(f\"guide_info built with {len(guide_info)} entries.\")\n",
    "    return guide_info\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Step 3: Download or confirm dataset files\n",
    "# ------------------------------------------------------------------------\n",
    "def get_dataset_files(root_dir: str, dataset_name: str):\n",
    "    \"\"\"\n",
    "    For each dataset, we expect 4 files:\n",
    "      1) _barcodes.tsv.gz\n",
    "      2) _features.tsv.gz\n",
    "      3) _matrix.mtx.gz\n",
    "      4) _calls.tsv.gz\n",
    "    Then we gunzip them if needed, returning final unzipped paths.\n",
    "    \"\"\"\n",
    "    dataset_dir = os.path.join(root_dir, dataset_name)\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    file_types = [\"barcodes.tsv\", \"features.tsv\", \"matrix.mtx\", \"calls.tsv\"]\n",
    "    final_paths = {}\n",
    "    for ft in file_types:\n",
    "        gz_name = f\"{GEO_ACCESSION}_{dataset_name}_{ft}.gz\"\n",
    "        gz_url  = f\"{BASE_URL}/{gz_name}\"\n",
    "        gz_path = os.path.join(dataset_dir, gz_name)\n",
    "\n",
    "        # Download\n",
    "        try:\n",
    "            download_file_if_needed(gz_url, gz_path)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Could not download {gz_url}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Unzip\n",
    "        try:\n",
    "            unzipped_path = gunzip_file_if_needed(gz_path)\n",
    "            final_paths[ft] = unzipped_path\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to gunzip {gz_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return final_paths\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Step 4: Load a single dataset into AnnData\n",
    "# ------------------------------------------------------------------------\n",
    "def load_dataset(file_paths: dict, debug=False):\n",
    "    \"\"\"\n",
    "    Given the unzipped file paths for 'matrix.mtx', 'features.tsv', 'barcodes.tsv', 'calls.tsv',\n",
    "    read them into an AnnData, ensuring var_names are unique.\n",
    "    \"\"\"\n",
    "    mtx_path = file_paths.get(\"matrix.mtx\")\n",
    "    feat_path = file_paths.get(\"features.tsv\")\n",
    "    bc_path   = file_paths.get(\"barcodes.tsv\")\n",
    "    calls_path = file_paths.get(\"calls.tsv\")\n",
    "\n",
    "    if not mtx_path or not feat_path or not bc_path:\n",
    "        raise FileNotFoundError(\"Missing one of matrix.mtx, features.tsv, or barcodes.tsv\")\n",
    "\n",
    "    X = mmread(mtx_path).tocsr()\n",
    "    var_df = pd.read_csv(feat_path, sep='\\t', header=None)\n",
    "    var_df.columns = ['gene_id', 'gene_symbol', 'feature_type']\n",
    "    obs_names = pd.read_csv(bc_path, sep='\\t', header=None)[0].values\n",
    "\n",
    "    # Check orientation\n",
    "    if X.shape[0] == len(var_df) and X.shape[1] == len(obs_names):\n",
    "        X = X.transpose()\n",
    "\n",
    "    adata = ad.AnnData(\n",
    "        X=X,\n",
    "        obs=pd.DataFrame(index=obs_names),\n",
    "        var=pd.DataFrame(index=var_df[\"gene_symbol\"].values)\n",
    "    )\n",
    "    adata.var[\"gene_ids\"] = var_df[\"gene_id\"].values\n",
    "    adata.var[\"feature_types\"] = var_df[\"feature_type\"].values\n",
    "\n",
    "    # calls\n",
    "    if calls_path and os.path.exists(calls_path):\n",
    "        calls_df = pd.read_csv(calls_path, sep='\\t')\n",
    "        if debug:\n",
    "            print(\"[DEBUG] Calls file columns:\", calls_df.columns.tolist())\n",
    "            print(calls_df.head(5))\n",
    "\n",
    "        # find cell col\n",
    "        cell_col = None\n",
    "        for c in CELL_BARCODE_COLS:\n",
    "            if c in calls_df.columns:\n",
    "                cell_col = c\n",
    "                break\n",
    "        # find guide col\n",
    "        guide_col = None\n",
    "        for g in GUIDE_COLS:\n",
    "            if g in calls_df.columns:\n",
    "                guide_col = g\n",
    "                break\n",
    "\n",
    "        if cell_col and guide_col:\n",
    "            raw_dict = dict(zip(calls_df[cell_col], calls_df[guide_col]))\n",
    "            # unify barcodes (-1 suffix)\n",
    "            adj_dict = {}\n",
    "            for k, v in raw_dict.items():\n",
    "                if k.endswith(\"-1\"):\n",
    "                    adj_dict[k] = v\n",
    "                else:\n",
    "                    adj_dict[k + \"-1\"] = v\n",
    "\n",
    "            guides = []\n",
    "            for bc in adata.obs_names:\n",
    "                guides.append(adj_dict.get(bc, \"Unknown\"))\n",
    "            adata.obs[\"guide\"] = pd.Categorical(guides)\n",
    "            if debug:\n",
    "                n_unk = (adata.obs[\"guide\"] == \"Unknown\").sum()\n",
    "                logging.info(f\"Barcodes->guide mapped: {adata.n_obs - n_unk} matched, {n_unk} unknown.\")\n",
    "        else:\n",
    "            logging.warning(\"No suitable cell/guide columns found in calls file.\")\n",
    "            adata.obs[\"guide\"] = \"Unknown\"\n",
    "    else:\n",
    "        logging.warning(\"No calls file found or path is missing.\")\n",
    "        adata.obs[\"guide\"] = \"Unknown\"\n",
    "\n",
    "    # **Ensure gene names are unique** to avoid issues when concatenating\n",
    "    adata.var_names_make_unique()\n",
    "\n",
    "    return adata\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Step 5A: Harmonize metadata\n",
    "# ------------------------------------------------------------------------\n",
    "def harmonize_metadata(adata, dataset_name: str, guide_info: dict, debug=False):\n",
    "    \"\"\"\n",
    "    Fill in obs fields: library, cell_line, loading, dataset, etc.\n",
    "    Then map guide -> perturbation_name if possible.\n",
    "    \"\"\"\n",
    "    parts = dataset_name.split('_')\n",
    "    if len(parts) >= 1:\n",
    "        adata.obs['library'] = parts[0]\n",
    "    if len(parts) >= 2:\n",
    "        adata.obs['cell_line'] = parts[1]\n",
    "    if len(parts) >= 3:\n",
    "        adata.obs['loading'] = parts[2]\n",
    "\n",
    "    adata.obs['dataset'] = dataset_name\n",
    "    adata.obs['organism'] = \"Homo sapiens\"\n",
    "    adata.obs['cell_type'] = \"Endothelial cells\"\n",
    "    adata.obs['crispr_type'] = \"CRISPRi\"\n",
    "    adata.obs['cancer_type'] = \"Non-Cancer\"\n",
    "    adata.obs['condition'] = \"Unknown\"  # Will be overwritten below\n",
    "\n",
    "    # Build 'perturbation_name'\n",
    "    if 'guide' in adata.obs.columns:\n",
    "        def get_gene(g):\n",
    "            return guide_info[g]['target_gene'] if g in guide_info else \"Unknown\"\n",
    "        adata.obs['perturbation_name'] = adata.obs['guide'].map(get_gene)\n",
    "\n",
    "        if debug:\n",
    "            n_unk = (adata.obs['perturbation_name'] == \"Unknown\").sum()\n",
    "            logging.info(f\"perturbation_name assigned for {adata.n_obs - n_unk} cells; {n_unk} unknown.\")\n",
    "    else:\n",
    "        adata.obs['perturbation_name'] = \"Unknown\"\n",
    "\n",
    "    return adata\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Step 5B: Filter out \"Unknown\" + rename \"NO TARGET...\" => \"Non-targeting\"\n",
    "#          Then set condition = \"Control\" or \"Test\" based on snippet\n",
    "# ------------------------------------------------------------------------\n",
    "def filter_and_assign_condition(adata: ad.AnnData, debug: bool=False) -> ad.AnnData:\n",
    "    \"\"\"\n",
    "    1) Exclude cells whose perturbation_name == 'Unknown'.\n",
    "    2) Rename \"NO TARGET...\" => \"Non-targeting\".\n",
    "    3) If perturbation_name in ['negative_control','safe_targeting','Non-targeting'],\n",
    "       condition = \"Control\", else \"Test\".\n",
    "    \"\"\"\n",
    "    # 1) Exclude \"Unknown\"\n",
    "    keep_mask = (adata.obs['perturbation_name'] != \"Unknown\")\n",
    "    before = adata.n_obs\n",
    "    adata = adata[keep_mask].copy()\n",
    "    after = adata.n_obs\n",
    "    if debug:\n",
    "        logging.info(f\"Excluded {before - after} cells with 'Unknown' perturbation_name.\")\n",
    "\n",
    "    # 2) Rename \"NO TARGET\" => \"Non-targeting\"\n",
    "    no_targ_mask = adata.obs['perturbation_name'].str.lower().str.contains(\"no\") & \\\n",
    "                   adata.obs['perturbation_name'].str.lower().str.contains(\"target\")\n",
    "    adata.obs.loc[no_targ_mask, 'perturbation_name'] = \"Non-targeting\"\n",
    "\n",
    "    # 3) Incorporate snippet:\n",
    "    #    if perturbation_name in [\"negative_control\", \"safe_targeting\", \"Non-targeting\"]\n",
    "    #    => condition = \"Control\"\n",
    "    #    else => condition = \"Test\"\n",
    "    control_perturbations = [\"negative_control\", \"safe_targeting\", \"Non-targeting\"]\n",
    "    mask_control = adata.obs['perturbation_name'].isin(control_perturbations)\n",
    "    adata.obs.loc[mask_control, \"condition\"] = \"Control\"\n",
    "    adata.obs.loc[~mask_control, \"condition\"] = \"Test\"\n",
    "\n",
    "    # convert object->categorical\n",
    "    for col in adata.obs.columns:\n",
    "        if adata.obs[col].dtype == object:\n",
    "            adata.obs[col] = adata.obs[col].astype('category')\n",
    "\n",
    "    return adata\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Main pipeline function\n",
    "# ------------------------------------------------------------------------\n",
    "def process_all_datasets(root_dir=\"./GSE212396_auto\", debug=False):\n",
    "    \"\"\"\n",
    "    1) Download/unzip guide files if missing\n",
    "    2) Build guide_info\n",
    "    3) For each dataset in DATASETS:\n",
    "        - download/unzip matrix/calls if missing\n",
    "        - load into AnnData\n",
    "        - harmonize\n",
    "        - filter out 'Unknown', rename 'NO TARGET' => 'Non-targeting', assign condition\n",
    "        - save .h5ad\n",
    "    4) Combine all datasets into \"combined.h5ad\"\n",
    "    \"\"\"\n",
    "    logging.info(f\"=== Starting GSE212396 pipeline ===\")\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "    guide_txt_paths = get_guide_files(root_dir, debug=debug)\n",
    "    guide_info = load_guide_info(guide_txt_paths, debug=debug)\n",
    "\n",
    "    adata_list = []\n",
    "    for ds in DATASETS:\n",
    "        logging.info(f\"=== Processing {ds} ===\")\n",
    "        ds_files = get_dataset_files(root_dir, ds)\n",
    "        adata = load_dataset(ds_files, debug=debug)\n",
    "        adata = harmonize_metadata(adata, ds, guide_info, debug=debug)\n",
    "        adata = filter_and_assign_condition(adata, debug=debug)\n",
    "        \n",
    "        out_path = os.path.join(root_dir, f\"{ds}.h5ad\")\n",
    "        adata.write_h5ad(out_path)\n",
    "        logging.info(f\"Saved {out_path}\")\n",
    "        adata_list.append(adata)\n",
    "\n",
    "    # Combine all\n",
    "    logging.info(\"Concatenating all datasets.\")\n",
    "    combined = ad.concat(adata_list, axis=0, join=\"outer\", label=\"batch\", keys=DATASETS)\n",
    "    combined_out = os.path.join(root_dir, \"combined.h5ad\")\n",
    "    combined.write_h5ad(combined_out)\n",
    "    logging.info(f\"Combined dataset saved to {combined_out}\")\n",
    "\n",
    "    logging.info(\"All datasets processed successfully!\")\n",
    "    return combined_out\n",
    "\n",
    "# %% [code]\n",
    "# In your notebook, just call:\n",
    "final_combined_path = process_all_datasets(debug=True)\n",
    "print(\"Final combined .h5ad:\", final_combined_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "703ca59a-be42-409e-b477-a3a0fbea9dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
