{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "703ca59a-be42-409e-b477-a3a0fbea9dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import glob\n",
    "import shutil\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from scipy import sparse\n",
    "import anndata as ad\n",
    "\n",
    "# URLs for downloading the dataset\n",
    "DATASET_URLS = {\n",
    "    'raw_data': 'https://ftp.ncbi.nlm.nih.gov/geo/series/GSE250nnn/GSE250558/suppl/GSE250558_RAW.tar',\n",
    "    'feature_ref': 'https://ftp.ncbi.nlm.nih.gov/geo/series/GSE250nnn/GSE250558/suppl/GSE250558_feature_ref.csv.gz',\n",
    "    'results': 'https://ftp.ncbi.nlm.nih.gov/geo/series/GSE250nnn/GSE250558/suppl/GSE250558_all_results_combined.tsv.gz'\n",
    "}\n",
    "\n",
    "def download_file(url, output_path):\n",
    "    \"\"\"Download a file from a URL to the specified output path.\"\"\"\n",
    "    print(f\"Downloading {url} to {output_path}...\")\n",
    "    urllib.request.urlretrieve(url, output_path)\n",
    "    print(f\"Downloaded {output_path}\")\n",
    "\n",
    "def ensure_data_available(data_dir):\n",
    "    \"\"\"Ensure all required data files are available, downloading if necessary.\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    data_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Check for raw data tar file\n",
    "    raw_tar_path = data_dir / 'GSE250558_RAW.tar'\n",
    "    if not raw_tar_path.exists():\n",
    "        download_file(DATASET_URLS['raw_data'], raw_tar_path)\n",
    "        # Extract the tar file\n",
    "        print(f\"Extracting {raw_tar_path}...\")\n",
    "        os.system(f\"tar -xf {raw_tar_path} -C {data_dir}\")\n",
    "    \n",
    "    # Check for feature reference file\n",
    "    feature_ref_path = data_dir / 'GSE250558_feature_ref.csv.gz'\n",
    "    if not feature_ref_path.exists():\n",
    "        download_file(DATASET_URLS['feature_ref'], feature_ref_path)\n",
    "    \n",
    "    # Check for results file\n",
    "    results_path = data_dir / 'GSE250558_all_results_combined.tsv.gz'\n",
    "    if not results_path.exists():\n",
    "        download_file(DATASET_URLS['results'], results_path)\n",
    "    \n",
    "    # Check if count matrices are extracted\n",
    "    count_matrices = list(data_dir.glob('GSM*.count_matrix.h5'))\n",
    "    if not count_matrices:\n",
    "        print(\"Count matrices not found. Checking if they need to be extracted from tar file...\")\n",
    "        os.system(f\"tar -xf {raw_tar_path} -C {data_dir}\")\n",
    "        count_matrices = list(data_dir.glob('GSM*.count_matrix.h5'))\n",
    "        if not count_matrices:\n",
    "            raise FileNotFoundError(\"Count matrices not found even after extraction. Please check the tar file.\")\n",
    "    \n",
    "    return {\n",
    "        'feature_ref_path': feature_ref_path,\n",
    "        'results_path': results_path,\n",
    "        'count_matrices': count_matrices\n",
    "    }\n",
    "\n",
    "def read_10x_h5(filename):\n",
    "    \"\"\"Read a 10X h5 file and return the matrix, barcodes, and features.\"\"\"\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        # Get the matrix dimensions\n",
    "        shape = f['matrix']['shape'][:]\n",
    "        \n",
    "        # Get the sparse matrix data\n",
    "        data = f['matrix']['data'][:]\n",
    "        indices = f['matrix']['indices'][:]\n",
    "        indptr = f['matrix']['indptr'][:]\n",
    "        \n",
    "        # Create a CSC matrix (genes are columns in 10X format)\n",
    "        matrix = sparse.csc_matrix((data, indices, indptr), shape=shape)\n",
    "        \n",
    "        # Get barcodes and features\n",
    "        barcodes = [b.decode('utf-8') for b in f['matrix']['barcodes'][:]]\n",
    "        \n",
    "        # Get feature information\n",
    "        feature_dict = {}\n",
    "        for key in f['matrix']['features'].keys():\n",
    "            if key != '_all_tag_keys':  # Skip this key\n",
    "                feature_dict[key] = [item.decode('utf-8') if isinstance(item, bytes) else item \n",
    "                                    for item in f['matrix']['features'][key][:]]\n",
    "        \n",
    "        feature_df = pd.DataFrame(feature_dict)\n",
    "        \n",
    "        return matrix, barcodes, feature_df\n",
    "\n",
    "def read_protospacer_calls(filename):\n",
    "    \"\"\"Read a protospacer calls file and return as a DataFrame.\"\"\"\n",
    "    with gzip.open(filename, 'rt') as f:\n",
    "        return pd.read_csv(f)\n",
    "\n",
    "def parse_sample_info(filename):\n",
    "    \"\"\"Parse sample information from filename.\"\"\"\n",
    "    basename = os.path.basename(filename)\n",
    "    parts = basename.split('_')\n",
    "    \n",
    "    # Extract time point (6h, 12h, 18h)\n",
    "    time_point = next((part for part in parts if part.endswith('h')), None)\n",
    "    \n",
    "    # Check if it's a replicate\n",
    "    is_replicate = any(part.startswith('R') and part[1:].isdigit() for part in parts)\n",
    "    replicate = next((part for part in parts if part.startswith('R') and part[1:].isdigit()), 'R1')\n",
    "    \n",
    "    # Check if it's untargeted\n",
    "    is_untargeted = 'untargeted' in basename\n",
    "    \n",
    "    # Create a sample key that can be used to match with protospacer files\n",
    "    # Format: Hek6h_R2_untargeted or Hek6h\n",
    "    sample_key = time_point if time_point else ''\n",
    "    if replicate != 'R1':\n",
    "        sample_key += f\"_{replicate}\"\n",
    "    if is_untargeted:\n",
    "        sample_key += \"_untargeted\"\n",
    "    \n",
    "    return {\n",
    "        'time_point': time_point,\n",
    "        'replicate': replicate,\n",
    "        'is_untargeted': is_untargeted,\n",
    "        'sample_id': basename.split('.')[0],\n",
    "        'sample_key': sample_key\n",
    "    }\n",
    "\n",
    "def process_sample(count_matrix_path, feature_ref, proto_file_dict, output_dir):\n",
    "    \"\"\"Process a single sample and save it to a file.\"\"\"\n",
    "    sample_info = parse_sample_info(count_matrix_path)\n",
    "    sample_id = sample_info['sample_id']\n",
    "    sample_key = sample_info['sample_key']\n",
    "    print(f\"Processing {sample_id} (key: {sample_key})...\")\n",
    "    \n",
    "    # Create a mapping from guide ID to target gene\n",
    "    guide_to_target = dict(zip(feature_ref['id'], feature_ref['target_gene_name']))\n",
    "    \n",
    "    # Find corresponding protospacer calls file\n",
    "    proto_file = proto_file_dict.get(sample_key)\n",
    "    \n",
    "    # Read count matrix\n",
    "    matrix, barcodes, feature_df = read_10x_h5(count_matrix_path)\n",
    "    \n",
    "    # Create AnnData object\n",
    "    adata = ad.AnnData(\n",
    "        X=matrix.T,  # Transpose to get cells as rows, genes as columns\n",
    "        obs=pd.DataFrame(index=barcodes),\n",
    "        var=feature_df.set_index('id')\n",
    "    )\n",
    "    \n",
    "    # Add sample information to obs\n",
    "    adata.obs['time_point'] = str(sample_info['time_point'])\n",
    "    adata.obs['replicate'] = str(sample_info['replicate'])\n",
    "    adata.obs['is_untargeted'] = str(sample_info['is_untargeted'])\n",
    "    adata.obs['sample_id'] = str(sample_id)\n",
    "    \n",
    "    if proto_file is None:\n",
    "        print(f\"Warning: No protospacer calls file found for {sample_id}. Creating AnnData without perturbation info.\")\n",
    "        # Add default perturbation information\n",
    "        adata.obs['perturbation'] = 'Unknown'\n",
    "        adata.obs['target_gene'] = 'Unknown'\n",
    "        adata.obs['condition'] = 'Unknown'\n",
    "    else:\n",
    "        # Read protospacer calls\n",
    "        proto_calls = read_protospacer_calls(proto_file)\n",
    "        \n",
    "        # Add perturbation information from protospacer calls\n",
    "        # Create a mapping from cell barcode to perturbation\n",
    "        cell_to_perturbation = {}\n",
    "        cell_to_target_gene = {}\n",
    "        \n",
    "        for _, row in proto_calls.iterrows():\n",
    "            cell_barcode = row['cell_barcode']\n",
    "            feature_call = row['feature_call']\n",
    "            cell_to_perturbation[cell_barcode] = feature_call\n",
    "            cell_to_target_gene[cell_barcode] = guide_to_target.get(feature_call, 'Unknown')\n",
    "        \n",
    "        # Add perturbation information to obs\n",
    "        adata.obs['perturbation'] = adata.obs.index.map(lambda x: cell_to_perturbation.get(x, 'Unknown'))\n",
    "        adata.obs['target_gene'] = adata.obs.index.map(lambda x: cell_to_target_gene.get(x, 'Unknown'))\n",
    "        \n",
    "        # Add condition (control or test)\n",
    "        adata.obs['condition'] = adata.obs['target_gene'].apply(\n",
    "            lambda x: 'Control' if x in ['Non-Targeting', 'Safe_Cutter'] else 'Test'\n",
    "        )\n",
    "    \n",
    "    # Add harmonized metadata\n",
    "    adata.obs['organism'] = 'Homo sapiens'\n",
    "    adata.obs['cell_type'] = 'HEK293'\n",
    "    adata.obs['crispr_type'] = 'CRISPR KO'\n",
    "    adata.obs['cancer_type'] = 'Non-Cancer'\n",
    "    adata.obs['perturbation_name'] = adata.obs['target_gene']\n",
    "    \n",
    "    # Save the sample\n",
    "    output_file = os.path.join(output_dir, f\"{sample_id}_processed.h5ad\")\n",
    "    adata.write_h5ad(output_file)\n",
    "    print(f\"Saved {sample_id} to {output_file}\")\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "def run_harmonization(data_dir, output_file=None):\n",
    "    \"\"\"\n",
    "    Run the harmonization pipeline in a Jupyter Notebook.\n",
    "    \n",
    "    Parameters:\n",
    "      data_dir (str): Directory where the data will be stored and processed.\n",
    "      output_file (str, optional): Final combined output file path.\n",
    "    \"\"\"\n",
    "    print(f\"Processing GSE250558 dataset from {data_dir}\")\n",
    "    if output_file is None:\n",
    "        output_file = os.path.join(data_dir, 'GSE250558_harmonized.h5ad')\n",
    "    print(f\"Combined output will be saved to {output_file}\")\n",
    "    \n",
    "    # Create a directory for intermediate files\n",
    "    data_dir_path = Path(data_dir)\n",
    "    output_dir = data_dir_path / 'processed_samples'\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Ensure data is available\n",
    "    file_paths = ensure_data_available(data_dir)\n",
    "    \n",
    "    # Read feature reference\n",
    "    with gzip.open(file_paths['feature_ref_path'], 'rt') as f:\n",
    "        feature_ref = pd.read_csv(f)\n",
    "    \n",
    "    # Get all protospacer call files\n",
    "    proto_files = list(data_dir_path.glob('GSM*.protospacer_calls_per_cell.csv.gz'))\n",
    "    proto_file_dict = {}\n",
    "    \n",
    "    # Create a mapping from sample key to protospacer file\n",
    "    for proto_file in proto_files:\n",
    "        file_name = proto_file.name\n",
    "        # Extract the sample key from the protospacer file name\n",
    "        # Format: GSM7981577_Hek6h.protospacer_calls_per_cell.csv.gz\n",
    "        # or: GSM7981577_Hek6h_untargeted.protospacer_calls_per_cell.csv.gz\n",
    "        parts = file_name.split('.')\n",
    "        if len(parts) >= 2:\n",
    "            sample_parts = parts[0].split('_')\n",
    "            # Remove the GSM number\n",
    "            sample_parts = sample_parts[1:]\n",
    "            # Join the remaining parts to get the sample key\n",
    "            sample_key = '_'.join(sample_parts)\n",
    "            proto_file_dict[sample_key] = proto_file\n",
    "            print(f\"Mapped protospacer file {file_name} to sample key {sample_key}\")\n",
    "    \n",
    "    # Process each sample individually\n",
    "    processed_files = []\n",
    "    for count_matrix_path in sorted(data_dir_path.glob('GSM*.count_matrix.h5')):\n",
    "        processed_file = process_sample(count_matrix_path, feature_ref, proto_file_dict, output_dir)\n",
    "        processed_files.append(processed_file)\n",
    "    \n",
    "    # Create a summary of the processed samples\n",
    "    print(f\"\\nProcessed {len(processed_files)} samples.\")\n",
    "    print(\"Creating a summary of the processed samples...\")\n",
    "    \n",
    "    # Create a summary file with metadata\n",
    "    summary_file = os.path.join(data_dir, 'GSE250558_summary.csv')\n",
    "    metadata_list = []\n",
    "    \n",
    "    for file_path in processed_files:\n",
    "        try:\n",
    "            adata = ad.read_h5ad(file_path, backed='r')\n",
    "            sample_obs = adata.obs.head(1).copy()\n",
    "            sample_obs['n_cells'] = adata.n_obs\n",
    "            sample_obs['n_genes'] = adata.n_vars\n",
    "            sample_obs['file_path'] = file_path\n",
    "            metadata_list.append(sample_obs)\n",
    "            del adata\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    if metadata_list:\n",
    "        metadata_df = pd.concat(metadata_list)\n",
    "        metadata_df.to_csv(summary_file)\n",
    "        print(f\"Summary saved to {summary_file}\")\n",
    "    \n",
    "    # Combine all processed samples into one AnnData object\n",
    "    print(\"\\nCombining all processed samples into one AnnData object...\")\n",
    "    sample_files = list(data_dir_path.glob('processed_samples/*.h5ad'))\n",
    "    if sample_files:\n",
    "        combined = ad.concat([ad.read_h5ad(f) for f in sample_files], join='outer', merge='same')\n",
    "        combined.write_h5ad(output_file)\n",
    "        print(f\"Combined AnnData saved to {output_file}\")\n",
    "    else:\n",
    "        print(\"No processed sample files found to combine.\")\n",
    "    \n",
    "    print(\"\\nIndividual sample files have been saved in the 'processed_samples' directory.\")\n",
    "    print(\"To work with the data, you can load individual samples or use the combined dataset as needed.\")\n",
    "    \n",
    "    # Print summary from the metadata\n",
    "    if metadata_list:\n",
    "        print(\"\\nSample summary:\")\n",
    "        print(f\"Total number of samples: {len(metadata_df)}\")\n",
    "        print(f\"Total number of cells: {metadata_df['n_cells'].sum()}\")\n",
    "        print(\"\\nTime points:\")\n",
    "        print(metadata_df['time_point'].value_counts())\n",
    "        print(\"\\nReplicates:\")\n",
    "        print(metadata_df['replicate'].value_counts())\n",
    "        if 'perturbation_name' in metadata_df.columns:\n",
    "            non_unknown = metadata_df[metadata_df['perturbation_name'] != 'Unknown']\n",
    "            if len(non_unknown) > 0:\n",
    "                print(\"\\nSamples with perturbation information:\")\n",
    "                print(non_unknown[['sample_id', 'n_cells']])\n",
    "    \n",
    "    return\n",
    "\n",
    "# --- Run the harmonization pipeline ---\n",
    "# Replace 'your_data_directory_path' with the path where you want to store and process the data.\n",
    "data_directory = \"/content/GSE250558\"  # e.g., \"./data\"\n",
    "run_harmonization(data_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3b25c8-58e1-4eb6-b7a6-5d7c96dae9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "\n",
    "# Load the combined AnnData object\n",
    "adata = ad.read_h5ad(\"/content/GSE250558/GSE250558_harmonized.h5ad\")  # adjust file path if needed\n",
    "\n",
    "# Exclude cells with \"Unknown\" perturbation\n",
    "adata = adata[adata.obs['perturbation_name'] != \"Unknown\"].copy()\n",
    "\n",
    "# If 'perturbation_name' is categorical, add \"Non-targeting\" as a category if needed.\n",
    "if hasattr(adata.obs['perturbation_name'], \"cat\"):\n",
    "    if \"Non-targeting\" not in adata.obs['perturbation_name'].cat.categories:\n",
    "        adata.obs['perturbation_name'] = adata.obs['perturbation_name'].cat.add_categories(\"Non-targeting\")\n",
    "\n",
    "# Standardize the labels: convert \"Non-Targeting\" and \"Safe_Cutter\" to \"Non-targeting\"\n",
    "adata.obs.loc[adata.obs['perturbation_name'].isin([\"Non-Targeting\", \"Safe_Cutter\"]), \"perturbation_name\"] = \"Non-targeting\"\n",
    "# Set the condition for these cells to \"Control\"\n",
    "adata.obs.loc[adata.obs['perturbation_name'] == \"Non-targeting\", \"condition\"] = \"Control\"\n",
    "\n",
    "# Print the number of cells after filtering and the number of control cells\n",
    "print(\"Total cells after filtering unknowns:\", adata.n_obs)\n",
    "control_count = (adata.obs['perturbation_name'] == \"Non-targeting\").sum()\n",
    "print(\"Number of control cells (Non-targeting):\", control_count)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
