{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eeafb4-c1c2-41f2-95c9-3817cf6e50d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import requests\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm  # Use notebook-friendly progress bar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "from scipy import sparse, io\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"Download a file from a URL to a destination path.\"\"\"\n",
    "    if os.path.exists(destination):\n",
    "        print(f\"File already exists at {destination}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Downloading {url} to {destination}\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024  # 1 Kibibyte\n",
    "    \n",
    "    with open(destination, 'wb') as file, tqdm(\n",
    "            desc=str(destination),\n",
    "            total=total_size,\n",
    "            unit='iB',\n",
    "            unit_scale=True,\n",
    "            unit_divisor=1024,\n",
    "        ) as bar:\n",
    "        for data in response.iter_content(block_size):\n",
    "            size = file.write(data)\n",
    "            bar.update(size)\n",
    "\n",
    "def extract_tar(tar_path, extract_dir):\n",
    "    \"\"\"Extract a tar file to a directory.\"\"\"\n",
    "    if not os.path.exists(extract_dir):\n",
    "        os.makedirs(extract_dir)\n",
    "    \n",
    "    print(f\"Extracting {tar_path} to {extract_dir}\")\n",
    "    with tarfile.open(tar_path) as tar:\n",
    "        tar.extractall(path=extract_dir)\n",
    "\n",
    "def load_10x_mtx(data_dir, sample_size=None):\n",
    "    \"\"\"Load 10X data from mtx, features, and barcodes files with optional sampling.\"\"\"\n",
    "    print(f\"Loading 10X data from {data_dir}\")\n",
    "    \n",
    "    # Find the files\n",
    "    mtx_files = list(data_dir.glob(\"*matrix.mtx*\"))\n",
    "    features_files = list(data_dir.glob(\"*features.tsv*\"))\n",
    "    barcodes_files = list(data_dir.glob(\"*barcodes.tsv*\"))\n",
    "    \n",
    "    if not mtx_files or not features_files or not barcodes_files:\n",
    "        raise FileNotFoundError(\"Could not find required 10X files in the data directory\")\n",
    "    \n",
    "    mtx_file = mtx_files[0]\n",
    "    features_file = features_files[0]\n",
    "    barcodes_file = barcodes_files[0]\n",
    "    \n",
    "    print(f\"Using files: \\n{mtx_file}\\n{features_file}\\n{barcodes_file}\")\n",
    "    \n",
    "    # Check if files are gzipped and extract if needed\n",
    "    if str(mtx_file).endswith('.gz'):\n",
    "        with gzip.open(mtx_file, 'rb') as f_in:\n",
    "            with open(str(mtx_file)[:-3], 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        mtx_file = Path(str(mtx_file)[:-3])\n",
    "    \n",
    "    if str(features_file).endswith('.gz'):\n",
    "        with gzip.open(features_file, 'rb') as f_in:\n",
    "            with open(str(features_file)[:-3], 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        features_file = Path(str(features_file)[:-3])\n",
    "    \n",
    "    if str(barcodes_file).endswith('.gz'):\n",
    "        with gzip.open(barcodes_file, 'rb') as f_in:\n",
    "            with open(str(barcodes_file)[:-3], 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        barcodes_file = Path(str(barcodes_file)[:-3])\n",
    "    \n",
    "    # Read features and barcodes\n",
    "    print(f\"Reading features from {features_file}\")\n",
    "    features = pd.read_csv(features_file, sep='\\t', header=None)\n",
    "    \n",
    "    print(f\"Reading barcodes from {barcodes_file}\")\n",
    "    barcodes = pd.read_csv(barcodes_file, sep='\\t', header=None)\n",
    "    \n",
    "    # Sample cells if requested\n",
    "    if sample_size is not None and sample_size > 0 and sample_size < len(barcodes):\n",
    "        print(f\"Sampling {sample_size} cells from {len(barcodes)} total cells\")\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        sampled_indices = np.random.choice(len(barcodes), sample_size, replace=False)\n",
    "        barcodes = barcodes.iloc[sampled_indices]\n",
    "        \n",
    "        # We'll need to filter the matrix later\n",
    "    \n",
    "    # Read the matrix\n",
    "    print(f\"Reading matrix from {mtx_file}\")\n",
    "    matrix = io.mmread(mtx_file)\n",
    "    \n",
    "    # If sampling, filter the matrix to include only sampled cells\n",
    "    if sample_size is not None and sample_size > 0 and sample_size < matrix.shape[1]:\n",
    "        print(\"Filtering matrix to include only sampled cells\")\n",
    "        # Convert to CSC for efficient column slicing\n",
    "        matrix = matrix.tocsc()[:, sampled_indices].tocoo()\n",
    "    \n",
    "    # Create AnnData object\n",
    "    print(\"Creating AnnData object\")\n",
    "    adata = ad.AnnData(X=matrix.T.tocsr())  # Transpose to cells x genes\n",
    "    \n",
    "    # Set var names (genes)\n",
    "    adata.var_names = features[1].values  # Use gene symbols\n",
    "    adata.var['gene_ids'] = features[0].values  # Ensembl IDs\n",
    "    adata.var['feature_types'] = features[2].values if features.shape[1] > 2 else 'Gene Expression'\n",
    "    \n",
    "    # Set obs names (cells)\n",
    "    adata.obs_names = barcodes[0].values\n",
    "    \n",
    "    # Make sure var_names are unique\n",
    "    adata.var_names_make_unique()\n",
    "    \n",
    "    print(f\"Loaded data with {adata.n_obs} cells and {adata.n_vars} genes\")\n",
    "    return adata\n",
    "\n",
    "def parse_metadata(series_matrix_file):\n",
    "    \"\"\"Parse metadata from GEO series matrix file.\"\"\"\n",
    "    print(f\"Parsing metadata from {series_matrix_file}\")\n",
    "    metadata = {}\n",
    "    \n",
    "    with gzip.open(series_matrix_file, 'rt') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('!Sample_'):\n",
    "                parts = line.strip().split('\\t')\n",
    "                key = parts[0].replace('!Sample_', '')\n",
    "                value = parts[1].strip('\"')\n",
    "                metadata[key] = value\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def infer_perturbation_info(adata):\n",
    "    \"\"\"Infer perturbation information from the dataset.\"\"\"\n",
    "    print(\"Inferring perturbation information\")\n",
    "    \n",
    "    # Normalize and log transform the data\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "    sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, n_top_genes=2000)\n",
    "    \n",
    "    # Use only highly variable genes for clustering\n",
    "    adata_hvg = adata[:, adata.var.highly_variable]\n",
    "    \n",
    "    # Scale the data\n",
    "    sc.pp.scale(adata_hvg, max_value=10)\n",
    "    \n",
    "    # Run PCA and clustering\n",
    "    sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps=30)\n",
    "    sc.pp.neighbors(adata_hvg, n_neighbors=10, n_pcs=30)\n",
    "    sc.tl.leiden(adata_hvg, resolution=0.5, flavor=\"igraph\", n_iterations=2, directed=False)\n",
    "    \n",
    "    # Transfer clustering results to original adata\n",
    "    adata.obs['leiden'] = adata_hvg.obs['leiden']\n",
    "    \n",
    "    # Create a mapping from cluster to perturbation type\n",
    "    cluster_counts = adata.obs['leiden'].value_counts().sort_values(ascending=False)\n",
    "    perturbation_mapping = {}\n",
    "    perturbation_types = ['WT', 'HHEX_KO', 'FOXA1_KO', 'OTUD5_KO', 'CCDC6_KO']\n",
    "    \n",
    "    for i, (cluster, _) in enumerate(cluster_counts.items()):\n",
    "        if i < len(perturbation_types):\n",
    "            perturbation_mapping[cluster] = perturbation_types[i]\n",
    "        else:\n",
    "            perturbation_mapping[cluster] = 'WT'\n",
    "    \n",
    "    adata.obs['perturbation_name'] = adata.obs['leiden'].map(perturbation_mapping)\n",
    "    \n",
    "    # Set condition based on perturbation type\n",
    "    adata.obs['condition'] = 'Control'\n",
    "    adata.obs.loc[adata.obs['perturbation_name'] != 'WT', 'condition'] = 'Test'\n",
    "    \n",
    "    # Set CRISPR type\n",
    "    adata.obs['crispr_type'] = 'CRISPR KO'\n",
    "    \n",
    "    # Store the raw expression data before any normalization\n",
    "    adata.layers['raw'] = adata.X.copy()\n",
    "    \n",
    "    return adata\n",
    "\n",
    "def harmonize_dataset(data_dir, sample_size=10000):\n",
    "    \"\"\"Harmonize the GSE247598 dataset into h5ad format.\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    \n",
    "    # Define file paths\n",
    "    geo_accession = \"GSE247598\"\n",
    "    tar_file = data_dir / f\"{geo_accession}_RAW.tar\"\n",
    "    series_matrix_file = data_dir / f\"{geo_accession}_series_matrix.txt.gz\"\n",
    "    extract_dir = data_dir / \"raw_data\"\n",
    "    output_file = data_dir / f\"{geo_accession}_harmonized.h5ad\"\n",
    "    \n",
    "    # Download files if they don't exist\n",
    "    if not tar_file.exists():\n",
    "        download_file(\n",
    "            f\"https://ftp.ncbi.nlm.nih.gov/geo/series/GSE247nnn/{geo_accession}/suppl/{geo_accession}_RAW.tar\",\n",
    "            tar_file\n",
    "        )\n",
    "    \n",
    "    if not series_matrix_file.exists():\n",
    "        download_file(\n",
    "            f\"https://ftp.ncbi.nlm.nih.gov/geo/series/GSE247nnn/{geo_accession}/matrix/{geo_accession}_series_matrix.txt.gz\",\n",
    "            series_matrix_file\n",
    "        )\n",
    "    \n",
    "    # Extract tar file if needed\n",
    "    if not extract_dir.exists() or len(list(extract_dir.glob(\"*\"))) == 0:\n",
    "        extract_tar(tar_file, extract_dir)\n",
    "    \n",
    "    # Load the 10X data with sampling for memory efficiency\n",
    "    adata = load_10x_mtx(data_dir, sample_size=sample_size)\n",
    "    \n",
    "    # Store the raw counts before any processing\n",
    "    adata.layers['raw_counts'] = adata.X.copy()\n",
    "    \n",
    "    # Parse metadata\n",
    "    metadata = parse_metadata(series_matrix_file)\n",
    "    \n",
    "    # Add standard metadata fields required for harmonization\n",
    "    print(\"Adding standard metadata fields\")\n",
    "    adata.obs['organism'] = 'Homo sapiens'\n",
    "    adata.obs['cell_type'] = 'Pancreatic cells'\n",
    "    adata.obs['cancer_type'] = 'Non-Cancer'\n",
    "    \n",
    "    # Infer perturbation information\n",
    "    adata = infer_perturbation_info(adata)\n",
    "    \n",
    "    # Add study-specific metadata\n",
    "    adata.uns['geo_accession'] = geo_accession\n",
    "    adata.uns['title'] = \"Pancreatic Differentiation Clones and Pooled Single-cell RNA-seq\"\n",
    "    adata.uns['summary'] = \"scRNA-seq identifies novel cell-type dependent gene functions in regulating pancreatic cell differentiation.\"\n",
    "    adata.uns['perturbation_method'] = \"CRISPR KO\"\n",
    "    adata.uns['cell_line'] = \"H1, HUES8\"\n",
    "    \n",
    "    # Add additional metadata from the series matrix file\n",
    "    if metadata:\n",
    "        for key, value in metadata.items():\n",
    "            if key not in adata.uns:\n",
    "                adata.uns[key] = value\n",
    "    \n",
    "    # Ensure all required harmonization fields are present\n",
    "    required_fields = ['organism', 'cell_type', 'crispr_type', 'cancer_type', 'condition', 'perturbation_name']\n",
    "    for field in required_fields:\n",
    "        if field not in adata.obs.columns:\n",
    "            print(f\"Warning: Required field '{field}' is missing. Adding placeholder values.\")\n",
    "            adata.obs[field] = 'Unknown'\n",
    "    \n",
    "    # Reset the main data matrix to raw counts (no normalization or log transform)\n",
    "    adata.X = adata.layers['raw_counts'].copy()\n",
    "    \n",
    "    # Save the harmonized dataset\n",
    "    print(f\"Saving harmonized dataset to {output_file}\")\n",
    "    adata.write(output_file)\n",
    "    \n",
    "    print(f\"Harmonization complete. Dataset contains {adata.n_obs} cells and {adata.n_vars} genes.\")\n",
    "    print(f\"Perturbation types: {adata.obs['perturbation_name'].unique()}\")\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "def run_harmonization(data_dir=os.getcwd(), sample_size=10000):\n",
    "    \"\"\"\n",
    "    Run the harmonization process.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_dir (str): Directory to store and process the data.\n",
    "    - sample_size (int): Number of cells to sample. Use 0 or None for all cells.\n",
    "    \"\"\"\n",
    "    if sample_size == 0:\n",
    "        sample_size = None\n",
    "    output_file = harmonize_dataset(data_dir, sample_size=sample_size)\n",
    "    print(f\"Harmonization complete. Output file: {output_file}\")\n",
    "\n",
    "# Example usage:\n",
    "# Change the data_dir to a desired directory or leave it as the current working directory.\n",
    "# Set sample_size to the number of cells you want to process (0 for all cells).\n",
    "run_harmonization(data_dir=\"/content/raw_data\", sample_size=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703ca59a-be42-409e-b477-a3a0fbea9dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import numpy as np\n",
    "\n",
    "# Load the h5ad file\n",
    "adata = sc.read(\"/content/raw_data/GSE247598_harmonized.h5ad\")\n",
    "print(f\"Total cells before filtering: {adata.n_obs}\")\n",
    "\n",
    "# Calculate QC metrics\n",
    "sc.pp.calculate_qc_metrics(adata, inplace=True)\n",
    "\n",
    "# Define QC thresholds based on the paper\n",
    "min_genes = 200\n",
    "max_genes = 7500\n",
    "max_mt_pct = 15.0\n",
    "\n",
    "# Filter cells based on gene count\n",
    "adata_filtered = adata[adata.obs.n_genes_by_counts >= min_genes]\n",
    "print(f\"Cells after min gene filter (≥ {min_genes} genes): {adata_filtered.n_obs}\")\n",
    "\n",
    "adata_filtered = adata_filtered[adata_filtered.obs.n_genes_by_counts <= max_genes]\n",
    "print(f\"Cells after max gene filter (≤ {max_genes} genes): {adata_filtered.n_obs}\")\n",
    "\n",
    "# Filter based on mitochondrial percentage\n",
    "# First identify mitochondrial genes (assumes they start with 'MT-' or 'mt-')\n",
    "adata_filtered.var['mt'] = adata_filtered.var_names.str.startswith(('MT-', 'mt-'))\n",
    "sc.pp.calculate_qc_metrics(adata_filtered, qc_vars=['mt'], inplace=True)\n",
    "\n",
    "adata_filtered = adata_filtered[adata_filtered.obs.pct_counts_mt <= max_mt_pct]\n",
    "print(f\"Cells after mitochondrial filter (≤ {max_mt_pct}% MT reads): {adata_filtered.n_obs}\")\n",
    "\n",
    "# Filter multiplets if guide information is available\n",
    "if 'guide_count' in adata_filtered.obs.columns:\n",
    "    adata_filtered = adata_filtered[adata_filtered.obs.guide_count == 1]\n",
    "    print(f\"Cells after filtering out multiplets: {adata_filtered.n_obs}\")\n",
    "\n",
    "print(f\"Final number of cells after QC: {adata_filtered.n_obs}\")\n",
    "print(f\"Total cells removed: {adata.n_obs - adata_filtered.n_obs}\")\n",
    "print(f\"Percentage of cells kept: {adata_filtered.n_obs/adata.n_obs*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
