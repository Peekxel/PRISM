{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703ca59a-be42-409e-b477-a3a0fbea9dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GSE120861 Dataset Harmonizer for Jupyter Notebook\n",
    "\n",
    "This notebook cell processes the GSE120861 dataset and harmonizes it into h5ad format.\n",
    "It downloads the required files (if not already present), processes metadata, and (optionally)\n",
    "downloads and processes a large expression matrix.\n",
    "\n",
    "Usage:\n",
    "    Call run_harmonizer() with optional parameters:\n",
    "      - data_dir: Directory to store/download files (default: 'GSE120861')\n",
    "      - download_expr_matrix: Set to True to download the large expression matrix automatically,\n",
    "                              False to skip, or leave as None to be prompted.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "import urllib.request\n",
    "import subprocess\n",
    "import anndata\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from scipy import sparse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# URLs for downloading the dataset files\n",
    "FILE_URLS = {\n",
    "    'at_scale_screen.cells.txt.gz': 'https://ftp.ncbi.nlm.nih.gov/geo/series/GSE120nnn/GSE120861/suppl/GSE120861_at_scale_screen.cells.txt.gz',\n",
    "    'at_scale_screen.genes.txt.gz': 'https://ftp.ncbi.nlm.nih.gov/geo/series/GSE120nnn/GSE120861/suppl/GSE120861_at_scale_screen.genes.txt.gz',\n",
    "    'at_scale_screen.phenoData.txt.gz': 'https://ftp.ncbi.nlm.nih.gov/geo/series/GSE120nnn/GSE120861/suppl/GSE120861_at_scale_screen.phenoData.txt.gz',\n",
    "    'grna_groups.at_scale.txt.gz': 'https://ftp.ncbi.nlm.nih.gov/geo/series/GSE120nnn/GSE120861/suppl/GSE120861_grna_groups.at_scale.txt.gz',\n",
    "    'gene_gRNAgroup_pair_table.at_scale.txt.gz': 'https://ftp.ncbi.nlm.nih.gov/geo/series/GSE120nnn/GSE120861/suppl/GSE120861_gene_gRNAgroup_pair_table.at_scale.txt.gz'\n",
    "}\n",
    "\n",
    "# Optional large expression matrix file (8.9GB)\n",
    "EXPRESSION_MATRIX_URL = 'https://ftp.ncbi.nlm.nih.gov/geo/series/GSE120nnn/GSE120861/suppl/GSE120861_at_scale_screen.exprs.mtx.gz'\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"Download a file from a URL to a destination path using wget (or urllib as fallback).\"\"\"\n",
    "    print(f\"Downloading {url} to {destination}...\")\n",
    "    try:\n",
    "        subprocess.run(['wget', '-q', '--show-progress', '-O', destination, url], check=True)\n",
    "        print(f\"Downloaded {destination}\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"Failed to download with wget. Trying urllib...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, destination)\n",
    "            print(f\"Downloaded {destination}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {url}: {e}\")\n",
    "            raise\n",
    "\n",
    "def ensure_files_exist(data_dir):\n",
    "    \"\"\"Ensure all required files exist, downloading them if necessary.\"\"\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    for filename, url in FILE_URLS.items():\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        if not os.path.exists(file_path):\n",
    "            download_file(url, file_path)\n",
    "        else:\n",
    "            print(f\"File {filename} already exists.\")\n",
    "\n",
    "def read_gzipped_text(file_path):\n",
    "    \"\"\"Read a gzipped text file line by line.\"\"\"\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        return [line.strip() for line in f]\n",
    "\n",
    "def read_mtx_file(file_path):\n",
    "    \"\"\"Read a Matrix Market file into a sparse matrix.\"\"\"\n",
    "    print(f\"Reading matrix file {file_path}...\")\n",
    "    with open(file_path, 'r') as f:\n",
    "        # Skip header comments and read dimensions\n",
    "        while True:\n",
    "            line = f.readline().strip()\n",
    "            if not line.startswith('%'):\n",
    "                dimensions = line.split()\n",
    "                n_genes, n_cells, n_entries = int(dimensions[0]), int(dimensions[1]), int(dimensions[2])\n",
    "                break\n",
    "    \n",
    "    data = []\n",
    "    row_indices = []\n",
    "    col_indices = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        # Skip header\n",
    "        line = f.readline()\n",
    "        while line.startswith('%'):\n",
    "            line = f.readline()\n",
    "        # Skip dimensions line\n",
    "        line = f.readline()\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            row_indices.append(int(parts[0]) - 1)  # convert to 0-based index\n",
    "            col_indices.append(int(parts[1]) - 1)\n",
    "            data.append(int(parts[2]))\n",
    "    \n",
    "    # Create sparse matrix (cells x genes)\n",
    "    matrix = sparse.csr_matrix((data, (row_indices, col_indices)), shape=(n_genes, n_cells))\n",
    "    return matrix.T  # transpose to have cells as rows\n",
    "\n",
    "def parse_phenodata(file_path):\n",
    "    \"\"\"Parse the phenoData file to extract cell metadata.\"\"\"\n",
    "    print(f\"Parsing phenoData file {file_path}...\")\n",
    "    columns = [\n",
    "        'sample_id', 'cell_id', 'umi_count', 'size_factor', \n",
    "        'perturbations', 'perturbation_ids', 'grna_sequences',\n",
    "        'detected_genes', 'detected_umis', 'frip', 'n_grnas',\n",
    "        'batch', 'grna_library', 'prep_batch', 'chip_batch', \n",
    "        'lane_batch', 'doublet_score'\n",
    "    ]\n",
    "    rows = []\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= len(columns):\n",
    "                rows.append(parts[:len(columns)])\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    numeric_cols = ['umi_count', 'size_factor', 'detected_genes', 'detected_umis', \n",
    "                    'frip', 'n_grnas', 'doublet_score']\n",
    "    for col in numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    return df\n",
    "\n",
    "def parse_grna_groups(file_path):\n",
    "    \"\"\"Parse the gRNA groups file to understand perturbation types.\"\"\"\n",
    "    print(f\"Parsing gRNA groups file {file_path}...\")\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "    df.columns = ['group_name', 'sequence']\n",
    "    df['category'] = 'unknown'\n",
    "    df.loc[df['group_name'].str.contains('TSS'), 'category'] = 'TSS'\n",
    "    df.loc[df['group_name'].str.contains('NTC'), 'category'] = 'NTC'\n",
    "    df.loc[df['group_name'].str.contains('chr'), 'category'] = 'enhancer'\n",
    "    return df\n",
    "\n",
    "def parse_gene_grna_pairs(file_path):\n",
    "    \"\"\"Parse the gene-gRNA pair table to understand gene-perturbation relationships.\"\"\"\n",
    "    print(f\"Parsing gene-gRNA pair table {file_path}...\")\n",
    "    df = pd.read_csv(file_path, sep='\\t', low_memory=False)\n",
    "    return df\n",
    "\n",
    "def extract_perturbation_info(phenodata_df, grna_groups_df):\n",
    "    \"\"\"Extract perturbation information from phenoData and gRNA groups.\"\"\"\n",
    "    print(\"Extracting perturbation information...\")\n",
    "    # Map group names to categories\n",
    "    group_to_category = dict(zip(grna_groups_df['group_name'], grna_groups_df['category']))\n",
    "    results = []\n",
    "    for _, row in phenodata_df.iterrows():\n",
    "        cell_id = row['cell_id']\n",
    "        perturbations = row['perturbations'].split('_')\n",
    "        # Extract TSS perturbations\n",
    "        tss_targets = []\n",
    "        i = 0\n",
    "        while i < len(perturbations):\n",
    "            if i+1 < len(perturbations) and perturbations[i+1] == 'TSS':\n",
    "                tss_targets.append(perturbations[i])\n",
    "                i += 2\n",
    "            else:\n",
    "                i += 1\n",
    "        # Extract enhancer perturbations\n",
    "        enhancer_targets = []\n",
    "        i = 0\n",
    "        while i < len(perturbations):\n",
    "            if i+2 < len(perturbations) and perturbations[i].startswith('chr') and perturbations[i+1] in ['top', 'second'] and perturbations[i+2] == 'two':\n",
    "                enhancer_targets.append(perturbations[i])\n",
    "                i += 3\n",
    "            else:\n",
    "                i += 1\n",
    "        is_control = len(tss_targets) == 0 and len(enhancer_targets) == 0\n",
    "        result = {\n",
    "            'cell_id': cell_id,\n",
    "            'tss_targets': ','.join(tss_targets) if tss_targets else 'None',\n",
    "            'enhancer_targets': ','.join(enhancer_targets) if enhancer_targets else 'None',\n",
    "            'is_control': is_control,\n",
    "            'n_perturbations': len(tss_targets) + len(enhancer_targets),\n",
    "            'perturbation_type': 'None' if is_control else ('TSS' if len(enhancer_targets) == 0 else ('enhancer' if len(tss_targets) == 0 else 'mixed'))\n",
    "        }\n",
    "        results.append(result)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def create_harmonized_metadata(phenodata_df, perturbation_df, gene_grna_pairs_df):\n",
    "    \"\"\"Create harmonized metadata according to the specified format.\"\"\"\n",
    "    print(\"Creating harmonized metadata...\")\n",
    "    metadata = pd.merge(phenodata_df, perturbation_df, on='cell_id')\n",
    "    metadata['organism'] = 'Homo sapiens'\n",
    "    metadata['cell_type'] = 'K562'\n",
    "    metadata['crispr_type'] = 'CRISPRi'\n",
    "    metadata['cancer_type'] = 'Chronic myelogenous leukemia'\n",
    "    metadata['condition'] = metadata['is_control'].map({True: 'Control', False: 'Test'})\n",
    "    \n",
    "    def format_perturbation_name(row):\n",
    "        if row['is_control']:\n",
    "            return 'Non-targeting'\n",
    "        if row['perturbation_type'] == 'TSS':\n",
    "            return row['tss_targets']\n",
    "        elif row['perturbation_type'] == 'enhancer':\n",
    "            return row['enhancer_targets']\n",
    "        else:\n",
    "            return f\"{row['tss_targets']} + {row['enhancer_targets']}\"\n",
    "    \n",
    "    metadata['perturbation_name'] = metadata.apply(format_perturbation_name, axis=1)\n",
    "    return metadata\n",
    "\n",
    "def create_h5ad_metadata_only(data_dir, output_file):\n",
    "    \"\"\"Create an h5ad file with metadata only (no expression matrix).\"\"\"\n",
    "    print(f\"Creating h5ad metadata file {output_file}...\")\n",
    "    ensure_files_exist(data_dir)\n",
    "    cells_file = os.path.join(data_dir, 'at_scale_screen.cells.txt.gz')\n",
    "    genes_file = os.path.join(data_dir, 'at_scale_screen.genes.txt.gz')\n",
    "    \n",
    "    cells = read_gzipped_text(cells_file)\n",
    "    genes = read_gzipped_text(genes_file)\n",
    "    print(f\"Found {len(cells)} cells and {len(genes)} genes\")\n",
    "    \n",
    "    phenodata_file = os.path.join(data_dir, 'at_scale_screen.phenoData.txt.gz')\n",
    "    grna_groups_file = os.path.join(data_dir, 'grna_groups.at_scale.txt.gz')\n",
    "    gene_grna_pairs_file = os.path.join(data_dir, 'gene_gRNAgroup_pair_table.at_scale.txt.gz')\n",
    "    \n",
    "    phenodata_df = parse_phenodata(phenodata_file)\n",
    "    grna_groups_df = parse_grna_groups(grna_groups_file)\n",
    "    gene_grna_pairs_df = parse_gene_grna_pairs(gene_grna_pairs_file)\n",
    "    \n",
    "    perturbation_df = extract_perturbation_info(phenodata_df, grna_groups_df)\n",
    "    metadata_df = create_harmonized_metadata(phenodata_df, perturbation_df, gene_grna_pairs_df)\n",
    "    \n",
    "    with h5py.File(output_file, 'w') as f:\n",
    "        obs_group = f.create_group('obs')\n",
    "        var_group = f.create_group('var')\n",
    "        for col in metadata_df.columns:\n",
    "            if col in ['organism', 'cell_type', 'crispr_type', 'cancer_type', 'condition', 'perturbation_name']:\n",
    "                obs_group.create_dataset(col, data=metadata_df[col].values.astype('S'))\n",
    "            else:\n",
    "                if metadata_df[col].dtype == 'object':\n",
    "                    obs_group.create_dataset(f\"original_{col}\", data=metadata_df[col].values.astype('S'))\n",
    "                else:\n",
    "                    obs_group.create_dataset(f\"original_{col}\", data=metadata_df[col].values)\n",
    "        obs_group.create_dataset('_index', data=np.array(cells, dtype='S'))\n",
    "        var_group.create_dataset('_index', data=np.array(genes, dtype='S'))\n",
    "        f.attrs['dataset_id'] = 'GSE120861'\n",
    "        f.attrs['title'] = 'A genome-wide framework for mapping gene regulation via cellular genetic screens'\n",
    "        f.attrs['description'] = 'CRISPRi screen in K562 cells targeting candidate enhancers'\n",
    "        f.attrs['note'] = 'This file contains metadata only. Expression matrix not included due to size constraints.'\n",
    "    \n",
    "    print(f\"Created h5ad metadata file {output_file}\")\n",
    "\n",
    "def download_expression_matrix(data_dir):\n",
    "    \"\"\"Download and decompress the expression matrix file.\"\"\"\n",
    "    print(\"Downloading expression matrix file...\")\n",
    "    exprs_gz_file = os.path.join(data_dir, 'at_scale_screen.exprs.mtx.gz')\n",
    "    exprs_file = os.path.join(data_dir, 'at_scale_screen.exprs.mtx')\n",
    "    \n",
    "    # Download if neither the gzipped nor decompressed file exists.\n",
    "    if not os.path.exists(exprs_gz_file) and not os.path.exists(exprs_file):\n",
    "        download_file(EXPRESSION_MATRIX_URL, exprs_gz_file)\n",
    "    else:\n",
    "        print(\"Expression matrix file already exists.\")\n",
    "    \n",
    "    # If the decompressed file doesn't exist, check if decompression is needed.\n",
    "    if not os.path.exists(exprs_file):\n",
    "        # Open the file in binary mode and check the first two bytes.\n",
    "        with open(exprs_gz_file, 'rb') as f:\n",
    "            magic = f.read(2)\n",
    "        if magic == b'\\x1f\\x8b':\n",
    "            print(f\"Decompressing {exprs_gz_file}...\")\n",
    "            try:\n",
    "                with gzip.open(exprs_gz_file, 'rb') as f_in:\n",
    "                    with open(exprs_file, 'wb') as f_out:\n",
    "                        f_out.write(f_in.read())\n",
    "                print(f\"Decompressed to {exprs_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error decompressing file: {e}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"File {exprs_gz_file} is not gzipped. Assuming it is already in plain text.\")\n",
    "            # If you want to rename it to the expected filename:\n",
    "            os.rename(exprs_gz_file, exprs_file)\n",
    "    return True\n",
    "\n",
    "\n",
    "def create_full_h5ad(data_dir, metadata_file, output_file):\n",
    "    \"\"\"Create a full h5ad file with expression matrix using an AnnData object.\"\"\"\n",
    "    print(f\"Creating full h5ad file {output_file}...\")\n",
    "    \n",
    "    # Path to expression matrix file\n",
    "    exprs_file = os.path.join(data_dir, 'at_scale_screen.exprs.mtx')\n",
    "    if not os.path.exists(exprs_file):\n",
    "        print(f\"Expression matrix file {exprs_file} not found. Please download it first.\")\n",
    "        return False\n",
    "\n",
    "    # Read metadata from the metadata file\n",
    "    with h5py.File(metadata_file, 'r') as f:\n",
    "        # Read cell and gene names\n",
    "        cells = [s.decode('utf-8') for s in f['obs']['_index'][:]]\n",
    "        genes = [s.decode('utf-8') for s in f['var']['_index'][:]]\n",
    "        \n",
    "        # Read harmonized metadata columns\n",
    "        harmonized_keys = ['organism', 'cell_type', 'crispr_type', 'cancer_type', 'condition', 'perturbation_name']\n",
    "        obs_data = {}\n",
    "        for key in harmonized_keys:\n",
    "            obs_data[key] = [s.decode('utf-8') for s in f['obs'][key][:]]\n",
    "    \n",
    "    # Read the expression matrix (as a sparse matrix)\n",
    "    matrix = read_mtx_file(exprs_file)\n",
    "    \n",
    "    # Create observation (obs) and variable (var) dataframes\n",
    "    obs_df = pd.DataFrame(obs_data, index=cells)\n",
    "    var_df = pd.DataFrame(index=genes)\n",
    "    \n",
    "    # Create AnnData object\n",
    "    adata = anndata.AnnData(X=matrix, obs=obs_df, var=var_df)\n",
    "    \n",
    "    # Optionally, add additional info to the uns field\n",
    "    adata.uns['dataset_id'] = 'GSE120861'\n",
    "    adata.uns['title'] = 'A genome-wide framework for mapping gene regulation via cellular genetic screens'\n",
    "    adata.uns['description'] = 'CRISPRi screen in K562 cells targeting candidate enhancers'\n",
    "    \n",
    "    # Write the AnnData object to an h5ad file (in the proper format)\n",
    "    adata.write_h5ad(output_file)\n",
    "    print(f\"Created full h5ad file {output_file}\")\n",
    "    return True\n",
    "\n",
    "def run_harmonizer(data_dir='GSE120861', download_expr_matrix=None):\n",
    "    \"\"\"\n",
    "    Run the harmonization process.\n",
    "    \n",
    "    Parameters:\n",
    "      data_dir (str): Directory for dataset files (default: 'GSE120861')\n",
    "      download_expr_matrix (bool or None): \n",
    "          True to download the large expression matrix automatically,\n",
    "          False to skip,\n",
    "          None to prompt the user.\n",
    "    \"\"\"\n",
    "    metadata_file = os.path.join(data_dir, 'GSE120861_harmonized_metadata.h5ad')\n",
    "    full_h5ad_file = os.path.join(data_dir, 'GSE120861_harmonized.h5ad')\n",
    "    \n",
    "    create_h5ad_metadata_only(data_dir, metadata_file)\n",
    "    \n",
    "    if download_expr_matrix is None:\n",
    "        choice = input(\"\\nThe expression matrix file is large (8.9GB). Do you want to download it? (y/n): \").strip().lower()\n",
    "        download_expr_matrix = (choice == 'y')\n",
    "    \n",
    "    if download_expr_matrix:\n",
    "        if download_expression_matrix(data_dir):\n",
    "            create_full_h5ad(data_dir, metadata_file, full_h5ad_file)\n",
    "    else:\n",
    "        print(\"\\nSkipping expression matrix download.\")\n",
    "        print(\"To download and process the expression matrix later, run:\")\n",
    "        print(f\"!wget -O {data_dir}/at_scale_screen.exprs.mtx.gz {EXPRESSION_MATRIX_URL}\")\n",
    "        print(f\"!gunzip {data_dir}/at_scale_screen.exprs.mtx.gz\")\n",
    "        print(f\"Then, call create_full_h5ad('{data_dir}', '{metadata_file}', '{full_h5ad_file}')\")\n",
    "\n",
    "# Run the harmonizer (adjust parameters as needed)\n",
    "run_harmonizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c7cc03-d531-4183-9026-f55891e4776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your input and output file paths\n",
    "input_h5ad = \"GSE120861/GSE120861_harmonized.h5ad\"      # Path to your original h5ad file\n",
    "output_h5ad = \"filtered_output.h5ad\"  # Update with your desired output path\n",
    "\n",
    "import anndata\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the AnnData object\n",
    "try:\n",
    "    adata = anndata.read_h5ad(input_h5ad)\n",
    "    print(f\"Loaded h5ad file with {adata.n_obs} cells.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error loading {input_h5ad}: {e}\")\n",
    "\n",
    "# Ensure the 'perturbation_name' column exists\n",
    "if \"perturbation_name\" not in adata.obs.columns:\n",
    "    raise ValueError(\"'perturbation_name' column not found in the AnnData object.\")\n",
    "\n",
    "def clean_perturbation(name):\n",
    "    \"\"\"\n",
    "    Splits the perturbation name on commas or plus signs (with optional spaces),\n",
    "    removes tokens that start with 'chr' (case-insensitive), and rejoins the remaining tokens with a plus sign.\n",
    "    Returns an empty string if no token remains.\n",
    "    \"\"\"\n",
    "    # Split on commas or plus signs with optional whitespace\n",
    "    tokens = re.split(r'\\s*\\+\\s*|,', name)\n",
    "    # Remove extra spaces and empty tokens\n",
    "    tokens = [token.strip() for token in tokens if token.strip()]\n",
    "    # Remove tokens that start with 'chr' (case-insensitive)\n",
    "    tokens = [token for token in tokens if not token.lower().startswith(\"chr\")]\n",
    "    return \"+\".join(tokens) if tokens else \"\"\n",
    "\n",
    "# Apply cleaning function to the 'perturbation_name' column\n",
    "adata.obs[\"perturbation_name\"] = adata.obs[\"perturbation_name\"].apply(clean_perturbation)\n",
    "\n",
    "# Print unique perturbation names after cleaning\n",
    "print(\"Unique perturbation names after cleaning:\")\n",
    "print(adata.obs[\"perturbation_name\"].unique())\n",
    "\n",
    "# Remove cells that have an empty perturbation name\n",
    "initial_count = adata.n_obs\n",
    "adata = adata[adata.obs[\"perturbation_name\"] != \"\"].copy()\n",
    "filtered_count = adata.n_obs\n",
    "print(f\"Removed {initial_count - filtered_count} cells with an empty perturbation name; {filtered_count} cells remain.\")\n",
    "\n",
    "# Save the modified AnnData object\n",
    "try:\n",
    "    adata.write_h5ad(output_h5ad)\n",
    "    print(f\"Modified h5ad file saved to: {output_h5ad}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error saving {output_h5ad}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
