{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eeafb4-c1c2-41f2-95c9-3817cf6e50d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install any required packages if not already installed (uncomment if needed)\n",
    "# !pip install anndata requests bs4 tqdm scipy pandas numpy\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import gzip\n",
    "import re\n",
    "import gc\n",
    "import requests\n",
    "import tarfile\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as sp\n",
    "from tqdm import tqdm\n",
    "import anndata\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Constants\n",
    "GEO_ACCESSION = \"GSE273271\"\n",
    "GEO_URL = f\"https://www.ncbi.nlm.nih.gov/geo/download/?acc={GEO_ACCESSION}&format=file\"\n",
    "DOWNLOAD_FILENAME = f\"{GEO_ACCESSION}_RAW.tar\"\n",
    "\n",
    "def download_data(data_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Download the dataset if it doesn't exist.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory to download the data to\n",
    "    \"\"\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    tar_path = os.path.join(data_dir, DOWNLOAD_FILENAME)\n",
    "    \n",
    "    if not os.path.exists(tar_path):\n",
    "        print(f\"Downloading {GEO_ACCESSION} dataset...\")\n",
    "        response = requests.get(GEO_URL, stream=True)\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        block_size = 1024  # 1 Kibibyte\n",
    "        \n",
    "        with open(tar_path, 'wb') as file:\n",
    "            with tqdm(total=total_size, unit='iB', unit_scale=True) as progress_bar:\n",
    "                for data in response.iter_content(block_size):\n",
    "                    progress_bar.update(len(data))\n",
    "                    file.write(data)\n",
    "        \n",
    "        print(f\"Download complete: {tar_path}\")\n",
    "    else:\n",
    "        print(f\"Dataset already downloaded: {tar_path}\")\n",
    "    \n",
    "    # Extract the tar file if needed\n",
    "    sample_files = glob.glob(os.path.join(data_dir, \"GSM*_*_barcodes.tsv.gz\"))\n",
    "    if not sample_files:\n",
    "        print(f\"Extracting {tar_path}...\")\n",
    "        with tarfile.open(tar_path) as tar:\n",
    "            tar.extractall(path=data_dir)\n",
    "        print(\"Extraction complete.\")\n",
    "    else:\n",
    "        print(\"Files already extracted.\")\n",
    "\n",
    "def get_sample_metadata(sample_ids: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract metadata for each sample from the sample IDs and titles.\n",
    "    \n",
    "    Args:\n",
    "        sample_ids: List of sample IDs (GSM numbers)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with sample metadata\n",
    "    \"\"\"\n",
    "    sample_metadata = []\n",
    "    \n",
    "    for sample_id in tqdm(sample_ids, desc=\"Fetching sample metadata\"):\n",
    "        # Find the corresponding barcode file to get the sample name\n",
    "        barcode_file = glob.glob(f\"{sample_id}_*_barcodes.tsv.gz\")[0]\n",
    "        sample_name = os.path.basename(barcode_file).split('_')[1]\n",
    "        \n",
    "        # Get the title from the GEO website\n",
    "        url = f\"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc={sample_id}\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            title_td = soup.find('td', string='Title')\n",
    "            if title_td:\n",
    "                title = title_td.find_next('td').text.strip()\n",
    "            else:\n",
    "                title = \"\"\n",
    "                \n",
    "            # Extract metadata from title\n",
    "            time_point_match = re.search(r'E(\\d+\\.\\d+|\\d+)', title)\n",
    "            time_point = time_point_match.group(0) if time_point_match else None\n",
    "            \n",
    "            genotype_match = re.search(r'(wild type|Chd8 het)', title, re.IGNORECASE)\n",
    "            genotype = genotype_match.group(0) if genotype_match else None\n",
    "            \n",
    "            sex_match = re.search(r'(male|female)', title, re.IGNORECASE)\n",
    "            sex = sex_match.group(0) if sex_match else None\n",
    "            \n",
    "            rep_match = re.search(r'rep(\\d+)', title)\n",
    "            rep = rep_match.group(1) if rep_match else None\n",
    "            \n",
    "            sample_metadata.append({\n",
    "                'sample_id': sample_id,\n",
    "                'sample_name': sample_name,\n",
    "                'title': title,\n",
    "                'time_point': time_point,\n",
    "                'genotype': genotype,\n",
    "                'sex': sex,\n",
    "                'replicate': rep\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {sample_id}: {e}\")\n",
    "            sample_metadata.append({\n",
    "                'sample_id': sample_id,\n",
    "                'sample_name': sample_name\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(sample_metadata)\n",
    "\n",
    "def read_10x_data(matrix_file: str, features_file: str, barcodes_file: str) -> Tuple[sp.csr_matrix, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Read 10X Genomics data files.\n",
    "    \n",
    "    Args:\n",
    "        matrix_file: Path to the matrix.mtx.gz file\n",
    "        features_file: Path to the features.tsv.gz file\n",
    "        barcodes_file: Path to the barcodes.tsv.gz file\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        - Sparse matrix of gene expression data\n",
    "        - Array of gene IDs\n",
    "        - Array of gene names\n",
    "        - Array of cell barcodes\n",
    "    \"\"\"\n",
    "    # Read the matrix\n",
    "    matrix = sio.mmread(matrix_file).T.tocsr()\n",
    "    \n",
    "    # Read features (genes)\n",
    "    with gzip.open(features_file, 'rt') as f:\n",
    "        features_df = pd.read_csv(f, sep='\\t', header=None)\n",
    "        gene_ids = features_df[0].values\n",
    "        gene_names = features_df[1].values\n",
    "    \n",
    "    # Read barcodes (cells)\n",
    "    with gzip.open(barcodes_file, 'rt') as f:\n",
    "        barcodes = pd.read_csv(f, sep='\\t', header=None)[0].values\n",
    "    \n",
    "    return matrix, gene_ids, gene_names, barcodes\n",
    "\n",
    "def process_sample(sample_id: str, metadata_df: pd.DataFrame) -> anndata.AnnData:\n",
    "    \"\"\"\n",
    "    Process a single sample and create an AnnData object.\n",
    "    \n",
    "    Args:\n",
    "        sample_id: Sample ID (GSM number)\n",
    "        metadata_df: DataFrame with sample metadata\n",
    "        \n",
    "    Returns:\n",
    "        AnnData object for the sample\n",
    "    \"\"\"\n",
    "    # Find the files for this sample\n",
    "    barcode_file = glob.glob(f\"{sample_id}_*_barcodes.tsv.gz\")[0]\n",
    "    features_file = glob.glob(f\"{sample_id}_*_features.tsv.gz\")[0]\n",
    "    matrix_file = glob.glob(f\"{sample_id}_*_matrix.mtx.gz\")[0]\n",
    "    \n",
    "    # Read the data\n",
    "    matrix, gene_ids, gene_names, barcodes = read_10x_data(matrix_file, features_file, barcode_file)\n",
    "    \n",
    "    # Create AnnData object with explicit dtype to avoid warnings\n",
    "    adata = anndata.AnnData(\n",
    "        X=matrix,\n",
    "        var=pd.DataFrame(index=gene_names),\n",
    "        obs=pd.DataFrame(index=barcodes),\n",
    "        dtype=np.float32\n",
    "    )\n",
    "    \n",
    "    # Make variable names unique\n",
    "    adata.var_names_make_unique()\n",
    "    \n",
    "    # Add gene IDs as a column in var\n",
    "    adata.var['gene_ids'] = gene_ids\n",
    "    \n",
    "    # Get sample metadata\n",
    "    sample_meta = metadata_df[metadata_df['sample_id'] == sample_id].iloc[0].to_dict()\n",
    "    \n",
    "    # Add sample metadata to each cell\n",
    "    for key, value in sample_meta.items():\n",
    "        if key not in ['sample_id', 'sample_name', 'title']:\n",
    "            adata.obs[key] = value\n",
    "    \n",
    "    # Add sample ID and name\n",
    "    adata.obs['sample_id'] = sample_id\n",
    "    adata.obs['sample_name'] = sample_meta['sample_name']\n",
    "    \n",
    "    return adata\n",
    "\n",
    "def harmonize_metadata(adata: anndata.AnnData) -> anndata.AnnData:\n",
    "    \"\"\"\n",
    "    Harmonize metadata according to the required format.\n",
    "    \n",
    "    Args:\n",
    "        adata: AnnData object with raw metadata\n",
    "        \n",
    "    Returns:\n",
    "        AnnData object with harmonized metadata\n",
    "    \"\"\"\n",
    "    # Create standardized metadata fields\n",
    "    \n",
    "    # 1. organism\n",
    "    adata.obs['organism'] = 'Mus musculus'\n",
    "    \n",
    "    # 2. cell_type\n",
    "    adata.obs['cell_type'] = 'Neural Cells'  # Default\n",
    "    \n",
    "    # 3. crispr_type\n",
    "    adata.obs['crispr_type'] = 'None'\n",
    "    \n",
    "    # 4. cancer_type\n",
    "    adata.obs['cancer_type'] = 'Non-Cancer'\n",
    "    \n",
    "    # 5. condition: Map genotype to condition\n",
    "    condition_map = {\n",
    "        'wild type': 'Control',\n",
    "        'Chd8 het': 'Test'\n",
    "    }\n",
    "    adata.obs['condition'] = adata.obs['genotype'].map(condition_map)\n",
    "    \n",
    "    # 6. perturbation_name\n",
    "    adata.obs['perturbation_name'] = 'None'\n",
    "    adata.obs.loc[adata.obs['genotype'] == 'Chd8 het', 'perturbation_name'] = 'Chd8'\n",
    "    \n",
    "    return adata\n",
    "\n",
    "def process_and_save_individual_samples(sample_ids: List[str], metadata_df: pd.DataFrame, data_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Process each sample individually and save to separate files.\n",
    "    \n",
    "    Args:\n",
    "        sample_ids: List of sample IDs\n",
    "        metadata_df: DataFrame with sample metadata\n",
    "        data_dir: Directory to save the results\n",
    "    \"\"\"\n",
    "    # Create a directory for individual sample files\n",
    "    samples_dir = os.path.join(data_dir, \"individual_samples\")\n",
    "    os.makedirs(samples_dir, exist_ok=True)\n",
    "    \n",
    "    # Process each sample individually\n",
    "    sample_files = []\n",
    "    for i, sample_id in enumerate(tqdm(sample_ids, desc=\"Processing samples\")):\n",
    "        try:\n",
    "            # Process the sample\n",
    "            adata = process_sample(sample_id, metadata_df)\n",
    "            \n",
    "            # Harmonize metadata\n",
    "            adata = harmonize_metadata(adata)\n",
    "            \n",
    "            # Save this sample\n",
    "            sample_file = os.path.join(samples_dir, f\"{sample_id}.h5ad\")\n",
    "            adata.write_h5ad(sample_file)\n",
    "            sample_files.append(sample_file)\n",
    "            \n",
    "            # Clear memory\n",
    "            del adata\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {sample_id}: {e}\")\n",
    "    \n",
    "    # Create a combined metadata file without loading the full data\n",
    "    print(\"Creating combined metadata file...\")\n",
    "    \n",
    "    all_obs = []\n",
    "    all_var_names = set()\n",
    "    total_cells = 0\n",
    "    \n",
    "    for sample_file in tqdm(sample_files, desc=\"Collecting metadata\"):\n",
    "        adata = anndata.read_h5ad(sample_file, backed='r')\n",
    "        obs = adata.obs.copy()\n",
    "        obs['_sample_file'] = os.path.basename(sample_file)\n",
    "        all_obs.append(obs)\n",
    "        all_var_names.update(adata.var_names)\n",
    "        total_cells += adata.n_obs\n",
    "        del adata\n",
    "        gc.collect()\n",
    "    \n",
    "    combined_obs = pd.concat(all_obs, axis=0)\n",
    "    all_var_names = list(all_var_names)\n",
    "    \n",
    "    metadata_file = os.path.join(data_dir, f\"{GEO_ACCESSION}_metadata.csv\")\n",
    "    combined_obs.to_csv(metadata_file)\n",
    "    \n",
    "    var_names_file = os.path.join(data_dir, f\"{GEO_ACCESSION}_var_names.txt\")\n",
    "    with open(var_names_file, 'w') as f:\n",
    "        for var_name in all_var_names:\n",
    "            f.write(f\"{var_name}\\n\")\n",
    "    \n",
    "    summary_file = os.path.join(data_dir, f\"{GEO_ACCESSION}_summary.txt\")\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(f\"Dataset: {GEO_ACCESSION}\\n\")\n",
    "        f.write(f\"Total samples: {len(sample_files)}\\n\")\n",
    "        f.write(f\"Total cells: {total_cells}\\n\")\n",
    "        f.write(f\"Total genes: {len(all_var_names)}\\n\")\n",
    "        f.write(f\"Metadata fields: {list(combined_obs.columns)}\\n\")\n",
    "    \n",
    "    print(\"Processing complete!\")\n",
    "    print(f\"Individual sample files saved to: {samples_dir}\")\n",
    "    print(f\"Metadata saved to: {metadata_file}\")\n",
    "    print(f\"Variable names saved to: {var_names_file}\")\n",
    "    print(f\"Summary saved to: {summary_file}\")\n",
    "    print(f\"Total samples: {len(sample_files)}\")\n",
    "    print(f\"Total cells: {total_cells}\")\n",
    "    print(f\"Total genes: {len(all_var_names)}\")\n",
    "    print(f\"Metadata fields: {list(combined_obs.columns)}\")\n",
    "\n",
    "def main(data_dir: str = None) -> None:\n",
    "    \"\"\"\n",
    "    Main function to process and harmonize the dataset.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing the data files. If None, uses the current directory.\n",
    "    \"\"\"\n",
    "    if data_dir is None:\n",
    "        data_dir = os.getcwd()\n",
    "    \n",
    "    # Download and extract data if needed\n",
    "    download_data(data_dir)\n",
    "    \n",
    "    # Change to data directory\n",
    "    os.chdir(data_dir)\n",
    "    \n",
    "    # Get all sample files\n",
    "    barcode_files = glob.glob(\"GSM*_*_barcodes.tsv.gz\")\n",
    "    sample_ids = [os.path.basename(f).split('_')[0] for f in barcode_files]\n",
    "    sample_ids = list(set(sample_ids))\n",
    "    \n",
    "    print(f\"Found {len(sample_ids)} samples\")\n",
    "    \n",
    "    # Get sample metadata\n",
    "    metadata_df = get_sample_metadata(sample_ids)\n",
    "    print(\"Sample metadata:\")\n",
    "    print(metadata_df[['sample_id', 'time_point', 'genotype', 'sex', 'replicate']].head())\n",
    "    \n",
    "    # Process each sample individually to manage memory usage\n",
    "    process_and_save_individual_samples(sample_ids, metadata_df, data_dir)\n",
    "\n",
    "# To run the script in Jupyter, simply call main() with your desired data directory.\n",
    "# For example, to use a folder named 'data' in the current directory:\n",
    "data_dir = \"data\"  # Change this as needed, or set to None to use the current directory\n",
    "main(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703ca59a-be42-409e-b477-a3a0fbea9dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import anndata\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Constants\n",
    "GEO_ACCESSION = \"GSE273271\"\n",
    "\n",
    "def combine_samples(data_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Combine individual sample h5ad files into a single h5ad file.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing the individual sample h5ad files\n",
    "    \"\"\"\n",
    "    # Path to the directory containing individual sample files\n",
    "    samples_dir = os.path.join(data_dir, \"individual_samples\")\n",
    "    \n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(samples_dir):\n",
    "        print(f\"Error: Directory {samples_dir} does not exist.\")\n",
    "        print(\"Please run the harmonize_data notebook cell first to create individual sample files.\")\n",
    "        return\n",
    "    \n",
    "    # Get all sample files\n",
    "    sample_files = glob.glob(os.path.join(samples_dir, \"GSM*.h5ad\"))\n",
    "    \n",
    "    if not sample_files:\n",
    "        print(f\"Error: No sample files found in {samples_dir}.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(sample_files)} sample files.\")\n",
    "    \n",
    "    # Load metadata\n",
    "    metadata_file = os.path.join(data_dir, f\"{GEO_ACCESSION}_metadata.csv\")\n",
    "    if os.path.exists(metadata_file):\n",
    "        print(f\"Loading metadata from {metadata_file}...\")\n",
    "        metadata = pd.read_csv(metadata_file, index_col=0)\n",
    "    else:\n",
    "        print(f\"Warning: Metadata file {metadata_file} not found.\")\n",
    "        metadata = None\n",
    "    \n",
    "    # Combine samples in batches to manage memory\n",
    "    print(\"Combining samples...\")\n",
    "    combined_adata = None\n",
    "    batch_size = 4  # adjust the batch size if needed\n",
    "    \n",
    "    for i in range(0, len(sample_files), batch_size):\n",
    "        batch_files = sample_files[i:i+batch_size]\n",
    "        batch_num = i // batch_size + 1\n",
    "        \n",
    "        print(f\"Processing batch {batch_num}/{(len(sample_files) + batch_size - 1) // batch_size}\")\n",
    "        \n",
    "        # Load and combine samples in this batch\n",
    "        batch_adatas = []\n",
    "        for sample_file in tqdm(batch_files, desc=f\"Batch {batch_num}\"):\n",
    "            try:\n",
    "                adata = anndata.read_h5ad(sample_file)\n",
    "                batch_adatas.append(adata)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {sample_file}: {e}\")\n",
    "        \n",
    "        # Concatenate samples in this batch\n",
    "        if batch_adatas:\n",
    "            batch_adata = anndata.concat(batch_adatas, join='outer')\n",
    "            \n",
    "            # Combine with previous batches\n",
    "            if combined_adata is None:\n",
    "                combined_adata = batch_adata\n",
    "            else:\n",
    "                combined_adata = anndata.concat([combined_adata, batch_adata], join='outer')\n",
    "            \n",
    "            # Clear memory\n",
    "            del batch_adatas, batch_adata\n",
    "            gc.collect()\n",
    "    \n",
    "    # Make variable names unique\n",
    "    combined_adata.var_names_make_unique()\n",
    "    \n",
    "    # Save the combined dataset\n",
    "    output_file = os.path.join(data_dir, f\"{GEO_ACCESSION}_combined.h5ad\")\n",
    "    print(f\"Saving combined dataset to {output_file}...\")\n",
    "    combined_adata.write_h5ad(output_file)\n",
    "    \n",
    "    print(\"Combination complete!\")\n",
    "    print(f\"Final dataset shape: {combined_adata.shape}\")\n",
    "    print(f\"Metadata fields: {list(combined_adata.obs.columns)}\")\n",
    "\n",
    "def main(data_dir: str = None) -> None:\n",
    "    \"\"\"\n",
    "    Main function to combine individual sample h5ad files.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing the individual sample h5ad files\n",
    "    \"\"\"\n",
    "    # Set data directory\n",
    "    if data_dir is None:\n",
    "        data_dir = os.getcwd()\n",
    "    \n",
    "    combine_samples(data_dir)\n",
    "\n",
    "# To run the script in a Jupyter Notebook, simply call main() with your desired data directory.\n",
    "# For example, to use a folder named 'data' in the current directory:\n",
    "data_dir = \"data\"  # Change this as needed, or set to None to use the current directory\n",
    "main(data_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
