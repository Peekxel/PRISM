{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a30c09d-2469-48af-96d5-7c0cae4d0517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gzip\n",
    "import tarfile\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Constants\n",
    "GEO_ACCESSION = \"GSE280767\"\n",
    "GEO_URL = f\"https://www.ncbi.nlm.nih.gov/geo/download/?acc={GEO_ACCESSION}&format=file\"\n",
    "MAX_ENTRIES_PER_SAMPLE = 100000000  # Limit entries per sample for efficiency\n",
    "\n",
    "def download_dataset(data_dir):\n",
    "    \"\"\"Download the dataset if it doesn't exist.\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    data_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    tar_file = data_dir / f\"{GEO_ACCESSION}_RAW.tar\"\n",
    "    \n",
    "    if not tar_file.exists():\n",
    "        print(f\"Downloading {GEO_ACCESSION} dataset...\")\n",
    "        urllib.request.urlretrieve(GEO_URL, tar_file)\n",
    "    \n",
    "    # Extract if not already extracted\n",
    "    if not glob.glob(f\"{data_dir}/*_barcodes.tsv.gz\"):\n",
    "        print(\"Extracting files...\")\n",
    "        with tarfile.open(tar_file, 'r') as tar:\n",
    "            tar.extractall(path=data_dir)\n",
    "    \n",
    "    return data_dir\n",
    "\n",
    "def improved_parse_sample_metadata(sample_id):\n",
    "    \"\"\"Improved parser for sample metadata from sample ID.\"\"\"\n",
    "    metadata = {\n",
    "        'sample_id': sample_id,\n",
    "        'organism': 'Homo sapiens',  # All samples are human\n",
    "        'cell_type': 'T Cells',      # Default cell type\n",
    "        'crispr_type': 'Unknown',\n",
    "        'cancer_type': 'Non-Cancer',\n",
    "        'condition': 'Unknown',\n",
    "        'perturbation_name': 'Unknown'\n",
    "    }\n",
    "    \n",
    "    # Sample ID mapping based on paper information\n",
    "    isc_scrna_mapping = {\n",
    "        # Day 3 samples\n",
    "        'ISC_SCRNA001': {'condition': 'Day 3', 'crispr_type': 'None', 'perturbation_name': 'Non-targeting'},\n",
    "        'ISC_SCRNA005': {'condition': 'Day 3', 'crispr_type': 'CRISPR Cas9', 'perturbation_name': 'B2M'},\n",
    "        'ISC_SCRNA006': {'condition': 'Day 3', 'crispr_type': 'CRISPR Cas9', 'perturbation_name': 'PDCD1'},\n",
    "        'ISC_SCRNA007': {'condition': 'Day 3', 'crispr_type': 'CRISPR Cas9', 'perturbation_name': 'Non-targeting'},\n",
    "        'ISC_SCRNA008': {'condition': 'Day 3', 'crispr_type': 'ABE', 'perturbation_name': 'B2M'},\n",
    "        'ISC_SCRNA009': {'condition': 'Day 3', 'crispr_type': 'ABE', 'perturbation_name': 'PDCD1'},\n",
    "        'ISC_SCRNA010': {'condition': 'Day 3', 'crispr_type': 'ABE', 'perturbation_name': 'Non-targeting'},\n",
    "        \n",
    "        # Day 7 samples\n",
    "        'ISC_SCRNA014': {'condition': 'Day 7', 'crispr_type': 'CRISPR Cas9', 'perturbation_name': 'B2M'},\n",
    "        'ISC_SCRNA015': {'condition': 'Day 7', 'crispr_type': 'CRISPR Cas9', 'perturbation_name': 'PDCD1'},\n",
    "        'ISC_SCRNA016': {'condition': 'Day 7', 'crispr_type': 'ABE', 'perturbation_name': 'B2M'},\n",
    "        'ISC_SCRNA017': {'condition': 'Day 7', 'crispr_type': 'ABE', 'perturbation_name': 'PDCD1'},\n",
    "        \n",
    "        # Day 21 samples\n",
    "        'ISC_SCRNA018': {'condition': 'Day 21', 'crispr_type': 'CRISPR Cas9', 'perturbation_name': 'B2M'},\n",
    "        'ISC_SCRNA019': {'condition': 'Day 21', 'crispr_type': 'CRISPR Cas9', 'perturbation_name': 'PDCD1'},\n",
    "    }\n",
    "    \n",
    "    # Check if this is an ISC_SCRNA sample and apply mapping\n",
    "    for scrna_id, info in isc_scrna_mapping.items():\n",
    "        if scrna_id in sample_id:\n",
    "            for key, value in info.items():\n",
    "                metadata[key] = value\n",
    "            break\n",
    "    \n",
    "    # Process explicitly labeled samples (as in original function)\n",
    "    # Extract CRISPR type\n",
    "    if 'Cas9' in sample_id or 'Casq' in sample_id:\n",
    "        metadata['crispr_type'] = 'CRISPR Cas9'\n",
    "    elif 'ABE' in sample_id:\n",
    "        if 'ABEe8' in sample_id or 'V106W' in sample_id:\n",
    "            metadata['crispr_type'] = 'ABE-V106W'  # High-fidelity ABE\n",
    "        else:\n",
    "            metadata['crispr_type'] = 'ABE'  # Adenine Base Editor\n",
    "    \n",
    "    # Extract perturbation target\n",
    "    if 'NTC' in sample_id:\n",
    "        metadata['perturbation_name'] = 'Non-targeting'\n",
    "    elif 'B2M' in sample_id:\n",
    "        metadata['perturbation_name'] = 'B2M'\n",
    "    elif 'PD1' in sample_id:\n",
    "        metadata['perturbation_name'] = 'PDCD1'\n",
    "    \n",
    "    # Extract cell type\n",
    "    if 'huh7' in sample_id.lower():\n",
    "        metadata['cell_type'] = 'Huh7'  # Liver cancer cell line\n",
    "        metadata['cancer_type'] = 'Hepatocellular carcinoma'\n",
    "    \n",
    "    # Extract time point for explicitly labeled samples\n",
    "    if 'day3' in sample_id.lower():\n",
    "        metadata['condition'] = \"Day 3\"\n",
    "    elif 'day7' in sample_id.lower():\n",
    "        metadata['condition'] = \"Day 7\"\n",
    "    elif 'day21' in sample_id.lower():\n",
    "        metadata['condition'] = \"Day 21\"\n",
    "    \n",
    "    # Handle blank control\n",
    "    if 'blank' in sample_id.lower():\n",
    "        metadata['perturbation_name'] = 'Non-targeting'\n",
    "        metadata['crispr_type'] = 'None'\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def load_10x_data(base_path, prefix, max_entries=MAX_ENTRIES_PER_SAMPLE):\n",
    "    \"\"\"Load 10X Genomics data files with a limit on the number of entries.\"\"\"\n",
    "    try:\n",
    "        # Find the files with the given prefix\n",
    "        barcode_file = glob.glob(f\"{base_path}/{prefix}_barcodes.tsv.gz\")[0]\n",
    "        features_file = glob.glob(f\"{base_path}/{prefix}_features.tsv.gz\")[0]\n",
    "        matrix_file = glob.glob(f\"{base_path}/{prefix}_matrix.mtx.gz\")[0]\n",
    "        \n",
    "        # Read barcodes\n",
    "        with gzip.open(barcode_file, 'rt') as f:\n",
    "            barcodes = [line.strip() for line in f]\n",
    "        \n",
    "        # Read features\n",
    "        with gzip.open(features_file, 'rt') as f:\n",
    "            features_data = [line.strip().split('\\t') for line in f]\n",
    "        \n",
    "        # Extract gene IDs and gene symbols\n",
    "        gene_ids = [row[0] for row in features_data]\n",
    "        gene_symbols = [row[1] for row in features_data]\n",
    "        \n",
    "        # Read the sparse matrix header\n",
    "        with gzip.open(matrix_file, 'rt') as f:\n",
    "            # Skip comments\n",
    "            header = next(f)\n",
    "            while header.startswith('%'):\n",
    "                header = next(f)\n",
    "            \n",
    "            # Parse dimensions\n",
    "            n_genes, n_cells, n_entries = map(int, header.strip().split())\n",
    "            \n",
    "            # Read a limited number of entries\n",
    "            data = []\n",
    "            row_indices = []\n",
    "            col_indices = []\n",
    "            \n",
    "            for i, line in enumerate(f):\n",
    "                if i >= max_entries:\n",
    "                    break\n",
    "                \n",
    "                if line.strip():\n",
    "                    gene_idx, cell_idx, value = line.strip().split()\n",
    "                    row_indices.append(int(gene_idx) - 1)  # Convert to 0-based indexing\n",
    "                    col_indices.append(int(cell_idx) - 1)  # Convert to 0-based indexing\n",
    "                    data.append(float(value))\n",
    "        \n",
    "        # Create sparse matrix\n",
    "        matrix = sparse.csr_matrix(\n",
    "            (data, (row_indices, col_indices)), \n",
    "            shape=(n_genes, n_cells)\n",
    "        )\n",
    "        \n",
    "        # Transpose to get cells as rows, genes as columns\n",
    "        matrix = matrix.T\n",
    "        \n",
    "        # Create DataFrame with gene symbols as index\n",
    "        var_df = pd.DataFrame(index=gene_symbols, data={'gene_ids': gene_ids})\n",
    "        \n",
    "        # Make gene symbols unique\n",
    "        var_df.index = pd.Index([f\"{s}_{i}\" if gene_symbols.count(s) > 1 else s \n",
    "                                for i, s in enumerate(gene_symbols)])\n",
    "        \n",
    "        # Create AnnData object\n",
    "        adata = ad.AnnData(\n",
    "            X=matrix,\n",
    "            obs=pd.DataFrame(index=barcodes),\n",
    "            var=var_df,\n",
    "            dtype=np.float32  # Explicitly set dtype to avoid warning\n",
    "        )\n",
    "        \n",
    "        # Make observation names unique\n",
    "        adata.obs_names_make_unique()\n",
    "        \n",
    "        return adata\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data for {prefix}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_dataset(data_dir, max_samples=None, max_entries=MAX_ENTRIES_PER_SAMPLE):\n",
    "    \"\"\"Process the entire dataset and create harmonized h5ad file.\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    \n",
    "    # Download and extract dataset if needed\n",
    "    data_dir = download_dataset(data_dir)\n",
    "    \n",
    "    # Find all sample files\n",
    "    sample_files = glob.glob(f\"{data_dir}/*_barcodes.tsv.gz\")\n",
    "    sample_prefixes = [os.path.basename(f).replace('_barcodes.tsv.gz', '') for f in sample_files]\n",
    "    \n",
    "    # Limit the number of samples if specified\n",
    "    if max_samples is not None:\n",
    "        sample_prefixes = sample_prefixes[:max_samples]\n",
    "    \n",
    "    print(f\"Processing {len(sample_prefixes)} samples\")\n",
    "    \n",
    "    # Process each sample\n",
    "    all_samples = []\n",
    "    for prefix in tqdm(sample_prefixes):\n",
    "        print(f\"Processing {prefix}...\")\n",
    "        \n",
    "        # Load data\n",
    "        adata = load_10x_data(data_dir, prefix, max_entries)\n",
    "        if adata is None:\n",
    "            continue\n",
    "        \n",
    "        # Add metadata using improved parser\n",
    "        metadata = improved_parse_sample_metadata(prefix)\n",
    "        adata.obs['sample_id'] = metadata['sample_id']\n",
    "        \n",
    "        # Add harmonized metadata fields\n",
    "        for key, value in metadata.items():\n",
    "            if key != 'sample_id':  # Already added\n",
    "                adata.obs[key] = value\n",
    "        \n",
    "        all_samples.append(adata)\n",
    "    \n",
    "    # Combine all samples\n",
    "    if all_samples:\n",
    "        print(\"Combining all samples...\")\n",
    "        \n",
    "        # Concatenate all samples directly\n",
    "        print(\"Concatenating all samples...\")\n",
    "        combined = ad.concat(\n",
    "            all_samples,\n",
    "            join='inner',  # Only keep genes that are in all datasets\n",
    "            merge='same',  # Only merge when attributes are the same\n",
    "            label='batch',  # Add batch annotation\n",
    "            keys=[adata.obs['sample_id'].iloc[0] for adata in all_samples]\n",
    "        )\n",
    "        \n",
    "        print(f\"Number of genes after concatenation: {combined.n_vars}\")\n",
    "        \n",
    "        # Ensure required metadata fields are present\n",
    "        required_fields = ['organism', 'cell_type', 'crispr_type', 'cancer_type', 'condition', 'perturbation_name']\n",
    "        for field in required_fields:\n",
    "            if field not in combined.obs.columns:\n",
    "                combined.obs[field] = 'Unknown'\n",
    "        \n",
    "        # ---- Filtering out cells with perturbation_name \"Unknown\" (or typo \"Unknonw\") ----\n",
    "        invalid_names = ['Unknown', 'Unknonw']\n",
    "        initial_cell_count = combined.n_obs\n",
    "        combined = combined[~combined.obs['perturbation_name'].isin(invalid_names)]\n",
    "        filtered_cell_count = combined.n_obs\n",
    "        print(f\"Filtered out {initial_cell_count - filtered_cell_count} cells with perturbation_name in {invalid_names}\")\n",
    "        # --------------------------------------------------------------------------\n",
    "        \n",
    "        # Convert metadata fields to categorical\n",
    "        for field in required_fields:\n",
    "            combined.obs[field] = combined.obs[field].astype('category')\n",
    "        \n",
    "        # Make observation names unique\n",
    "        combined.obs_names_make_unique()\n",
    "        \n",
    "        # Save the combined dataset\n",
    "        output_file = data_dir / f\"{GEO_ACCESSION}_harmonized_improved.h5ad\"\n",
    "        print(f\"Saving harmonized dataset to {output_file}\")\n",
    "        combined.write(output_file)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\nDataset Summary:\")\n",
    "        print(f\"Total cells: {combined.n_obs}\")\n",
    "        print(f\"Total genes: {combined.n_vars}\")\n",
    "        print(\"\\nMetadata fields:\")\n",
    "        for field in required_fields:\n",
    "            categories = combined.obs[field].cat.categories\n",
    "            print(f\"- {field}: {list(categories)}\")\n",
    "            print(f\"  Categories ({len(categories)}, object): {list(categories)}\")\n",
    "        \n",
    "        return combined\n",
    "    else:\n",
    "        print(\"No samples were successfully processed.\")\n",
    "        return None\n",
    "\n",
    "def update_existing_h5ad(h5ad_path):\n",
    "    \"\"\"Update metadata in an existing h5ad file using the improved parser.\"\"\"\n",
    "    import scanpy as sc\n",
    "    \n",
    "    print(f\"Loading existing h5ad file: {h5ad_path}\")\n",
    "    adata = sc.read(h5ad_path)\n",
    "    \n",
    "    print(\"Updating sample metadata...\")\n",
    "    # Get unique sample IDs\n",
    "    unique_samples = adata.obs['sample_id'].unique()\n",
    "    \n",
    "    # Create a mapping dictionary for each unique sample\n",
    "    sample_metadata = {}\n",
    "    for sample_id in unique_samples:\n",
    "        sample_metadata[sample_id] = improved_parse_sample_metadata(sample_id)\n",
    "    \n",
    "    # Update metadata for each cell based on its sample_id\n",
    "    for field in ['crispr_type', 'condition', 'perturbation_name', 'cell_type', 'cancer_type']:\n",
    "        adata.obs[field] = adata.obs['sample_id'].map(lambda x: sample_metadata[x][field])\n",
    "    \n",
    "    # ---- Filtering out cells with perturbation_name \"Unknown\" (or typo \"Unknonw\") ----\n",
    "    invalid_names = ['Unknown', 'Unknonw']\n",
    "    initial_cell_count = adata.n_obs\n",
    "    adata = adata[~adata.obs['perturbation_name'].isin(invalid_names)]\n",
    "    filtered_cell_count = adata.n_obs\n",
    "    print(f\"Filtered out {initial_cell_count - filtered_cell_count} cells with perturbation_name in {invalid_names}\")\n",
    "    # ------------------------------------------------------------------------------\n",
    "    \n",
    "    # Convert categorical columns\n",
    "    for field in ['organism', 'cell_type', 'crispr_type', 'cancer_type', 'condition', 'perturbation_name']:\n",
    "        adata.obs[field] = adata.obs[field].astype('category')\n",
    "    \n",
    "    # Print summary statistics of updated metadata\n",
    "    print(\"\\nUpdated Dataset Summary:\")\n",
    "    print(f\"Total cells: {adata.n_obs}\")\n",
    "    print(f\"Total genes: {adata.n_vars}\")\n",
    "    \n",
    "    print(\"\\nMetadata field distributions:\")\n",
    "    for field in ['perturbation_name', 'condition', 'crispr_type', 'cell_type']:\n",
    "        print(f\"\\n{field} distribution:\")\n",
    "        print(adata.obs[field].value_counts())\n",
    "    \n",
    "    # Save the updated dataset\n",
    "    output_path = h5ad_path.replace('.h5ad', '_improved.h5ad')\n",
    "    print(f\"\\nSaving updated dataset to {output_path}\")\n",
    "    adata.write(output_path)\n",
    "    \n",
    "    return adata\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    data_dir = \"./GSE280767\"  # Change if you prefer a different directory\n",
    "    \n",
    "    # Check if we already have the h5ad file\n",
    "    existing_h5ad = Path(f\"{data_dir}/{GEO_ACCESSION}_harmonized.h5ad\")\n",
    "    \n",
    "    if existing_h5ad.exists():\n",
    "        print(f\"Found existing h5ad file. Updating metadata...\")\n",
    "        updated_adata = update_existing_h5ad(existing_h5ad)\n",
    "    else:\n",
    "        # Process the entire dataset from raw files\n",
    "        max_samples = None  # Set a specific number to limit samples if needed\n",
    "        combined_dataset = process_dataset(data_dir, max_samples, MAX_ENTRIES_PER_SAMPLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b8e670-7c91-4cb5-8aac-00102b8176b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify cells where perturbation_name is \"Non-targeting control\"\n",
    "nt_control_mask = adata.obs[\"perturbation_name\"] == \"Non-targeting control\"\n",
    "num_nt_control = nt_control_mask.sum()\n",
    "\n",
    "# Update the perturbation_name and condition columns for these cells\n",
    "adata.obs.loc[nt_control_mask, \"perturbation_name\"] = \"Non-targeting\"\n",
    "adata.obs.loc[nt_control_mask, \"condition\"] = \"Control\"\n",
    "print(f\"Updated {num_nt_control} cells from 'Non-targeting control' to 'Non-targeting' and set their condition to 'Control'.\")\n",
    "\n",
    "# Additionally, ensure that any cell with perturbation_name \"Non-targeting\" has condition set to \"Control\"\n",
    "nt_mask = adata.obs[\"perturbation_name\"] == \"Non-targeting\"\n",
    "adata.obs.loc[nt_mask, \"condition\"] = \"Control\"\n",
    "\n",
    "# Optionally convert columns to categorical type if not already\n",
    "if not pd.api.types.is_categorical_dtype(adata.obs[\"perturbation_name\"]):\n",
    "    adata.obs[\"perturbation_name\"] = adata.obs[\"perturbation_name\"].astype(\"category\")\n",
    "if not pd.api.types.is_categorical_dtype(adata.obs[\"condition\"]):\n",
    "    adata.obs[\"condition\"] = adata.obs[\"condition\"].astype(\"category\")\n",
    "\n",
    "adata.write_h5ad(\"/content/GSE280767.h5ad\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "703ca59a-be42-409e-b477-a3a0fbea9dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3b38bd-1bdb-4f0a-801c-082ce9d71f87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
