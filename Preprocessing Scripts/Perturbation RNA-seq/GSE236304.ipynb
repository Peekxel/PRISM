{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "703ca59a-be42-409e-b477-a3a0fbea9dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Jupyter Notebook version of the GSE236304 harmonization script with orientation fix and updated perturbation_name formatting.\n",
    "\n",
    "This script processes the GSE236304 dataset (MethNet: a robust approach to identify regulatory hubs \n",
    "and their distal targets in cancer [Perturb-seq]) and harmonizes it into h5ad format with standardized metadata.\n",
    "\n",
    "The dataset contains CRISPR-i perturbation data from A549 cells with dCas9-KRAB-MECP2.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from urllib.request import urlretrieve\n",
    "import gzip\n",
    "import shutil\n",
    "from scipy import sparse\n",
    "import anndata\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('GSE236304_harmonizer')\n",
    "\n",
    "# URLs for the dataset files\n",
    "DATASET_FILES = {\n",
    "    'barcodes': 'https://ftp.ncbi.nlm.nih.gov/geo/series/GSE236nnn/GSE236304/suppl/GSE236304_barcodes.tsv.gz',\n",
    "    'features': 'https://ftp.ncbi.nlm.nih.gov/geo/series/GSE236nnn/GSE236304/suppl/GSE236304_features.tsv.gz',\n",
    "    'matrix': 'https://ftp.ncbi.nlm.nih.gov/geo/series/GSE236nnn/GSE236304/suppl/GSE236304_matrix.mtx.gz',\n",
    "    'feature_reference': 'https://ftp.ncbi.nlm.nih.gov/geo/series/GSE236nnn/GSE236304/suppl/GSE236304_feature_reference.csv.gz',\n",
    "    'protospacer_umi_thresholds': 'https://ftp.ncbi.nlm.nih.gov/geo/series/GSE236nnn/GSE236304/suppl/GSE236304_protospacer_umi_thresholds.csv.gz',\n",
    "    'h5_matrix': 'https://ftp.ncbi.nlm.nih.gov/geo/series/GSE236nnn/GSE236304/suppl/GSE236304_filtered_feature_bc_matrix.h5'\n",
    "}\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"Download a file from a URL to a destination.\"\"\"\n",
    "    if os.path.exists(destination):\n",
    "        logger.info(f\"File already exists: {destination}\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Downloading {url} to {destination}\")\n",
    "    urlretrieve(url, destination)\n",
    "    logger.info(f\"Downloaded {destination}\")\n",
    "\n",
    "def ensure_dataset_files(data_dir):\n",
    "    \"\"\"Ensure all dataset files are available, downloading if necessary.\"\"\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    for file_key, url in DATASET_FILES.items():\n",
    "        file_name = os.path.basename(url)\n",
    "        file_path = os.path.join(data_dir, file_name)\n",
    "        download_file(url, file_path)\n",
    "    \n",
    "    return {file_key: os.path.join(data_dir, os.path.basename(url)) for file_key, url in DATASET_FILES.items()}\n",
    "\n",
    "def load_10x_h5(filename):\n",
    "    \"\"\"Load data from a 10X h5 file and detect the correct orientation.\"\"\"\n",
    "    logger.info(f\"Loading data from {filename}\")\n",
    "    \n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        # Get the underlying CSR components\n",
    "        data = f['matrix/data'][:]\n",
    "        indices = f['matrix/indices'][:]\n",
    "        indptr = f['matrix/indptr'][:]\n",
    "        shape_raw = f['matrix/shape'][:]\n",
    "        \n",
    "        # Determine correct shape:\n",
    "        # If indptr length equals shape_raw[0]+1 then matrix is stored as (features x cells)\n",
    "        # Otherwise, if it equals shape_raw[1]+1 then matrix is stored as (cells x features)\n",
    "        if len(indptr) == shape_raw[0] + 1:\n",
    "            matrix_shape = tuple(shape_raw)\n",
    "        elif len(indptr) == shape_raw[1] + 1:\n",
    "            matrix_shape = (shape_raw[1], shape_raw[0])\n",
    "        else:\n",
    "            raise ValueError(\"indptr length does not match any dimension of shape.\")\n",
    "        \n",
    "        # Get barcodes\n",
    "        barcodes = f['matrix/barcodes'][:]\n",
    "        barcodes = [bc.decode('utf-8') for bc in barcodes]\n",
    "        \n",
    "        # Get feature info\n",
    "        feature_ids = f['matrix/features/id'][:]\n",
    "        feature_ids = [fid.decode('utf-8') for fid in feature_ids]\n",
    "        \n",
    "        feature_names = f['matrix/features/name'][:]\n",
    "        feature_names = [fn.decode('utf-8') for fn in feature_names]\n",
    "        \n",
    "        feature_types = f['matrix/features/feature_type'][:]\n",
    "        feature_types = [ft.decode('utf-8') for ft in feature_types]\n",
    "        \n",
    "        target_gene_ids = f['matrix/features/target_gene_id'][:]\n",
    "        target_gene_ids = [tg.decode('utf-8') for tg in target_gene_ids]\n",
    "        \n",
    "        target_gene_names = f['matrix/features/target_gene_name'][:]\n",
    "        target_gene_names = [tg.decode('utf-8') for tg in target_gene_names]\n",
    "    \n",
    "    # Create feature DataFrame\n",
    "    features = pd.DataFrame({\n",
    "        'id': feature_ids,\n",
    "        'name': feature_names,\n",
    "        'feature_type': feature_types,\n",
    "        'target_gene_id': target_gene_ids,\n",
    "        'target_gene_name': target_gene_names\n",
    "    })\n",
    "    \n",
    "    return data, indices, indptr, matrix_shape, barcodes, features\n",
    "\n",
    "def create_anndata_object(data, indices, indptr, shape, barcodes, features):\n",
    "    \"\"\"Create an AnnData object from the loaded data.\n",
    "    \n",
    "    This function ensures that the returned sparse matrix has rows corresponding to features.\n",
    "    If the number of barcodes equals the first dimension of the matrix, then the matrix is stored as (cells, features)\n",
    "    and will be transposed.\n",
    "    \"\"\"\n",
    "    logger.info(\"Creating AnnData object\")\n",
    "    \n",
    "    # Create the sparse matrix from the loaded data\n",
    "    matrix = sparse.csr_matrix((data, indices, indptr), shape=shape)\n",
    "    \n",
    "    # If the number of barcodes equals the first dimension, then the matrix is (cells x features).\n",
    "    # Transpose it so that rows correspond to features.\n",
    "    if len(barcodes) == shape[0]:\n",
    "        matrix = matrix.T  # now matrix is (features x cells)\n",
    "    \n",
    "    # Now, extract gene expression features (rows where feature_type is \"Gene Expression\")\n",
    "    gene_expr_features = features[features['feature_type'] == 'Gene Expression']\n",
    "    gene_expr_indices = gene_expr_features.index.tolist()\n",
    "    \n",
    "    # Extract the gene expression matrix (rows: gene expression features, columns: cells)\n",
    "    gene_expr_matrix = matrix[gene_expr_indices, :]\n",
    "    \n",
    "    # Create AnnData object: transpose gene_expr_matrix so that cells are rows\n",
    "    adata = anndata.AnnData(\n",
    "        X=gene_expr_matrix.T,\n",
    "        obs=pd.DataFrame(index=barcodes),\n",
    "        var=gene_expr_features.set_index('id')\n",
    "    )\n",
    "    \n",
    "    # Add gene names to the var DataFrame\n",
    "    adata.var['gene_name'] = gene_expr_features['name'].values\n",
    "    \n",
    "    return adata, matrix\n",
    "\n",
    "def assign_perturbations(adata, features, matrix, protospacer_df):\n",
    "    \"\"\"Assign perturbations to cells based on CRISPR guide counts.\"\"\"\n",
    "    logger.info(\"Assigning perturbations to cells\")\n",
    "    \n",
    "    # Get CRISPR guide features\n",
    "    crispr_features = features[features['feature_type'] == 'CRISPR Guide Capture']\n",
    "    crispr_indices = crispr_features.index.tolist()\n",
    "    \n",
    "    # Extract CRISPR guide counts for each cell from the full matrix\n",
    "    # (matrix is assumed to be (features x cells))\n",
    "    crispr_matrix = matrix[crispr_indices, :].toarray()\n",
    "    \n",
    "    # Create a DataFrame with CRISPR guide counts\n",
    "    crispr_df = pd.DataFrame(\n",
    "        crispr_matrix,\n",
    "        index=crispr_features['name'],\n",
    "        columns=adata.obs.index\n",
    "    ).T  # now rows: cells, columns: guides\n",
    "    \n",
    "    # Merge protospacer UMI thresholds\n",
    "    protospacer_thresholds = dict(zip(protospacer_df['Protospacer'], protospacer_df['UMI threshold']))\n",
    "    \n",
    "    # Determine which guides are detected in each cell based on UMI thresholds\n",
    "    detected_guides = pd.DataFrame(index=adata.obs.index, columns=['detected_guides', 'target_genes'])\n",
    "    \n",
    "    for cell in adata.obs.index:\n",
    "        cell_guides = []\n",
    "        cell_targets = []\n",
    "        \n",
    "        for guide in crispr_df.columns:\n",
    "            threshold = protospacer_thresholds.get(guide, 0)\n",
    "            if crispr_df.loc[cell, guide] >= threshold:\n",
    "                cell_guides.append(guide)\n",
    "                \n",
    "                # Find the target gene for this guide\n",
    "                target = crispr_features[crispr_features['name'] == guide]['target_gene_name'].values\n",
    "                if len(target) > 0 and target[0]:\n",
    "                    cell_targets.append(target[0])\n",
    "        \n",
    "        # Join target genes with semicolon for internal use\n",
    "        detected_guides.loc[cell, 'detected_guides'] = ';'.join(cell_guides) if cell_guides else 'None'\n",
    "        detected_guides.loc[cell, 'target_genes'] = ';'.join(set(cell_targets)) if cell_targets else 'None'\n",
    "    \n",
    "    # Add detected guides to adata.obs\n",
    "    adata.obs['detected_guides'] = detected_guides['detected_guides']\n",
    "    adata.obs['target_genes'] = detected_guides['target_genes']\n",
    "    \n",
    "    # Determine perturbation status\n",
    "    # If target_genes is 'None' or 'Non-Targeting', assign 'Non-targeting'\n",
    "    # Otherwise, replace semicolons with plus signs\n",
    "    adata.obs['perturbation_name'] = adata.obs['target_genes'].apply(\n",
    "        lambda x: 'Non-targeting' if x.strip().lower() in ['non-targeting', 'none'] else x.replace(';', '+')\n",
    "    )\n",
    "    \n",
    "    # Determine condition (control or test)\n",
    "    adata.obs['condition'] = adata.obs['perturbation_name'].apply(\n",
    "        lambda x: 'control' if x == 'Non-targeting' else 'test'\n",
    "    )\n",
    "    \n",
    "    return adata\n",
    "\n",
    "def assign_cell_hashing(adata, features, matrix):\n",
    "    \"\"\"Assign cell hashing information based on CMO counts.\"\"\"\n",
    "    logger.info(\"Assigning cell hashing information\")\n",
    "    \n",
    "    # Get CMO features (Antibody Capture)\n",
    "    cmo_features = features[features['feature_type'] == 'Antibody Capture']\n",
    "    cmo_indices = cmo_features.index.tolist()\n",
    "    \n",
    "    # Extract CMO counts for each cell (matrix is (features x cells))\n",
    "    cmo_matrix = matrix[cmo_indices, :].toarray()\n",
    "    \n",
    "    # Create a DataFrame with CMO counts (rows: cells, columns: CMOs)\n",
    "    cmo_df = pd.DataFrame(\n",
    "        cmo_matrix,\n",
    "        index=cmo_features['name'],\n",
    "        columns=adata.obs.index\n",
    "    ).T\n",
    "    \n",
    "    # For each cell, determine which CMO has the highest count\n",
    "    adata.obs['cmo_assignment'] = cmo_df.idxmax(axis=1)\n",
    "    adata.obs['cmo_count'] = cmo_df.max(axis=1)\n",
    "    \n",
    "    return adata\n",
    "\n",
    "def harmonize_metadata(adata):\n",
    "    \"\"\"Harmonize metadata according to the required standards.\"\"\"\n",
    "    logger.info(\"Harmonizing metadata\")\n",
    "    \n",
    "    # Set organism\n",
    "    adata.obs['organism'] = 'Homo sapiens'\n",
    "    \n",
    "    # Set cell type (A549 cells)\n",
    "    adata.obs['cell_type'] = 'A549'\n",
    "    \n",
    "    # Set CRISPR type (CRISPRi)\n",
    "    adata.obs['crispr_type'] = 'CRISPRi'\n",
    "    \n",
    "    # Set cancer type (Lung Cancer - A549 is a lung adenocarcinoma cell line)\n",
    "    adata.obs['cancer_type'] = 'Lung Cancer'\n",
    "    \n",
    "    return adata\n",
    "\n",
    "def process_dataset(data_dir, output_file):\n",
    "    \"\"\"Process the GSE236304 dataset and save as h5ad.\"\"\"\n",
    "    # Ensure dataset files are available\n",
    "    file_paths = ensure_dataset_files(data_dir)\n",
    "    \n",
    "    # Load data from h5 file with orientation detection\n",
    "    data, indices, indptr, matrix_shape, barcodes, features = load_10x_h5(file_paths['h5_matrix'])\n",
    "    \n",
    "    # Load protospacer UMI thresholds\n",
    "    protospacer_df = pd.read_csv(file_paths['protospacer_umi_thresholds'])\n",
    "    \n",
    "    # Create AnnData object and get the full matrix (now guaranteed to be in orientation (features x cells))\n",
    "    adata, matrix = create_anndata_object(data, indices, indptr, matrix_shape, barcodes, features)\n",
    "    \n",
    "    # Assign perturbations\n",
    "    adata = assign_perturbations(adata, features, matrix, protospacer_df)\n",
    "    \n",
    "    # Assign cell hashing information\n",
    "    adata = assign_cell_hashing(adata, features, matrix)\n",
    "    \n",
    "    # Harmonize metadata\n",
    "    adata = harmonize_metadata(adata)\n",
    "    \n",
    "    # Save the harmonized dataset\n",
    "    logger.info(f\"Saving harmonized dataset to {output_file}\")\n",
    "    adata.write_h5ad(output_file)\n",
    "    logger.info(\"Done!\")\n",
    "    \n",
    "    return adata\n",
    "\n",
    "# -------------------------------\n",
    "# Run the harmonization process\n",
    "# -------------------------------\n",
    "\n",
    "# Set the directory where dataset files will be stored\n",
    "data_dir = 'data_'  # Change this path as needed\n",
    "\n",
    "# Define the output file for the harmonized AnnData object\n",
    "output_file = os.path.join(data_dir, 'GSE236304_harmonized.h5ad')\n",
    "\n",
    "# Ensure the data directory exists\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Process the dataset and save the result\n",
    "adata = process_dataset(data_dir, output_file)\n",
    "print(f\"Harmonized dataset saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
