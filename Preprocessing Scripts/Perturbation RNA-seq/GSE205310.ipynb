{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a30c09d-2469-48af-96d5-7c0cae4d0517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import gzip\n",
    "import shutil\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import anndata as ad\n",
    "from scipy import io\n",
    "from pathlib import Path\n",
    "\n",
    "def download_and_extract(data_dir):\n",
    "    \"\"\"\n",
    "    Download and extract the GSE205310 dataset if not already present.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Path to the directory where the data should be stored.\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    # Check if the tar file exists\n",
    "    tar_path = os.path.join(data_dir, \"GSE205310_RAW.tar\")\n",
    "    if not os.path.exists(tar_path):\n",
    "        print(f\"Downloading GSE205310_RAW.tar to {tar_path}...\")\n",
    "        url = \"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE205310&format=file\"\n",
    "        urllib.request.urlretrieve(url, tar_path)\n",
    "    \n",
    "    # Check if files are already extracted\n",
    "    if not os.path.exists(os.path.join(data_dir, \"GSM6210116_dual.matrix.mtx.gz\")):\n",
    "        print(f\"Extracting files from {tar_path}...\")\n",
    "        with tarfile.open(tar_path) as tar:\n",
    "            tar.extractall(path=data_dir)\n",
    "    \n",
    "    print(\"Download and extraction complete.\")\n",
    "\n",
    "def parse_guide_identity(guide_identity):\n",
    "    \"\"\"\n",
    "    Parse the guide identity string to extract perturbation information.\n",
    "    \n",
    "    Args:\n",
    "        guide_identity: String containing guide identity information.\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with perturbation information.\n",
    "    \"\"\"\n",
    "    info = {\n",
    "        'perturbation_name': None,\n",
    "        'condition': 'test'\n",
    "    }\n",
    "    \n",
    "    # Check if it's a non-targeting control\n",
    "    if 'non-targeting' in guide_identity.lower():\n",
    "        info['perturbation_name'] = 'Non-targeting'\n",
    "        info['condition'] = 'control'\n",
    "        return info\n",
    "    \n",
    "    # For targeting guides, extract the gene name\n",
    "    if '_' in guide_identity:\n",
    "        # For dual library format: GENE_STRAND_POSITION\n",
    "        gene = guide_identity.split('_')[0]\n",
    "        info['perturbation_name'] = gene\n",
    "    else:\n",
    "        # For Dolcetto format: GENE_SEQUENCE\n",
    "        gene = guide_identity.split('_')[0]\n",
    "        info['perturbation_name'] = gene\n",
    "    \n",
    "    return info\n",
    "\n",
    "def process_sample(sample_id, data_dir):\n",
    "    \"\"\"\n",
    "    Process a single sample from the GSE205310 dataset.\n",
    "    \n",
    "    Args:\n",
    "        sample_id: Sample identifier (e.g., 'GSM6210116_dual').\n",
    "        data_dir: Path to the directory containing the data.\n",
    "        \n",
    "    Returns:\n",
    "        AnnData object with the processed data.\n",
    "    \"\"\"\n",
    "    print(f\"Processing {sample_id}...\")\n",
    "    \n",
    "    # File paths\n",
    "    matrix_file = os.path.join(data_dir, f\"{sample_id}.matrix.mtx.gz\")\n",
    "    features_file = os.path.join(data_dir, f\"{sample_id}.features.tsv.gz\")\n",
    "    barcodes_file = os.path.join(data_dir, f\"{sample_id}.barcodes.tsv.gz\")\n",
    "    cell_identities_file = os.path.join(data_dir, f\"{sample_id}_cell_identities.csv.gz\")\n",
    "    \n",
    "    # Read the expression data\n",
    "    with gzip.open(matrix_file, 'rb') as f:\n",
    "        X = io.mmread(f).T.tocsr()\n",
    "    \n",
    "    # Read the features (genes)\n",
    "    with gzip.open(features_file, 'rt') as f:\n",
    "        gene_ids = []\n",
    "        gene_symbols = []\n",
    "        for line in f:\n",
    "            fields = line.strip().split('\\t')\n",
    "            gene_ids.append(fields[0])\n",
    "            gene_symbols.append(fields[1])\n",
    "    \n",
    "    # Read the barcodes\n",
    "    with gzip.open(barcodes_file, 'rt') as f:\n",
    "        barcodes = [line.strip() for line in f]\n",
    "    \n",
    "    # Create a DataFrame for var with gene symbols as index\n",
    "    var_df = pd.DataFrame(index=gene_symbols)\n",
    "    var_df['gene_ids'] = gene_ids\n",
    "    \n",
    "    # Make gene symbols unique\n",
    "    gene_counts = var_df.groupby(level=0).cumcount().values\n",
    "    new_index = [f\"{gene}_{i}\" if i > 0 else gene for gene, i in zip(var_df.index, gene_counts)]\n",
    "    var_df.index = new_index\n",
    "    \n",
    "    # Create the AnnData object\n",
    "    adata = ad.AnnData(X=X, obs=pd.DataFrame(index=barcodes), var=var_df)\n",
    "    \n",
    "    # Read cell identities\n",
    "    cell_identities = pd.read_csv(cell_identities_file)\n",
    "    cell_identities.set_index('cell_barcode', inplace=True)\n",
    "    \n",
    "    # Create metadata DataFrame with the same index as adata.obs\n",
    "    metadata = pd.DataFrame(index=adata.obs.index)\n",
    "    metadata['sample_id'] = sample_id\n",
    "    \n",
    "    # Add guide information\n",
    "    guide_info = {}\n",
    "    for cell_barcode, row in cell_identities.iterrows():\n",
    "        if cell_barcode in adata.obs.index:\n",
    "            guide_info[cell_barcode] = parse_guide_identity(row['guide_identity'])\n",
    "    \n",
    "    guide_df = pd.DataFrame.from_dict(guide_info, orient='index')\n",
    "    metadata = pd.concat([metadata, guide_df], axis=1)\n",
    "    \n",
    "    metadata['perturbation_name'] = metadata['perturbation_name'].fillna('Unknown')\n",
    "    metadata['condition'] = metadata['condition'].fillna('Unknown')\n",
    "    \n",
    "    # Add standard harmonized metadata fields\n",
    "    metadata['organism'] = 'Homo sapiens'\n",
    "    metadata['cell_type'] = 'K562'\n",
    "    metadata['crispr_type'] = 'CRISPRi'\n",
    "    metadata['cancer_type'] = 'Chronic myelogenous leukemia'\n",
    "    \n",
    "    # Add original guide identity\n",
    "    cell_barcodes_in_both = set(cell_identities.index).intersection(set(adata.obs.index))\n",
    "    metadata['guide_identity'] = pd.Series({\n",
    "        bc: cell_identities.loc[bc, 'guide_identity'] if bc in cell_barcodes_in_both else 'Unknown'\n",
    "        for bc in metadata.index\n",
    "    })\n",
    "    \n",
    "    # Add read count and UMI count if available\n",
    "    if 'read_count' in cell_identities.columns:\n",
    "        metadata['read_count'] = pd.Series({\n",
    "            bc: cell_identities.loc[bc, 'read_count'] if bc in cell_barcodes_in_both else np.nan\n",
    "            for bc in metadata.index\n",
    "        })\n",
    "    \n",
    "    if 'UMI_count' in cell_identities.columns:\n",
    "        metadata['UMI_count'] = pd.Series({\n",
    "            bc: cell_identities.loc[bc, 'UMI_count'] if bc in cell_barcodes_in_both else np.nan\n",
    "            for bc in metadata.index\n",
    "        })\n",
    "    \n",
    "    # Add library type\n",
    "    if 'dual' in sample_id:\n",
    "        metadata['library_type'] = 'dual_sgRNA'\n",
    "    elif 'dolcetto' in sample_id:\n",
    "        metadata['library_type'] = 'dolcetto'\n",
    "    \n",
    "    # Update adata.obs with metadata\n",
    "    adata.obs = metadata\n",
    "    \n",
    "    print(f\"Number of genes: {adata.n_vars}\")\n",
    "    print(f\"Number of cells: {adata.n_obs}\")\n",
    "    \n",
    "    return adata\n",
    "\n",
    "def harmonize_dataset(data_dir):\n",
    "    \"\"\"\n",
    "    Harmonize the GSE205310 dataset.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Path to the directory containing the data.\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of AnnData objects, one for each sample.\n",
    "    \"\"\"\n",
    "    # Ensure data is downloaded and extracted\n",
    "    download_and_extract(data_dir)\n",
    "    \n",
    "    # Process each sample\n",
    "    sample_ids = [\n",
    "        'GSM6210116_dual',\n",
    "        'GSM6210117_dolcetto'\n",
    "    ]\n",
    "    \n",
    "    adatas = {}\n",
    "    for sample_id in sample_ids:\n",
    "        adatas[sample_id] = process_sample(sample_id, data_dir)\n",
    "    \n",
    "    # Save each AnnData object\n",
    "    for sample_id, adata in adatas.items():\n",
    "        output_file = os.path.join(data_dir, f\"{sample_id}_harmonized.h5ad\")\n",
    "        print(f\"Saving harmonized data to {output_file}...\")\n",
    "        adata.write(output_file)\n",
    "    \n",
    "    # Create a combined dataset\n",
    "    print(\"Creating combined dataset...\")\n",
    "    \n",
    "    # Ensure unique observation names\n",
    "    for sample_id, adata in adatas.items():\n",
    "        adata.obs_names = [f\"{sample_id}-{obs}\" for obs in adata.obs_names]\n",
    "    \n",
    "    # Find common genes across datasets\n",
    "    common_genes = set.intersection(*[set(adata.var_names) for adata in adatas.values()])\n",
    "    print(f\"Number of common genes across datasets: {len(common_genes)}\")\n",
    "    \n",
    "    # Subset each dataset to the common genes\n",
    "    for sample_id in adatas:\n",
    "        adatas[sample_id] = adatas[sample_id][:, list(common_genes)]\n",
    "    \n",
    "    # Concatenate datasets\n",
    "    combined = ad.concat(\n",
    "        list(adatas.values()),\n",
    "        join='outer',\n",
    "        label='sample_id',\n",
    "        keys=list(adatas.keys()),\n",
    "        index_unique='-'\n",
    "    )\n",
    "    \n",
    "    # Filter out cells where perturbation_name is 'Unknown'\n",
    "    combined = combined[combined.obs['perturbation_name'] != 'Unknown'].copy()\n",
    "    \n",
    "    output_file = os.path.join(data_dir, \"GSE205310_combined_harmonized.h5ad\")\n",
    "    print(f\"Saving combined harmonized data to {output_file}...\")\n",
    "    combined.write(output_file)\n",
    "    \n",
    "    return adatas\n",
    "\n",
    "def main(data_dir=None):\n",
    "    \"\"\"\n",
    "    Main function to run the harmonization process in Jupyter.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: (Optional) Directory where the data will be stored.\n",
    "                  If None, the current working directory is used.\n",
    "    \"\"\"\n",
    "    if data_dir is None:\n",
    "        data_dir = os.getcwd()\n",
    "    \n",
    "    print(f\"Using data directory: {data_dir}\")\n",
    "    adatas = harmonize_dataset(data_dir)\n",
    "    \n",
    "    print(\"Harmonization complete.\")\n",
    "    \n",
    "    # Print summary of the harmonized data\n",
    "    for sample_id, adata in adatas.items():\n",
    "        print(f\"\\nSummary for {sample_id}:\")\n",
    "        print(f\"  Number of cells: {adata.n_obs}\")\n",
    "        print(f\"  Number of genes: {adata.n_vars}\")\n",
    "        print(f\"  Perturbation targets: {adata.obs['perturbation_name'].nunique()}\")\n",
    "        print(f\"  Conditions: {', '.join(adata.obs['condition'].unique())}\")\n",
    "\n",
    "# Run the main function directly in the notebook\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "703ca59a-be42-409e-b477-a3a0fbea9dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
