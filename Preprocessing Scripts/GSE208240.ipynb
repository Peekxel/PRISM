{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a30c09d-2469-48af-96d5-7c0cae4d0517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Check if required packages are installed\n",
    "try:\n",
    "    import anndata as ad\n",
    "    import scanpy as sc\n",
    "except ImportError:\n",
    "    print(\"Installing required packages...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"anndata\", \"scanpy\"])\n",
    "    import anndata as ad\n",
    "    import scanpy as sc\n",
    "\n",
    "def download_dataset(output_dir):\n",
    "    \"\"\"\n",
    "    Download the GSE208240 dataset if not already present.\n",
    "    \n",
    "    Args:\n",
    "        output_dir: Directory to save the downloaded files\n",
    "    \n",
    "    Returns:\n",
    "        Path to the extracted data directory\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Define paths\n",
    "    main_dataset_path = output_dir / \"GSE208240_CRISPRi_perturbseq_sarscov2_filtered.tar.gz\"\n",
    "    viral_ref_path = output_dir / \"GSE208240_viral_references.tar.gz\"\n",
    "    extracted_dir = output_dir / \"GSE208240_extracted\"\n",
    "    data_path = extracted_dir / \"data\" / \"sunshine\" / \"perturb_seq\" / \"sars_cov_2_geo_upload\" / \"CRISPRi_perturbseq_sarscov2_filtered\"\n",
    "    \n",
    "    # Check if we already have the extracted data\n",
    "    if data_path.exists():\n",
    "        print(f\"Using already extracted data at {data_path}\")\n",
    "        return data_path\n",
    "    \n",
    "    # URLs for the dataset files\n",
    "    geo_url = \"https://ftp.ncbi.nlm.nih.gov/geo/series/GSE208nnn/GSE208240/suppl/GSE208240_CRISPRi_perturbseq_sarscov2_filtered.tar.gz\"\n",
    "    viral_ref_url = \"https://ftp.ncbi.nlm.nih.gov/geo/series/GSE208nnn/GSE208240/suppl/GSE208240_viral_references.tar.gz\"\n",
    "    \n",
    "    # Download and extract the main dataset\n",
    "    if not main_dataset_path.exists():\n",
    "        print(f\"Downloading main dataset from {geo_url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(geo_url, main_dataset_path)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to download dataset: {e}\")\n",
    "    \n",
    "    # Extract the main dataset if not already extracted\n",
    "    if not extracted_dir.exists():\n",
    "        print(\"Extracting main dataset...\")\n",
    "        try:\n",
    "            with tarfile.open(main_dataset_path, \"r:gz\") as tar:\n",
    "                tar.extractall(path=extracted_dir)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to extract dataset: {e}\")\n",
    "    \n",
    "    # Download and extract viral references\n",
    "    if not viral_ref_path.exists():\n",
    "        print(f\"Downloading viral references from {viral_ref_url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(viral_ref_url, viral_ref_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to download viral references: {e}\")\n",
    "    \n",
    "    # Extract viral references if not already extracted\n",
    "    if viral_ref_path.exists() and not (output_dir / \"viral_references\").exists():\n",
    "        print(\"Extracting viral references...\")\n",
    "        try:\n",
    "            with tarfile.open(viral_ref_path, \"r:gz\") as tar:\n",
    "                tar.extractall(path=output_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to extract viral references: {e}\")\n",
    "    \n",
    "    # Verify the data path exists\n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(f\"Data path not found after extraction: {data_path}\")\n",
    "    \n",
    "    return data_path\n",
    "\n",
    "def load_metadata(data_dir):\n",
    "    \"\"\"\n",
    "    Load metadata files (features, barcodes, cell identities).\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing the 10x Genomics data files\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (features, barcodes, cell_identities)\n",
    "    \"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    \n",
    "    # Check if required files exist\n",
    "    required_files = ['features.tsv.gz', 'barcodes.tsv.gz', 'cell_identities.csv']\n",
    "    for file in required_files:\n",
    "        if not (data_dir / file).exists():\n",
    "            raise FileNotFoundError(f\"Required file not found: {data_dir / file}\")\n",
    "    \n",
    "    print(\"Loading features...\")\n",
    "    with gzip.open(data_dir / 'features.tsv.gz', 'rt') as f:\n",
    "        features = pd.read_csv(f, sep='\\t', header=None, names=['gene_id', 'gene_symbol', 'feature_type'])\n",
    "    \n",
    "    print(\"Loading barcodes...\")\n",
    "    with gzip.open(data_dir / 'barcodes.tsv.gz', 'rt') as f:\n",
    "        barcodes = pd.read_csv(f, sep='\\t', header=None, names=['barcode'])\n",
    "    \n",
    "    print(\"Loading cell identities...\")\n",
    "    cell_identities = pd.read_csv(data_dir / 'cell_identities.csv')\n",
    "    \n",
    "    return features, barcodes, cell_identities\n",
    "\n",
    "def process_cell_identities(cell_identities):\n",
    "    \"\"\"\n",
    "    Process cell identities to extract perturbation information.\n",
    "    \n",
    "    Args:\n",
    "        cell_identities: DataFrame containing cell identity information\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with processed perturbation information\n",
    "    \"\"\"\n",
    "    print(\"Processing cell identities...\")\n",
    "    \n",
    "    # Check required columns\n",
    "    required_columns = ['cell_barcode', 'guide_identity', 'good_coverage', 'number_of_guides']\n",
    "    for col in required_columns:\n",
    "        if col not in cell_identities.columns:\n",
    "            raise ValueError(f\"Required column not found in cell_identities: {col}\")\n",
    "    \n",
    "    # Filter for cells with good coverage\n",
    "    good_cells = cell_identities[cell_identities['good_coverage'] == True].copy()\n",
    "    print(f\"Found {len(good_cells)} cells with good coverage out of {len(cell_identities)} total cells\")\n",
    "    \n",
    "    # Extract perturbation information\n",
    "    def extract_perturbation(guide_identity):\n",
    "        \"\"\"Extract gene names from guide identities\"\"\"\n",
    "        if pd.isna(guide_identity):\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        # Split by semicolon to handle multiple guides\n",
    "        guides = guide_identity.split(';')\n",
    "        \n",
    "        # Extract gene names from guides\n",
    "        genes = []\n",
    "        for guide in guides:\n",
    "            # Handle non-targeting guides\n",
    "            if guide.lower() == 'non-targeting' or guide.lower() == 'nt' or 'non-targeting' in guide.lower():\n",
    "                genes.append('non-targeting')\n",
    "                continue\n",
    "                \n",
    "            # The gene name is typically at the beginning before an underscore\n",
    "            if '_' in guide:\n",
    "                gene = guide.split('_')[0]\n",
    "                genes.append(gene)\n",
    "        \n",
    "        if not genes:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        # Remove duplicates and sort\n",
    "        unique_genes = sorted(set(genes))\n",
    "        \n",
    "        # Join multiple genes with a plus sign\n",
    "        return \" + \".join(unique_genes)\n",
    "    \n",
    "    # Apply the extraction function\n",
    "    good_cells['perturbation_name'] = good_cells['guide_identity'].apply(extract_perturbation)\n",
    "    \n",
    "    # Determine if the perturbation is targeting or non-targeting\n",
    "    def is_targeting(guide_identity):\n",
    "        \"\"\"Determine if a guide is targeting or non-targeting\"\"\"\n",
    "        if pd.isna(guide_identity):\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        # Non-targeting guides often contain terms like \"NT\" or \"non-targeting\"\n",
    "        if \"NT\" in guide_identity or \"non-targeting\" in guide_identity.lower() or \"scrambled\" in guide_identity.lower():\n",
    "            return \"Non-targeting\"\n",
    "        return \"Targeting\"\n",
    "    \n",
    "    good_cells['targeting'] = good_cells['guide_identity'].apply(is_targeting)\n",
    "    \n",
    "    # Set condition based on targeting status\n",
    "    good_cells['condition'] = good_cells['targeting'].map({\n",
    "        \"Targeting\": \"test\", \n",
    "        \"Non-targeting\": \"control\",\n",
    "        \"Unknown\": \"unknown\"\n",
    "    })\n",
    "    \n",
    "    # Print some statistics\n",
    "    print(f\"Perturbation types: {good_cells['perturbation_name'].nunique()} unique perturbations\")\n",
    "    print(f\"Targeting distribution: {good_cells['targeting'].value_counts().to_dict()}\")\n",
    "    \n",
    "    return good_cells\n",
    "\n",
    "def create_anndata_from_mtx(data_dir, features, barcodes, cell_identities_processed):\n",
    "    \"\"\"\n",
    "    Create an AnnData object from the matrix.mtx.gz file.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing the 10x Genomics data files\n",
    "        features: DataFrame containing gene information\n",
    "        barcodes: DataFrame containing cell barcodes\n",
    "        cell_identities_processed: DataFrame containing processed cell identity information\n",
    "    \n",
    "    Returns:\n",
    "        AnnData object\n",
    "    \"\"\"\n",
    "    print(\"Creating AnnData object from matrix.mtx.gz...\")\n",
    "    \n",
    "    # Check if matrix file exists\n",
    "    mtx_file = Path(data_dir) / 'matrix.mtx.gz'\n",
    "    if not mtx_file.exists():\n",
    "        raise FileNotFoundError(f\"Matrix file not found: {mtx_file}\")\n",
    "    \n",
    "    # Import scipy here to avoid potential import issues\n",
    "    import scipy.io\n",
    "    import scipy.sparse\n",
    "    \n",
    "    # Load the matrix\n",
    "    print(\"Loading matrix.mtx.gz (this may take a while)...\")\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        mat = scipy.io.mmread(str(mtx_file))\n",
    "        print(f\"Matrix loaded in {time.time() - start_time:.2f} seconds\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading matrix with mmread: {e}\")\n",
    "        print(\"Trying alternative loading method...\")\n",
    "        # Alternative loading method\n",
    "        try:\n",
    "            with gzip.open(mtx_file, 'rb') as f:\n",
    "                # Skip header lines\n",
    "                for line in f:\n",
    "                    if not line.startswith(b'%'):\n",
    "                        break\n",
    "                # Read dimensions\n",
    "                dims = line.decode().strip().split()\n",
    "                n_genes, n_cells, n_entries = map(int, dims)\n",
    "                \n",
    "                # Initialize sparse matrix in COO format\n",
    "                rows = np.zeros(n_entries, dtype=np.int32)\n",
    "                cols = np.zeros(n_entries, dtype=np.int32)\n",
    "                data = np.zeros(n_entries, dtype=np.float32)\n",
    "                \n",
    "                # Read entries\n",
    "                for i in range(n_entries):\n",
    "                    if i % 1000000 == 0:\n",
    "                        print(f\"  Loaded {i}/{n_entries} entries...\")\n",
    "                    line = f.readline().decode().strip().split()\n",
    "                    rows[i] = int(line[0]) - 1  # 1-based to 0-based indexing\n",
    "                    cols[i] = int(line[1]) - 1\n",
    "                    data[i] = float(line[2])\n",
    "                \n",
    "                mat = scipy.sparse.coo_matrix((data, (rows, cols)), shape=(n_genes, n_cells))\n",
    "                print(f\"Matrix loaded in {time.time() - start_time:.2f} seconds\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Error with alternative loading method: {e2}\")\n",
    "            raise RuntimeError(f\"Failed to load matrix file: {e} / {e2}\")\n",
    "    \n",
    "    # Convert to CSR format for efficient operations\n",
    "    print(\"Converting to CSR format...\")\n",
    "    mat = scipy.sparse.csr_matrix(mat)\n",
    "    \n",
    "    # Verify dimensions\n",
    "    if mat.shape[0] != len(features) or mat.shape[1] != len(barcodes):\n",
    "        print(f\"Warning: Matrix dimensions ({mat.shape}) don't match features ({len(features)}) and barcodes ({len(barcodes)})\")\n",
    "    \n",
    "    # Create AnnData object\n",
    "    print(\"Creating AnnData object...\")\n",
    "    adata = ad.AnnData(\n",
    "        X=mat.T,  # Transpose because 10x matrix is genes x cells\n",
    "        obs=pd.DataFrame(index=barcodes['barcode']),\n",
    "        var=pd.DataFrame(index=features['gene_symbol'])\n",
    "    )\n",
    "    \n",
    "    # Add gene_ids as a column in var\n",
    "    adata.var['gene_id'] = features['gene_id'].values\n",
    "    \n",
    "    # Ensure var_names are unique\n",
    "    adata.var_names_make_unique()\n",
    "    \n",
    "    # Filter the AnnData object to include only cells with good coverage\n",
    "    good_barcodes = cell_identities_processed['cell_barcode'].values\n",
    "    print(f\"Filtering to {len(good_barcodes)} cells with good coverage...\")\n",
    "    adata = adata[adata.obs.index.isin(good_barcodes)].copy()\n",
    "    \n",
    "    # Create a mapping from barcode to row index in cell_identities_processed\n",
    "    barcode_to_idx = {bc: i for i, bc in enumerate(cell_identities_processed['cell_barcode'])}\n",
    "    \n",
    "    # Create a list of indices in cell_identities_processed for each barcode in adata\n",
    "    indices = [barcode_to_idx.get(bc) for bc in adata.obs.index]\n",
    "    \n",
    "    # Filter out None values (barcodes not in cell_identities_processed)\n",
    "    valid_cells = [i for i, idx in enumerate(indices) if idx is not None]\n",
    "    valid_indices = [idx for idx in indices if idx is not None]\n",
    "    \n",
    "    # Filter adata to include only valid cells\n",
    "    adata = adata[valid_cells].copy()\n",
    "    \n",
    "    # Add metadata to adata.obs\n",
    "    print(\"Adding metadata to AnnData object...\")\n",
    "    for col in ['perturbation_name', 'condition', 'targeting', 'guide_identity', 'number_of_guides']:\n",
    "        if col in cell_identities_processed.columns:\n",
    "            adata.obs[col] = cell_identities_processed.iloc[valid_indices][col].values\n",
    "    \n",
    "    # Add standardized metadata fields\n",
    "    adata.obs['organism'] = 'Homo sapiens'\n",
    "    adata.obs['cell_type'] = 'Lung epithelial cells'  # Calu-3 cells are lung epithelial cells\n",
    "    adata.obs['crispr_type'] = 'CRISPRi'\n",
    "    adata.obs['cancer_type'] = 'Lung Cancer'  # Calu-3 is a lung cancer cell line\n",
    "    \n",
    "    # Add dataset information to uns\n",
    "    adata.uns['dataset'] = {\n",
    "        'id': 'GSE208240',\n",
    "        'title': 'Systematic functional interrogation of SARS-CoV-2 host factors using Perturb-seq',\n",
    "        'description': 'CRISPRi perturbation of host factors in Calu-3 cells infected with SARS-CoV-2',\n",
    "        'organism': 'Homo sapiens',\n",
    "        'publication': 'Sunshine S, et al. Systematic functional interrogation of SARS-CoV-2 host factors using Perturb-seq. Nat Commun 2023.'\n",
    "    }\n",
    "    \n",
    "    return adata\n",
    "\n",
    "def run_pipeline(data_dir=\".\"):\n",
    "    \"\"\"Run the full data processing pipeline in a Jupyter-friendly way.\"\"\"\n",
    "    try:\n",
    "        # Download or locate the dataset\n",
    "        data_path = download_dataset(data_dir)\n",
    "        \n",
    "        # Load metadata\n",
    "        features, barcodes, cell_identities = load_metadata(data_path)\n",
    "        \n",
    "        # Process cell identities\n",
    "        cell_identities_processed = process_cell_identities(cell_identities)\n",
    "        \n",
    "        # Create AnnData object\n",
    "        adata = create_anndata_from_mtx(data_path, features, barcodes, cell_identities_processed)\n",
    "        \n",
    "        # Save the harmonized data to an h5ad file\n",
    "        output_file = os.path.join(data_dir, \"GSE208240_harmonized.h5ad\")\n",
    "        print(f\"Saving harmonized data to {output_file}...\")\n",
    "        adata.write(output_file)\n",
    "        \n",
    "        print(f\"Harmonized data saved to {output_file}\")\n",
    "        print(f\"Final data shape: {adata.shape}\")\n",
    "        print(f\"Metadata fields: {list(adata.obs.columns)}\")\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(\"\\nSummary Statistics:\")\n",
    "        print(f\"Number of cells: {adata.n_obs}\")\n",
    "        print(f\"Number of genes: {adata.n_vars}\")\n",
    "        print(f\"Number of unique perturbations: {adata.obs['perturbation_name'].nunique()}\")\n",
    "        print(f\"Condition distribution: {adata.obs['condition'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return adata\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing dataset: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Run the pipeline. You can change data_dir as needed.\n",
    "adata = run_pipeline(data_dir=\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "703ca59a-be42-409e-b477-a3a0fbea9dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
