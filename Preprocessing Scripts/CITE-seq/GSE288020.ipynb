{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b61713b-4c06-4222-8853-da54ecde594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# 1. Imports and setup\n",
    "# --------------------------------------------------------------------------------\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import anndata as ad\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import gzip\n",
    "import tarfile\n",
    "import time\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Check for GEOparse or install Bioconductor packages if needed\n",
    "try:\n",
    "    # Try to import GEOparse for Python\n",
    "    import GEOparse\n",
    "    have_geoparse = True\n",
    "except ImportError:\n",
    "    have_geoparse = False\n",
    "    print(\"GEOparse package not available. Will use alternative download methods.\")\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 2. Constants and metadata\n",
    "# --------------------------------------------------------------------------------\n",
    "ACCESSION_NUMBER = \"GSE288020\"\n",
    "ORGANISM = \"Homo sapiens\"\n",
    "\n",
    "# Sample metadata mapping\n",
    "SAMPLE_METADATA = {\n",
    "    'GSM8757538': {'sample_type': 'MGUS', 'immune_age': 'Old', 'sample_name': 'R001'},\n",
    "    'GSM8757539': {'sample_type': 'MGUS', 'immune_age': 'Young', 'sample_name': 'R003'},\n",
    "    'GSM8757540': {'sample_type': 'MGUS', 'immune_age': 'Young', 'sample_name': 'R005'},\n",
    "    'GSM8757541': {'sample_type': 'MGUS', 'immune_age': 'Old', 'sample_name': 'R006'},\n",
    "    'GSM8757542': {'sample_type': 'MGUS', 'immune_age': 'Young', 'sample_name': 'R008'},\n",
    "    'GSM8757543': {'sample_type': 'MGUS', 'immune_age': 'Young', 'sample_name': 'R009'},\n",
    "    'GSM8757544': {'sample_type': 'MGUS', 'immune_age': 'Young', 'sample_name': 'R010'},\n",
    "    'GSM8757545': {'sample_type': 'MGUS', 'immune_age': 'Old', 'sample_name': 'R013'},\n",
    "    'GSM8757546': {'sample_type': 'MGUS', 'immune_age': 'Old', 'sample_name': 'R014'},\n",
    "    'GSM8757547': {'sample_type': 'MGUS', 'immune_age': 'Young', 'sample_name': 'R015'},\n",
    "    'GSM8757548': {'sample_type': 'MGUS', 'immune_age': 'Old', 'sample_name': 'R016'},\n",
    "    'GSM8757549': {'sample_type': 'MGUS', 'immune_age': 'Old', 'sample_name': 'R020'},\n",
    "    'GSM8757550': {'sample_type': 'MGUS', 'immune_age': 'Young', 'sample_name': 'R023'},\n",
    "    'GSM8757551': {'sample_type': 'MGUS', 'immune_age': 'Old', 'sample_name': 'R024'},\n",
    "    'GSM8757552': {'sample_type': 'MM', 'immune_age': 'Young', 'sample_name': 'E2228'},\n",
    "    'GSM8757553': {'sample_type': 'MM', 'immune_age': 'Old', 'sample_name': 'E2238'},\n",
    "    'GSM8757554': {'sample_type': 'MM', 'immune_age': 'Young', 'sample_name': 'E2242'},\n",
    "    'GSM8757555': {'sample_type': 'MM', 'immune_age': 'Old', 'sample_name': 'E2243'},\n",
    "    'GSM8757556': {'sample_type': 'MM', 'immune_age': 'Young', 'sample_name': 'E2263'},\n",
    "    'GSM8757557': {'sample_type': 'MM', 'immune_age': 'Old', 'sample_name': 'E2324'},\n",
    "    'GSM8757558': {'sample_type': 'MM', 'immune_age': 'Young', 'sample_name': 'E2326'},\n",
    "    'GSM8757559': {'sample_type': 'MM', 'immune_age': 'Old', 'sample_name': 'E2328'},\n",
    "    'GSM8757560': {'sample_type': 'MM', 'immune_age': 'Old', 'sample_name': 'E2329'}\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 3. Download Functions - Multiple approaches\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "def download_family_soft_file(geo_id, output_dir):\n",
    "    \"\"\"\n",
    "    Download the family SOFT file for a GEO Series\n",
    "    Args:\n",
    "        geo_id (str): GEO accession ID (e.g., GSE288020)\n",
    "        output_dir (str): Directory to save the downloaded file\n",
    "    Returns:\n",
    "        str: Path to the downloaded file\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file = os.path.join(output_dir, f\"{geo_id}_family.soft.gz\")\n",
    "    \n",
    "    if os.path.exists(output_file):\n",
    "        logger.info(f\"Family SOFT file already exists at {output_file}\")\n",
    "        return output_file\n",
    "    \n",
    "    # Construct URL for family SOFT file\n",
    "    url = f\"https://ftp.ncbi.nlm.nih.gov/geo/series/{geo_id[:-3]}nnn/{geo_id}/soft/{geo_id}_family.soft.gz\"\n",
    "    \n",
    "    logger.info(f\"Downloading family SOFT file from {url}\")\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with open(output_file, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        \n",
    "        logger.info(f\"Downloaded family SOFT file to {output_file}\")\n",
    "        return output_file\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to download family SOFT file: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_supplementary_file_urls(soft_file):\n",
    "    \"\"\"\n",
    "    Extract supplementary file URLs from a GEO SOFT file\n",
    "    Args:\n",
    "        soft_file (str): Path to the GEO SOFT file\n",
    "    Returns:\n",
    "        list: List of supplementary file URLs\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    \n",
    "    if soft_file.endswith('.gz'):\n",
    "        open_func = gzip.open\n",
    "        mode = 'rt'\n",
    "    else:\n",
    "        open_func = open\n",
    "        mode = 'r'\n",
    "    \n",
    "    try:\n",
    "        with open_func(soft_file, mode) as f:\n",
    "            for line in f:\n",
    "                if line.startswith('!Series_supplementary_file') or line.startswith('!Sample_supplementary_file'):\n",
    "                    # Extract URL from line\n",
    "                    url = line.strip().split(' = ')[1]\n",
    "                    urls.append(url)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error parsing SOFT file: {e}\")\n",
    "    \n",
    "    logger.info(f\"Found {len(urls)} supplementary file URLs\")\n",
    "    return urls\n",
    "\n",
    "def download_file(url, output_path, max_retries=3, retry_delay=5):\n",
    "    \"\"\"\n",
    "    Download a file from a URL with retries\n",
    "    Args:\n",
    "        url (str): URL to download\n",
    "        output_path (str): Path to save the file\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "        retry_delay (int): Delay between retries in seconds\n",
    "    Returns:\n",
    "        bool: True if download successful, False otherwise\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            logger.info(f\"Downloading {url} to {output_path} (Attempt {attempt + 1})\")\n",
    "            response = requests.get(url, stream=True, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            with open(output_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            \n",
    "            logger.info(f\"Successfully downloaded {url}\")\n",
    "            return True\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.warning(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                logger.error(f\"Failed to download {url} after {max_retries} attempts\")\n",
    "                return False\n",
    "\n",
    "def try_alternative_url_format(url, output_path):\n",
    "    \"\"\"\n",
    "    Try alternative URL format if original fails\n",
    "    Args:\n",
    "        url (str): Original URL\n",
    "        output_path (str): Path to save the file\n",
    "    Returns:\n",
    "        bool: True if download successful, False otherwise\n",
    "    \"\"\"\n",
    "    # Try direct FTP format\n",
    "    if url.startswith('ftp://'):\n",
    "        # Try HTTPS alternative\n",
    "        https_url = url.replace('ftp://', 'https://')\n",
    "        if download_file(https_url, output_path):\n",
    "            return True\n",
    "    \n",
    "    # Try GEO direct URL format\n",
    "    if 'geo/samples/' in url or 'geo/series/' in url:\n",
    "        # Parse the accession from the URL\n",
    "        match = re.search(r'(GSM\\d+|GSE\\d+)', url)\n",
    "        if match:\n",
    "            accession = match.group(1)\n",
    "            filename = os.path.basename(url)\n",
    "            \n",
    "            if accession.startswith('GSM'):\n",
    "                alternative_url = f\"https://www.ncbi.nlm.nih.gov/geo/download/?acc={accession}&format=file&file={filename}\"\n",
    "            else:  # GSE\n",
    "                alternative_url = f\"https://www.ncbi.nlm.nih.gov/geo/download/?acc={accession}&format=file\"\n",
    "            \n",
    "            logger.info(f\"Trying alternative URL: {alternative_url}\")\n",
    "            if download_file(alternative_url, output_path):\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def attempt_geoparse_download(geo_id, output_dir):\n",
    "    \"\"\"\n",
    "    Attempt to download files using GEOparse\n",
    "    Args:\n",
    "        geo_id (str): GEO accession ID\n",
    "        output_dir (str): Directory to save the downloaded files\n",
    "    Returns:\n",
    "        list: List of paths to downloaded files\n",
    "    \"\"\"\n",
    "    if not have_geoparse:\n",
    "        logger.warning(\"GEOparse not available, skipping this download method\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Attempting to download {geo_id} using GEOparse\")\n",
    "        gse = GEOparse.get_GEO(geo=geo_id, destdir=output_dir)\n",
    "        \n",
    "        # Try to download supplementary files\n",
    "        gse.download_supplementary_files(directory=output_dir)\n",
    "        \n",
    "        # Get list of downloaded files\n",
    "        downloaded_files = glob.glob(os.path.join(output_dir, f\"{geo_id}_*\"))\n",
    "        logger.info(f\"Downloaded {len(downloaded_files)} files using GEOparse\")\n",
    "        return downloaded_files\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"GEOparse download failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def attempt_r_geoquery_download(geo_id, output_dir):\n",
    "    \"\"\"\n",
    "    Try to download GEO files using R's GEOquery package\n",
    "    Args:\n",
    "        geo_id (str): GEO accession ID\n",
    "        output_dir (str): Directory to save the downloaded files\n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    # Check if R is installed\n",
    "    try:\n",
    "        r_version = subprocess.check_output(['R', '--version'], text=True)\n",
    "        logger.info(f\"R is available: {r_version.split()[0]} {r_version.split()[1]}\")\n",
    "    except:\n",
    "        logger.warning(\"R is not available, skipping GEOquery download\")\n",
    "        return False\n",
    "    \n",
    "    # Create R script for download\n",
    "    r_script = os.path.join(output_dir, \"download_geo.R\")\n",
    "    \n",
    "    # Convert Windows path to R-compatible path (forward slashes)\n",
    "    r_output_dir = output_dir.replace('\\\\', '/')\n",
    "    \n",
    "    with open(r_script, \"w\") as f:\n",
    "        f.write(f'''\n",
    "# Install BiocManager if not already installed\n",
    "if (!requireNamespace(\"BiocManager\", quietly = TRUE))\n",
    "    install.packages(\"BiocManager\", repos = \"https://cran.r-project.org\")\n",
    "\n",
    "# Install GEOquery if not already installed\n",
    "if (!requireNamespace(\"GEOquery\", quietly = TRUE))\n",
    "    BiocManager::install(\"GEOquery\")\n",
    "\n",
    "library(GEOquery)\n",
    "\n",
    "# Set timeout to a larger value\n",
    "options(timeout = 600)\n",
    "\n",
    "# Set download methods\n",
    "options(download.file.method.GEOquery = \"auto\")\n",
    "\n",
    "# Download supplementary files\n",
    "print(\"Downloading {geo_id} supplementary files...\")\n",
    "try({{\n",
    "    files <- getGEOSuppFiles(\"{geo_id}\", baseDir = \"{r_output_dir}\")\n",
    "    print(\"Downloaded files:\")\n",
    "    print(files)\n",
    "}}, silent = FALSE)\n",
    "\n",
    "# Also try to get the Series Matrix file\n",
    "print(\"Downloading {geo_id} Series Matrix files...\")\n",
    "try({{\n",
    "    gse <- getGEO(\"{geo_id}\", GSEMatrix = TRUE, destdir = \"{r_output_dir}\")\n",
    "    print(\"Downloaded Series Matrix files\")\n",
    "}}, silent = FALSE)\n",
    "''')\n",
    "    \n",
    "    # Run the R script\n",
    "    logger.info(f\"Running R script to download {geo_id} using GEOquery\")\n",
    "    try:\n",
    "        result = subprocess.run(['Rscript', r_script], capture_output=True, text=True, timeout=1200)\n",
    "        logger.info(f\"R script output: {result.stdout}\")\n",
    "        if result.stderr:\n",
    "            logger.warning(f\"R script errors: {result.stderr}\")\n",
    "        \n",
    "        # Check if files were downloaded\n",
    "        files = glob.glob(os.path.join(output_dir, f\"{geo_id}*\"))\n",
    "        if files:\n",
    "            logger.info(f\"GEOquery download successful, found {len(files)} files\")\n",
    "            return True\n",
    "        else:\n",
    "            logger.warning(\"GEOquery ran but no files were found\")\n",
    "            return False\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        logger.error(\"R script timed out after 20 minutes\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error running R script: {e}\")\n",
    "        return False\n",
    "\n",
    "def download_geo_supplementary_files(geo_id, output_dir, recursive_samples=True):\n",
    "    \"\"\"\n",
    "    Download all supplementary files for a GEO Series\n",
    "    Args:\n",
    "        geo_id (str): GEO accession ID\n",
    "        output_dir (str): Directory to save the downloaded files\n",
    "        recursive_samples (bool): Whether to also download supplementary files for samples\n",
    "    Returns:\n",
    "        list: List of paths to downloaded files\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    downloaded_files = []\n",
    "    \n",
    "    # First try GEOparse if available\n",
    "    geoparse_files = attempt_geoparse_download(geo_id, output_dir)\n",
    "    if geoparse_files:\n",
    "        downloaded_files.extend(geoparse_files)\n",
    "        return downloaded_files\n",
    "    \n",
    "    # Fallback to manual download\n",
    "    # 1. Download the family SOFT file\n",
    "    soft_file = download_family_soft_file(geo_id, output_dir)\n",
    "    if not soft_file:\n",
    "        logger.error(\"Failed to download family SOFT file, cannot proceed with manual download\")\n",
    "        return downloaded_files\n",
    "    \n",
    "    # 2. Extract supplementary file URLs\n",
    "    urls = extract_supplementary_file_urls(soft_file)\n",
    "    \n",
    "    # 3. Download each file\n",
    "    for url in urls:\n",
    "        filename = os.path.basename(url)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        \n",
    "        if os.path.exists(output_path):\n",
    "            logger.info(f\"File {filename} already exists, skipping download\")\n",
    "            downloaded_files.append(output_path)\n",
    "            continue\n",
    "        \n",
    "        success = download_file(url, output_path)\n",
    "        if not success:\n",
    "            success = try_alternative_url_format(url, output_path)\n",
    "        \n",
    "        if success:\n",
    "            downloaded_files.append(output_path)\n",
    "    \n",
    "    # 4. If recursive, also check for supplementary files for each sample\n",
    "    if recursive_samples and geo_id.startswith('GSE'):\n",
    "        # Get all sample IDs from the SOFT file\n",
    "        sample_ids = []\n",
    "        with gzip.open(soft_file, 'rt') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('^SAMPLE = '):\n",
    "                    sample_id = line.strip().split(' = ')[1]\n",
    "                    sample_ids.append(sample_id)\n",
    "        \n",
    "        logger.info(f\"Found {len(sample_ids)} samples, checking for supplementary files\")\n",
    "        \n",
    "        for sample_id in sample_ids:\n",
    "            sample_dir = os.path.join(output_dir, sample_id)\n",
    "            os.makedirs(sample_dir, exist_ok=True)\n",
    "            \n",
    "            sample_files = download_geo_supplementary_files(sample_id, sample_dir, recursive_samples=False)\n",
    "            downloaded_files.extend(sample_files)\n",
    "    \n",
    "    return downloaded_files\n",
    "\n",
    "def get_h5_files(data_dir, sample_ids=None, create_dummy=False):\n",
    "    \"\"\"\n",
    "    Get list of h5 files in the data directory or create dummy files for testing\n",
    "    Args:\n",
    "        data_dir (str): Directory containing h5 files\n",
    "        sample_ids (list): List of sample IDs to look for\n",
    "        create_dummy (bool): Whether to create dummy h5 files for testing\n",
    "    Returns:\n",
    "        list: List of paths to h5 files\n",
    "    \"\"\"\n",
    "    if sample_ids is None:\n",
    "        sample_ids = list(SAMPLE_METADATA.keys())\n",
    "    \n",
    "    h5_files = []\n",
    "    missing_files = []\n",
    "    \n",
    "    # Look for h5 files matching expected patterns\n",
    "    for sample_id in sample_ids:\n",
    "        sample_name = SAMPLE_METADATA[sample_id]['sample_name']\n",
    "        expected_patterns = [\n",
    "            f\"{sample_id}_{sample_name}*filtered_feature_bc_matrix.h5\",\n",
    "            f\"{sample_id}*filtered_feature_bc_matrix.h5\",\n",
    "            f\"{sample_name}*filtered_feature_bc_matrix.h5\",\n",
    "            f\"{sample_id}*.h5\",\n",
    "            f\"{sample_id}_*matrix*.h5\"\n",
    "        ]\n",
    "        \n",
    "        found = False\n",
    "        for pattern in expected_patterns:\n",
    "            matching_files = glob.glob(os.path.join(data_dir, pattern))\n",
    "            if matching_files:\n",
    "                h5_files.extend(matching_files)\n",
    "                found = True\n",
    "                break\n",
    "                \n",
    "        if not found:\n",
    "            missing_files.append(sample_id)\n",
    "    \n",
    "    # If create_dummy is True and there are missing files, create dummy h5 files\n",
    "    if create_dummy and missing_files:\n",
    "        logger.warning(f\"Creating dummy h5 files for {len(missing_files)} samples\")\n",
    "        for sample_id in missing_files:\n",
    "            sample_name = SAMPLE_METADATA[sample_id]['sample_name']\n",
    "            dummy_file = os.path.join(data_dir, f\"{sample_id}_{sample_name}_dummy_matrix.h5\")\n",
    "            \n",
    "            # Create a minimal h5 file for testing\n",
    "            with h5py.File(dummy_file, 'w') as f:\n",
    "                # Create a matrix group\n",
    "                matrix = f.create_group(\"matrix\")\n",
    "                \n",
    "                # Add small test data\n",
    "                matrix.create_dataset(\"data\", data=np.array([1, 2, 3, 4]))\n",
    "                matrix.create_dataset(\"indices\", data=np.array([0, 1, 0, 1]))\n",
    "                matrix.create_dataset(\"indptr\", data=np.array([0, 2, 4]))\n",
    "                \n",
    "                # Add barcodes and features\n",
    "                barcodes = [f\"CELL_{i}\" for i in range(2)]\n",
    "                features_id = [f\"GENE_{i}\" for i in range(2)]\n",
    "                features_name = [f\"Gene_{i}\" for i in range(2)]\n",
    "                features_type = [\"Gene Expression\", \"Gene Expression\"]\n",
    "                \n",
    "                # Convert to bytes\n",
    "                barcodes_bytes = [s.encode('utf-8') for s in barcodes]\n",
    "                features_id_bytes = [s.encode('utf-8') for s in features_id]\n",
    "                features_name_bytes = [s.encode('utf-8') for s in features_name]\n",
    "                features_type_bytes = [s.encode('utf-8') for s in features_type]\n",
    "                \n",
    "                # Add to file\n",
    "                matrix.create_dataset(\"barcodes\", data=barcodes_bytes)\n",
    "                \n",
    "                # Create features group\n",
    "                features = matrix.create_group(\"features\")\n",
    "                features.create_dataset(\"id\", data=features_id_bytes)\n",
    "                features.create_dataset(\"name\", data=features_name_bytes)\n",
    "                features.create_dataset(\"feature_type\", data=features_type_bytes)\n",
    "            \n",
    "            logger.info(f\"Created dummy h5 file at {dummy_file}\")\n",
    "            h5_files.append(dummy_file)\n",
    "    \n",
    "    return h5_files\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 4. Reading and processing h5 files\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "def read_10x_h5(file_path):\n",
    "    \"\"\"\n",
    "    Read 10X h5 file and extract gene expression and protein expression data.\n",
    "    Args:\n",
    "        file_path (str): Path to h5 file\n",
    "    Returns:\n",
    "        tuple: (gene_data, protein_data, metadata)\n",
    "    \"\"\"\n",
    "    logger.info(f\"Reading {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            # Check if this is a dummy file\n",
    "            is_dummy = 'dummy' in os.path.basename(file_path).lower()\n",
    "            \n",
    "            # Extract barcodes and features\n",
    "            barcodes = [b.decode('utf-8') for b in f['matrix']['barcodes'][:]]\n",
    "            feature_ids = [g.decode('utf-8') for g in f['matrix']['features']['id'][:]]\n",
    "            feature_names = [g.decode('utf-8') for g in f['matrix']['features']['name'][:]]\n",
    "            feature_types = [g.decode('utf-8') for g in f['matrix']['features']['feature_type'][:]]\n",
    "            \n",
    "            # Extract sample_id and sample_name from the file name\n",
    "            base_name = os.path.basename(file_path)\n",
    "            parts = base_name.split('_')\n",
    "            sample_id = parts[0]\n",
    "            sample_name = parts[1] if len(parts) > 1 else \"Unknown\"\n",
    "            \n",
    "            metadata = {'gsm_id': sample_id, 'sample_name': sample_name}\n",
    "            \n",
    "            # Split data by feature_type\n",
    "            gene_indices = np.array([i for i, ft in enumerate(feature_types) if ft == 'Gene Expression'])\n",
    "            protein_indices = np.array([i for i, ft in enumerate(feature_types) if ft == 'Antibody Capture'])\n",
    "            \n",
    "            # If no protein indices, create dummy ones for testing\n",
    "            if len(protein_indices) == 0 and is_dummy:\n",
    "                logger.warning(f\"No protein data found in {file_path}, creating dummy protein data\")\n",
    "                protein_indices = np.array([0])  # Just use the first gene as a dummy protein\n",
    "            \n",
    "            gene_idx_map = {idx: i for i, idx in enumerate(gene_indices)}\n",
    "            protein_idx_map = {idx: i for i, idx in enumerate(protein_indices)}\n",
    "            \n",
    "            data = f['matrix']['data'][:]\n",
    "            indices = f['matrix']['indices'][:]\n",
    "            indptr = f['matrix']['indptr'][:]\n",
    "            \n",
    "            gene_data_arr = []\n",
    "            gene_indices_arr = []\n",
    "            gene_indptr = [0]\n",
    "            \n",
    "            protein_data_arr = []\n",
    "            protein_indices_arr = []\n",
    "            protein_indptr = [0]\n",
    "            \n",
    "            for i in range(len(indptr) - 1):\n",
    "                start, end = indptr[i], indptr[i+1]\n",
    "                cell_indices = indices[start:end]\n",
    "                cell_data = data[start:end]\n",
    "                \n",
    "                # Gene\n",
    "                gene_mask = np.isin(cell_indices, gene_indices)\n",
    "                gene_cell_indices = cell_indices[gene_mask]\n",
    "                gene_cell_data = cell_data[gene_mask]\n",
    "                \n",
    "                if len(gene_cell_indices) > 0:\n",
    "                    gene_cell_indices_mapped = np.array([gene_idx_map[idx] for idx in gene_cell_indices])\n",
    "                    \n",
    "                    # Accumulate gene\n",
    "                    gene_data_arr.append(gene_cell_data)\n",
    "                    gene_indices_arr.append(gene_cell_indices_mapped)\n",
    "                    gene_indptr.append(gene_indptr[-1] + len(gene_cell_data))\n",
    "                else:\n",
    "                    gene_indptr.append(gene_indptr[-1])\n",
    "                \n",
    "                # Protein\n",
    "                protein_mask = np.isin(cell_indices, protein_indices)\n",
    "                protein_cell_indices = cell_indices[protein_mask]\n",
    "                protein_cell_data = cell_data[protein_mask]\n",
    "                \n",
    "                if len(protein_cell_indices) > 0:\n",
    "                    protein_cell_indices_mapped = np.array([protein_idx_map[idx] for idx in protein_cell_indices])\n",
    "                    \n",
    "                    # Accumulate protein\n",
    "                    protein_data_arr.append(protein_cell_data)\n",
    "                    protein_indices_arr.append(protein_cell_indices_mapped)\n",
    "                    protein_indptr.append(protein_indptr[-1] + len(protein_cell_data))\n",
    "                else:\n",
    "                    protein_indptr.append(protein_indptr[-1])\n",
    "            \n",
    "            # Concatenate\n",
    "            gene_data_concat = np.concatenate(gene_data_arr) if gene_data_arr else np.array([])\n",
    "            gene_indices_concat = np.concatenate(gene_indices_arr) if gene_indices_arr else np.array([])\n",
    "            protein_data_concat = np.concatenate(protein_data_arr) if protein_data_arr else np.array([])\n",
    "            protein_indices_concat = np.concatenate(protein_indices_arr) if protein_indices_arr else np.array([])\n",
    "            \n",
    "            # Build sparse matrices\n",
    "            gene_matrix = sparse.csr_matrix(\n",
    "                (gene_data_concat, gene_indices_concat, gene_indptr),\n",
    "                shape=(len(barcodes), len(gene_indices))\n",
    "            )\n",
    "            \n",
    "            protein_matrix = sparse.csr_matrix(\n",
    "                (protein_data_concat, protein_indices_concat, protein_indptr),\n",
    "                shape=(len(barcodes), len(protein_indices) or 1)\n",
    "            )\n",
    "            \n",
    "            # Subset feature IDs and names\n",
    "            gene_ids = [feature_ids[i] for i in gene_indices]\n",
    "            gene_names = [feature_names[i] for i in gene_indices]\n",
    "            \n",
    "            if len(protein_indices) > 0:\n",
    "                protein_ids = [feature_ids[i] for i in protein_indices]\n",
    "                protein_names = [feature_names[i] for i in protein_indices]\n",
    "            else:\n",
    "                # Create dummy protein data for testing\n",
    "                protein_ids = ['dummy_protein']\n",
    "                protein_names = ['Dummy Protein']\n",
    "            \n",
    "            gene_data = {\n",
    "                'matrix': gene_matrix,\n",
    "                'feature_ids': gene_ids,\n",
    "                'feature_names': gene_names,\n",
    "                'barcodes': barcodes\n",
    "            }\n",
    "            \n",
    "            protein_data = {\n",
    "                'matrix': protein_matrix,\n",
    "                'feature_ids': protein_ids,\n",
    "                'feature_names': protein_names,\n",
    "                'barcodes': barcodes\n",
    "            }\n",
    "            \n",
    "            return gene_data, protein_data, metadata\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading h5 file {file_path}: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def get_sample_metadata(sample_id):\n",
    "    \"\"\"\n",
    "    Get metadata for a sample based on its ID.\n",
    "    Args:\n",
    "        sample_id (str): Sample ID (e.g., GSM8757538)\n",
    "    Returns:\n",
    "        dict: Metadata dictionary\n",
    "    \"\"\"\n",
    "    if sample_id in SAMPLE_METADATA:\n",
    "        metadata = SAMPLE_METADATA[sample_id].copy()\n",
    "        metadata['gsm_id'] = sample_id\n",
    "        metadata['organism'] = ORGANISM\n",
    "        \n",
    "        metadata['cell_type'] = 'Bone Marrow Cells' \n",
    "        metadata['cancer_type'] = 'Multiple Myeloma' if metadata['sample_type'] == 'MM' else 'MGUS'\n",
    "        metadata['condition'] = 'Control'\n",
    "        metadata['crispr_type'] = 'None'\n",
    "        metadata['perturbation_name'] = 'None'\n",
    "        return metadata\n",
    "    else:\n",
    "        logger.warning(f\"No metadata found for sample {sample_id}\")\n",
    "        return {\n",
    "            'gsm_id': sample_id,\n",
    "            'organism': ORGANISM,\n",
    "            'sample_name': 'Unknown',\n",
    "            'sample_type': 'Unknown',\n",
    "            'immune_age': 'Unknown',\n",
    "            'cell_type': 'Bone Marrow Cells',\n",
    "            'cancer_type': 'Unknown',\n",
    "            'condition': 'Control',\n",
    "            'crispr_type': 'None',\n",
    "            'perturbation_name': 'None'\n",
    "        }\n",
    "\n",
    "\n",
    "def create_anndata(data_dict, metadata, data_type):\n",
    "    \"\"\"\n",
    "    Create AnnData object from data dictionary and metadata.\n",
    "    Args:\n",
    "        data_dict (dict): Data dictionary with matrix, feature_ids, feature_names, barcodes\n",
    "        metadata (dict): Metadata dictionary\n",
    "        data_type (str): Type of data ('gene' or 'protein')\n",
    "    Returns:\n",
    "        AnnData: AnnData object\n",
    "    \"\"\"\n",
    "    # Check if data_dict is valid\n",
    "    if data_dict is None:\n",
    "        logger.error(f\"Invalid data dictionary for {metadata['gsm_id']}\")\n",
    "        return None\n",
    "    \n",
    "    adata = ad.AnnData(X=data_dict['matrix'])\n",
    "    adata.obs_names = [f\"{metadata['sample_name']}_{bc}\" for bc in data_dict['barcodes']]\n",
    "    \n",
    "    # Create var DataFrame\n",
    "    var_df = pd.DataFrame(index=data_dict['feature_names'])\n",
    "    \n",
    "    # Make var names unique if duplicates\n",
    "    if len(var_df.index) != len(set(var_df.index)):\n",
    "        logger.warning(f\"Duplicate feature names found in {metadata['sample_name']}. Making them unique.\")\n",
    "        var_df.index = pd.Index([\n",
    "            f\"{name}_{i}\" if list(var_df.index).count(name) > 1 else name \n",
    "            for i, name in enumerate(var_df.index)\n",
    "        ])\n",
    "    \n",
    "    adata.var_names = var_df.index\n",
    "\n",
    "    # Clean up protein names if data_type is protein\n",
    "    if data_type == 'protein':\n",
    "        adata.var_names = [re.sub(r'_\\(.*\\)', '', name) for name in adata.var_names]\n",
    "    \n",
    "    adata.var['feature_id'] = data_dict['feature_ids']\n",
    "    \n",
    "    # Add metadata to obs\n",
    "    for key, value in metadata.items():\n",
    "        adata.obs[key] = value\n",
    "    \n",
    "    adata.uns['data_type'] = data_type\n",
    "    return adata\n",
    "\n",
    "\n",
    "def process_dataset(data_dir, output_dir, use_dummy=False):\n",
    "    \"\"\"\n",
    "    Process the dataset and create h5ad files.\n",
    "    Args:\n",
    "        data_dir (str): Directory containing h5 files\n",
    "        output_dir (str): Directory to save output files\n",
    "        use_dummy (bool): Whether to create dummy files if real ones are missing\n",
    "    Returns:\n",
    "        tuple: (gene_adata, protein_adata) - AnnData objects for gene and protein expression\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    temp_dir = os.path.join(output_dir, 'temp')\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    \n",
    "    # First, try to download data if it doesn't exist\n",
    "    try:\n",
    "        # Try to download using multiple methods\n",
    "        logger.info(f\"Checking for data in {data_dir}\")\n",
    "        h5_files = get_h5_files(data_dir)\n",
    "        \n",
    "        if not h5_files:\n",
    "            logger.info(f\"No h5 files found in {data_dir}. Attempting to download...\")\n",
    "            \n",
    "            # Try GEOquery (if R is available)\n",
    "            r_download_successful = attempt_r_geoquery_download(ACCESSION_NUMBER, data_dir)\n",
    "            \n",
    "            if not r_download_successful:\n",
    "                # Try manual downloads\n",
    "                downloaded_files = download_geo_supplementary_files(ACCESSION_NUMBER, data_dir)\n",
    "                \n",
    "                if not downloaded_files:\n",
    "                    logger.warning(\"All download methods failed. Using dummy data for testing if requested.\")\n",
    "                    \n",
    "                    if use_dummy:\n",
    "                        h5_files = get_h5_files(data_dir, create_dummy=True)\n",
    "                    else:\n",
    "                        logger.error(\"No data available and dummy mode not enabled. Cannot proceed.\")\n",
    "                        return None, None\n",
    "                else:\n",
    "                    # Check if any of the downloaded files are h5 files\n",
    "                    h5_files = [f for f in downloaded_files if f.endswith('.h5')]\n",
    "                    \n",
    "                    if not h5_files:\n",
    "                        logger.warning(\"Downloaded files don't include h5 files. Using dummy data if requested.\")\n",
    "                        \n",
    "                        if use_dummy:\n",
    "                            h5_files = get_h5_files(data_dir, create_dummy=True)\n",
    "                        else:\n",
    "                            logger.error(\"No h5 data available and dummy mode not enabled. Cannot proceed.\")\n",
    "                            return None, None\n",
    "            else:\n",
    "                # R download was successful, check for h5 files\n",
    "                h5_files = get_h5_files(data_dir)\n",
    "                \n",
    "                if not h5_files and use_dummy:\n",
    "                    logger.warning(\"R download successful but no h5 files found. Using dummy data.\")\n",
    "                    h5_files = get_h5_files(data_dir, create_dummy=True)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during data acquisition: {e}\")\n",
    "        if use_dummy:\n",
    "            logger.warning(\"Using dummy data due to error in data acquisition.\")\n",
    "            h5_files = get_h5_files(data_dir, create_dummy=True)\n",
    "        else:\n",
    "            return None, None\n",
    "    \n",
    "    # Now process the files we have\n",
    "    gene_adatas, protein_adatas = [], []\n",
    "    \n",
    "    for file_path in h5_files:\n",
    "        # Extract sample_id from filename\n",
    "        sample_id = os.path.basename(file_path).split('_')[0]\n",
    "        if not sample_id.startswith('GSM'):\n",
    "            # Try to infer sample_id from the filename or directory\n",
    "            for gsm_id in SAMPLE_METADATA.keys():\n",
    "                if gsm_id in file_path or SAMPLE_METADATA[gsm_id]['sample_name'] in file_path:\n",
    "                    sample_id = gsm_id\n",
    "                    break\n",
    "        \n",
    "        # Extract data\n",
    "        gene_data, protein_data, file_metadata = read_10x_h5(file_path)\n",
    "        \n",
    "        if gene_data is None:\n",
    "            logger.warning(f\"Skipping file {file_path} due to read error\")\n",
    "            continue\n",
    "        \n",
    "        # Get sample metadata\n",
    "        metadata = get_sample_metadata(sample_id)\n",
    "        \n",
    "        # Create AnnData\n",
    "        gene_adata = create_anndata(gene_data, metadata, 'gene')\n",
    "        protein_adata = create_anndata(protein_data, metadata, 'protein')\n",
    "        \n",
    "        if gene_adata is not None and protein_adata is not None:\n",
    "            gene_adatas.append(gene_adata)\n",
    "            protein_adatas.append(protein_adata)\n",
    "    \n",
    "    if not gene_adatas or not protein_adatas:\n",
    "        logger.error(\"No valid data files were processed. Cannot create AnnData objects.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Concatenate\n",
    "    logger.info(\"Concatenating gene expression data\")\n",
    "    gene_adata_combined = ad.concat(gene_adatas, join='outer', merge='same')\n",
    "    \n",
    "    logger.info(\"Concatenating protein expression data\")\n",
    "    protein_adata_combined = ad.concat(protein_adatas, join='outer', merge='same')\n",
    "    \n",
    "    # Make gene names unique if needed\n",
    "    if len(gene_adata_combined.var_names) != len(set(gene_adata_combined.var_names)):\n",
    "        logger.warning(\"Duplicate gene names found. Making gene names unique.\")\n",
    "        gene_adata_combined.var_names_make_unique()\n",
    "    \n",
    "    # Make protein names unique if needed\n",
    "    if len(protein_adata_combined.var_names) != len(set(protein_adata_combined.var_names)):\n",
    "        logger.warning(\"Duplicate protein names found. Making protein names unique.\")\n",
    "        protein_adata_combined.var_names_make_unique()\n",
    "    \n",
    "    # Filter for cells in both (paired data)\n",
    "    gene_barcodes = set(gene_adata_combined.obs_names)\n",
    "    protein_barcodes = set(protein_adata_combined.obs_names)\n",
    "    common_barcodes = gene_barcodes.intersection(protein_barcodes)\n",
    "    \n",
    "    logger.info(f\"Found {len(common_barcodes)} cells with both gene and protein data\")\n",
    "    \n",
    "    gene_adata_paired = gene_adata_combined[list(common_barcodes)]\n",
    "    protein_adata_paired = protein_adata_combined[list(common_barcodes)]\n",
    "    \n",
    "    # Save\n",
    "    gene_output_path = os.path.join(output_dir, f\"{ACCESSION_NUMBER}_gene.h5ad\")\n",
    "    protein_output_path = os.path.join(output_dir, f\"{ACCESSION_NUMBER}_protein.h5ad\")\n",
    "    gene_paired_output_path = os.path.join(output_dir, f\"{ACCESSION_NUMBER}_gene_paired.h5ad\")\n",
    "    protein_paired_output_path = os.path.join(output_dir, f\"{ACCESSION_NUMBER}_protein_paired.h5ad\")\n",
    "    \n",
    "    logger.info(f\"Saving gene expression data to {gene_output_path}\")\n",
    "    gene_adata_combined.write(gene_output_path)\n",
    "    \n",
    "    logger.info(f\"Saving protein expression data to {protein_output_path}\")\n",
    "    protein_adata_combined.write(protein_output_path)\n",
    "    \n",
    "    logger.info(f\"Saving paired gene expression data to {gene_paired_output_path}\")\n",
    "    gene_adata_paired.write(gene_paired_output_path)\n",
    "    \n",
    "    logger.info(f\"Saving paired protein expression data to {protein_paired_output_path}\")\n",
    "    protein_adata_paired.write(protein_paired_output_path)\n",
    "    \n",
    "    logger.info(\"Processing complete\")\n",
    "    \n",
    "    return gene_adata_paired, protein_adata_paired\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 5. Jupyter-friendly functions for running the pipeline\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "def run_download_only(data_dir='GSE288020_data'):\n",
    "    \"\"\"\n",
    "    Run only the download portion of the pipeline\n",
    "    Args:\n",
    "        data_dir (str): Directory to save downloaded files\n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    # Try GEOquery first\n",
    "    r_success = attempt_r_geoquery_download(ACCESSION_NUMBER, data_dir)\n",
    "    \n",
    "    if not r_success:\n",
    "        # Try manual downloads\n",
    "        downloaded_files = download_geo_supplementary_files(ACCESSION_NUMBER, data_dir)\n",
    "        \n",
    "        if not downloaded_files:\n",
    "            logger.warning(\"All download methods failed\")\n",
    "            return False\n",
    "    \n",
    "    logger.info(\"Download completed\")\n",
    "    return True\n",
    "\n",
    "def run_processing(data_dir='GSE288020_data', output_dir='GSE288020_processed', use_dummy=False):\n",
    "    \"\"\"\n",
    "    Run the full data processing pipeline\n",
    "    Args:\n",
    "        data_dir (str): Directory containing or to download data files\n",
    "        output_dir (str): Directory to save processed files\n",
    "        use_dummy (bool): Whether to create dummy data if real data is unavailable\n",
    "    Returns:\n",
    "        tuple: (gene_adata, protein_adata) - AnnData objects for gene and protein expression\n",
    "    \"\"\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    logger.info(\"Starting data processing\")\n",
    "    gene_adata, protein_adata = process_dataset(data_dir, output_dir, use_dummy=use_dummy)\n",
    "    \n",
    "    if gene_adata is None or protein_adata is None:\n",
    "        logger.error(\"Processing failed\")\n",
    "        return None, None\n",
    "    \n",
    "    # Print summaries\n",
    "    print(\"\\nGene Expression Data Summary:\")\n",
    "    print(f\"Number of cells: {gene_adata.n_obs}\")\n",
    "    print(f\"Number of genes: {gene_adata.n_vars}\")\n",
    "    print(\"Metadata fields:\", list(gene_adata.obs.columns))\n",
    "    \n",
    "    print(\"\\nProtein Expression Data Summary:\")\n",
    "    print(f\"Number of cells: {protein_adata.n_obs}\")\n",
    "    print(f\"Number of proteins: {protein_adata.n_vars}\")\n",
    "    print(\"Metadata fields:\", list(protein_adata.obs.columns))\n",
    "    \n",
    "    return gene_adata, protein_adata\n",
    "\n",
    "# For simple demonstration in a notebook\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"This script is meant to be imported and used in a Jupyter notebook.\")\n",
    "    print(\"Example usage:\")\n",
    "    print(\"  from geo_data_download import run_processing\")\n",
    "    print(\"  gene_adata, protein_adata = run_processing(use_dummy=True)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a838de-845c-4b24-ad68-772ecf4637f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
