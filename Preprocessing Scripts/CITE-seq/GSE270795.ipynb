{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b61713b-4c06-4222-8853-da54ecde594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import gzip\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "\n",
    "# Constants\n",
    "ACCESSION = \"GSE270795\"\n",
    "BASE_URL = f\"https://ftp.ncbi.nlm.nih.gov/geo/series/GSE270nnn/{ACCESSION}/suppl/\"\n",
    "FILE_PATTERNS = [\n",
    "    f\"{ACCESSION}_BC*_Tet*_sample_feature_bc_matrix.h5\",\n",
    "    f\"{ACCESSION}_BC*_Tet*_filtered_contig_annotations.csv.gz\",\n",
    "    f\"{ACCESSION}_TS_TotalSeq_01_features.csv.gz\"\n",
    "]\n",
    "\n",
    "# Metadata mapping\n",
    "METADATA_MAPPING = {\n",
    "    \"organism\": \"Homo sapiens\",  # Based on the dataset description\n",
    "    \"cell_type\": \"MAIT cells\",   # Based on the dataset description\n",
    "    \"crispr_type\": \"None\",       # Not applicable for this dataset\n",
    "    \"cancer_type\": \"Non-Cancer\", # Based on the dataset description (healthy donors)\n",
    "    \"condition\": {\n",
    "        \"Tet\": \"Control\",\n",
    "        \"Tet_IL12\": \"IL-12 stimulated\",\n",
    "        \"Tet_IL23\": \"IL-23 stimulated\"\n",
    "    },\n",
    "    \"perturbation_name\": \"None\"  # No CRISPR perturbations in this dataset\n",
    "}\n",
    "\n",
    "def clean_protein_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean up the protein name by removing extra prefixes and trailing parts.\n",
    "    \"\"\"\n",
    "    # If it's an isotype control (contains 'Ctrl'), only keep the part before the underscore.\n",
    "    if \"Ctrl\" in name:\n",
    "        name = name.split(\"_\")[0]\n",
    "    \n",
    "    # Remove specific prefixes in order of priority\n",
    "    for prefix in [\"anti-mouse-\", \"antihuman\", \"anti-\"]:\n",
    "        if name.startswith(prefix):\n",
    "            name = name[len(prefix):]\n",
    "            break\n",
    "\n",
    "    # Remove trailing \"isotype\" (case-insensitive) if present\n",
    "    if name.lower().endswith(\"isotype\"):\n",
    "        name = name[:-len(\"isotype\")]\n",
    "    \n",
    "    return name.strip()\n",
    "\n",
    "def download_files(data_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Download dataset files if they don't exist.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory to save the downloaded files\n",
    "    \"\"\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    # Get list of files to download\n",
    "    file_list = []\n",
    "    for pattern in FILE_PATTERNS:\n",
    "        file_list.extend(glob.glob(os.path.join(data_dir, pattern)))\n",
    "    \n",
    "    # If no files found, download them\n",
    "    if not file_list:\n",
    "        print(f\"No files found in {data_dir}. Downloading...\")\n",
    "        \n",
    "        # Install required packages if needed\n",
    "        try:\n",
    "            import requests\n",
    "            from bs4 import BeautifulSoup\n",
    "        except ImportError:\n",
    "            print(\"Installing required packages...\")\n",
    "            os.system(\"pip install requests beautifulsoup4\")\n",
    "            import requests\n",
    "            from bs4 import BeautifulSoup\n",
    "        \n",
    "        # Define the files we need to download\n",
    "        files_to_download = [\n",
    "            f\"{ACCESSION}_BC2_Tet_IL12_filtered_contig_annotations.csv.gz\",\n",
    "            f\"{ACCESSION}_BC2_Tet_IL12_sample_feature_bc_matrix.h5\",\n",
    "            f\"{ACCESSION}_BC2_Tet_IL23_filtered_contig_annotations.csv.gz\",\n",
    "            f\"{ACCESSION}_BC2_Tet_IL23_sample_feature_bc_matrix.h5\",\n",
    "            f\"{ACCESSION}_BC2_Tet_filtered_contig_annotations.csv.gz\",\n",
    "            f\"{ACCESSION}_BC2_Tet_sample_feature_bc_matrix.h5\",\n",
    "            f\"{ACCESSION}_BC3_Tet_IL12_filtered_contig_annotations.csv.gz\",\n",
    "            f\"{ACCESSION}_BC3_Tet_IL12_sample_feature_bc_matrix.h5\",\n",
    "            f\"{ACCESSION}_BC3_Tet_IL23_filtered_contig_annotations.csv.gz\",\n",
    "            f\"{ACCESSION}_BC3_Tet_IL23_sample_feature_bc_matrix.h5\",\n",
    "            f\"{ACCESSION}_BC3_Tet_filtered_contig_annotations.csv.gz\",\n",
    "            f\"{ACCESSION}_BC3_Tet_sample_feature_bc_matrix.h5\",\n",
    "            f\"{ACCESSION}_BC4_Tet_IL12_filtered_contig_annotations.csv.gz\",\n",
    "            f\"{ACCESSION}_BC4_Tet_IL12_sample_feature_bc_matrix.h5\",\n",
    "            f\"{ACCESSION}_BC4_Tet_IL23_filtered_contig_annotations.csv.gz\",\n",
    "            f\"{ACCESSION}_BC4_Tet_IL23_sample_feature_bc_matrix.h5\",\n",
    "            f\"{ACCESSION}_BC4_Tet_filtered_contig_annotations.csv.gz\",\n",
    "            f\"{ACCESSION}_BC4_Tet_sample_feature_bc_matrix.h5\",\n",
    "            f\"{ACCESSION}_TS_TotalSeq_01_features.csv.gz\"\n",
    "        ]\n",
    "        \n",
    "        # Download each file directly from the FTP server\n",
    "        for file in files_to_download:\n",
    "            file_url = f\"{BASE_URL}{file}\"\n",
    "            file_path = os.path.join(data_dir, file)\n",
    "            \n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"Downloading {file}...\")\n",
    "                try:\n",
    "                    # Try with requests first\n",
    "                    response = requests.get(file_url, stream=True)\n",
    "                    if response.status_code == 200:\n",
    "                        with open(file_path, 'wb') as f:\n",
    "                            for chunk in response.iter_content(chunk_size=8192):\n",
    "                                f.write(chunk)\n",
    "                        print(f\"Downloaded {file}\")\n",
    "                    else:\n",
    "                        # Fall back to urllib if requests fails\n",
    "                        try:\n",
    "                            urllib.request.urlretrieve(file_url, file_path)\n",
    "                            print(f\"Downloaded {file} using urllib\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Failed to download {file}: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error downloading {file}: {e}\")\n",
    "                    # Try with wget as a last resort\n",
    "                    try:\n",
    "                        os.system(f\"wget -O {file_path} {file_url}\")\n",
    "                        if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
    "                            print(f\"Downloaded {file} using wget\")\n",
    "                        else:\n",
    "                            print(f\"Failed to download {file} using wget\")\n",
    "                    except Exception as e2:\n",
    "                        print(f\"Error using wget: {e2}\")\n",
    "    else:\n",
    "        print(f\"Found {len(file_list)} files in {data_dir}\")\n",
    "\n",
    "def load_h5_data(file_path: str) -> Tuple[ad.AnnData, ad.AnnData]:\n",
    "    \"\"\"\n",
    "    Load data from a 10x h5 file and split into gene expression and protein data.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the h5 file\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (gene_expression_adata, protein_adata)\n",
    "    \"\"\"\n",
    "    print(f\"Processing {os.path.basename(file_path)}...\")\n",
    "    \n",
    "    # Extract sample info from filename\n",
    "    filename = os.path.basename(file_path)\n",
    "    parts = filename.split('_')\n",
    "    donor = parts[1]  # BC2, BC3, BC4\n",
    "    \n",
    "    # Handle different condition formats\n",
    "    if len(parts) > 3 and parts[3] in [\"IL12\", \"IL23\"]:\n",
    "        condition = f\"{parts[2]}_{parts[3]}\"  # Tet_IL12, Tet_IL23\n",
    "    else:\n",
    "        condition = parts[2]  # Tet\n",
    "    \n",
    "    # Load the h5 file manually to ensure correct feature handling\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        # Get matrix dimensions\n",
    "        shape = f['matrix']['shape'][:]\n",
    "        n_features, n_barcodes = shape\n",
    "        \n",
    "        # Get feature metadata\n",
    "        feature_ids = [x.decode('utf-8') for x in f['matrix']['features']['id'][:]]\n",
    "        feature_names = [x.decode('utf-8') for x in f['matrix']['features']['name'][:]]\n",
    "        feature_types = [x.decode('utf-8') for x in f['matrix']['features']['feature_type'][:]]\n",
    "        \n",
    "        # Get cell barcodes\n",
    "        barcodes = [x.decode('utf-8') for x in f['matrix']['barcodes'][:]]\n",
    "        \n",
    "        # Get sparse matrix data\n",
    "        data = f['matrix']['data'][:]\n",
    "        indices = f['matrix']['indices'][:]\n",
    "        indptr = f['matrix']['indptr'][:]\n",
    "        \n",
    "        # Create feature metadata DataFrame\n",
    "        feature_metadata = pd.DataFrame({\n",
    "            'id': feature_ids,\n",
    "            'name': feature_names,\n",
    "            'feature_type': feature_types\n",
    "        })\n",
    "        \n",
    "        # Split features by type\n",
    "        gene_metadata = feature_metadata[feature_metadata['feature_type'] == 'Gene Expression']\n",
    "        protein_metadata = feature_metadata[feature_metadata['feature_type'] == 'Antibody Capture']\n",
    "        \n",
    "        # Create cell metadata\n",
    "        cell_metadata = pd.DataFrame(index=barcodes)\n",
    "        cell_metadata['donor'] = donor\n",
    "        cell_metadata['condition_raw'] = condition\n",
    "        cell_metadata['sample'] = f\"{donor}_{condition}\"\n",
    "        \n",
    "        # Add harmonized metadata\n",
    "        cell_metadata['organism'] = METADATA_MAPPING['organism']\n",
    "        cell_metadata['cell_type'] = METADATA_MAPPING['cell_type']\n",
    "        cell_metadata['crispr_type'] = METADATA_MAPPING['crispr_type']\n",
    "        cell_metadata['cancer_type'] = METADATA_MAPPING['cancer_type']\n",
    "        cell_metadata['condition'] = cell_metadata['condition_raw'].map(METADATA_MAPPING['condition'])\n",
    "        cell_metadata['perturbation_name'] = METADATA_MAPPING['perturbation_name']\n",
    "        \n",
    "        # Create gene expression AnnData\n",
    "        if not gene_metadata.empty:\n",
    "            from scipy import sparse\n",
    "            \n",
    "            # Get indices of gene expression features\n",
    "            gene_indices = gene_metadata.index.tolist()\n",
    "            \n",
    "            # Create a mapping from original indices to new positions\n",
    "            gene_idx_map = {idx: i for i, idx in enumerate(gene_indices)}\n",
    "            \n",
    "            # Filter the sparse matrix for gene expression\n",
    "            gene_data = []\n",
    "            gene_indices_new = []\n",
    "            gene_indptr = [0]\n",
    "            \n",
    "            for i in range(len(indptr) - 1):\n",
    "                start, end = indptr[i], indptr[i+1]\n",
    "                count = 0\n",
    "                for j in range(start, end):\n",
    "                    if indices[j] in gene_idx_map:\n",
    "                        gene_data.append(data[j])\n",
    "                        gene_indices_new.append(gene_idx_map[indices[j]])\n",
    "                        count += 1\n",
    "                gene_indptr.append(gene_indptr[-1] + count)\n",
    "            \n",
    "            # Create sparse matrix\n",
    "            gene_matrix = sparse.csr_matrix(\n",
    "                (gene_data, gene_indices_new, gene_indptr),\n",
    "                shape=(len(barcodes), len(gene_indices))\n",
    "            )\n",
    "            \n",
    "            # Create AnnData object\n",
    "            gene_adata = ad.AnnData(\n",
    "                X=gene_matrix,\n",
    "                obs=cell_metadata,\n",
    "                var=gene_metadata.reset_index(drop=True)\n",
    "            )\n",
    "            \n",
    "            # Set var_names to gene symbols and ensure uniqueness\n",
    "            gene_adata.var_names = gene_adata.var['name'].values\n",
    "            gene_adata.var.index.name = None\n",
    "            if gene_adata.var_names.duplicated().any():\n",
    "                gene_adata.var_names_make_unique()\n",
    "                \n",
    "            print(f\"Created gene expression AnnData: {gene_adata.shape[0]} cells, {gene_adata.shape[1]} genes\")\n",
    "        else:\n",
    "            gene_adata = None\n",
    "            print(\"No gene expression features found\")\n",
    "        \n",
    "        # Create protein AnnData\n",
    "        if not protein_metadata.empty:\n",
    "            from scipy import sparse\n",
    "            \n",
    "            # Get indices of protein features\n",
    "            protein_indices = protein_metadata.index.tolist()\n",
    "            \n",
    "            # Create a mapping from original indices to new positions\n",
    "            protein_idx_map = {idx: i for i, idx in enumerate(protein_indices)}\n",
    "            \n",
    "            # Filter the sparse matrix for protein expression\n",
    "            protein_data = []\n",
    "            protein_indices_new = []\n",
    "            protein_indptr = [0]\n",
    "            \n",
    "            for i in range(len(indptr) - 1):\n",
    "                start, end = indptr[i], indptr[i+1]\n",
    "                count = 0\n",
    "                for j in range(start, end):\n",
    "                    if indices[j] in protein_idx_map:\n",
    "                        protein_data.append(data[j])\n",
    "                        protein_indices_new.append(protein_idx_map[indices[j]])\n",
    "                        count += 1\n",
    "                protein_indptr.append(protein_indptr[-1] + count)\n",
    "            \n",
    "            # Create sparse matrix\n",
    "            protein_matrix = sparse.csr_matrix(\n",
    "                (protein_data, protein_indices_new, protein_indptr),\n",
    "                shape=(len(barcodes), len(protein_indices))\n",
    "            )\n",
    "            \n",
    "            # Create AnnData object\n",
    "            protein_adata = ad.AnnData(\n",
    "                X=protein_matrix,\n",
    "                obs=cell_metadata,\n",
    "                var=protein_metadata.reset_index(drop=True)\n",
    "            )\n",
    "            \n",
    "            # Set var_names to protein names\n",
    "            protein_adata.var_names = protein_adata.var['name'].values\n",
    "            protein_adata.var.index.name = None\n",
    "            \n",
    "            # Clean the protein names using the helper function\n",
    "            protein_adata.var_names = [clean_protein_name(name) for name in protein_adata.var_names]\n",
    "            \n",
    "            # Ensure protein var_names are unique\n",
    "            if protein_adata.var_names.duplicated().any():\n",
    "                protein_adata.var_names_make_unique()\n",
    "                \n",
    "            print(f\"Created protein expression AnnData: {protein_adata.shape[0]} cells, {protein_adata.shape[1]} proteins\")\n",
    "        else:\n",
    "            protein_adata = None\n",
    "            print(\"No protein expression features found\")\n",
    "    \n",
    "    return gene_adata, protein_adata\n",
    "\n",
    "def load_vdj_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load VDJ data from a filtered contig annotations file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the filtered contig annotations file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with VDJ data\n",
    "    \"\"\"\n",
    "    # Load the VDJ data\n",
    "    vdj_data = pd.read_csv(file_path, compression='gzip')\n",
    "    \n",
    "    # Extract sample info from filename\n",
    "    filename = os.path.basename(file_path)\n",
    "    parts = filename.split('_')\n",
    "    donor = parts[1]  # BC2, BC3, BC4\n",
    "    \n",
    "    # Handle different condition formats\n",
    "    if len(parts) > 3 and parts[3] in [\"IL12\", \"IL23\"]:\n",
    "        condition = f\"{parts[2]}_{parts[3]}\"  # Tet_IL12, Tet_IL23\n",
    "    else:\n",
    "        condition = parts[2]  # Tet\n",
    "    \n",
    "    # Add sample information\n",
    "    vdj_data['donor'] = donor\n",
    "    vdj_data['condition'] = condition\n",
    "    vdj_data['sample'] = f\"{donor}_{condition}\"\n",
    "    \n",
    "    return vdj_data\n",
    "\n",
    "def process_dataset(data_dir: str) -> Tuple[ad.AnnData, ad.AnnData]:\n",
    "    \"\"\"\n",
    "    Process the entire dataset and create harmonized AnnData objects.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing the dataset files\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (gene_expression_adata, protein_adata)\n",
    "    \"\"\"\n",
    "    # Find all h5 files\n",
    "    h5_files = glob.glob(os.path.join(data_dir, f\"{ACCESSION}_BC*_Tet*_sample_feature_bc_matrix.h5\"))\n",
    "    \n",
    "    # Find all VDJ files\n",
    "    vdj_files = glob.glob(os.path.join(data_dir, f\"{ACCESSION}_BC*_Tet*_filtered_contig_annotations.csv.gz\"))\n",
    "    \n",
    "    # Load protein features\n",
    "    protein_features_file = os.path.join(data_dir, f\"{ACCESSION}_TS_TotalSeq_01_features.csv.gz\")\n",
    "    if os.path.exists(protein_features_file):\n",
    "        protein_features = pd.read_csv(protein_features_file, compression='gzip')\n",
    "        print(f\"Loaded {len(protein_features)} protein features\")\n",
    "    else:\n",
    "        protein_features = None\n",
    "        print(\"Protein features file not found\")\n",
    "    \n",
    "    # Process each h5 file\n",
    "    gene_adatas = []\n",
    "    protein_adatas = []\n",
    "    \n",
    "    for h5_file in h5_files:\n",
    "        gene_adata, protein_adata = load_h5_data(h5_file)\n",
    "        \n",
    "        if gene_adata is not None:\n",
    "            gene_adatas.append(gene_adata)\n",
    "        \n",
    "        if protein_adata is not None:\n",
    "            protein_adatas.append(protein_adata)\n",
    "    \n",
    "    # Process VDJ data\n",
    "    vdj_data_list = []\n",
    "    for vdj_file in vdj_files:\n",
    "        vdj_data = load_vdj_data(vdj_file)\n",
    "        vdj_data_list.append(vdj_data)\n",
    "    \n",
    "    if vdj_data_list:\n",
    "        vdj_data_combined = pd.concat(vdj_data_list, ignore_index=True)\n",
    "        print(f\"Loaded VDJ data for {len(vdj_data_combined['barcode'].unique())} cells\")\n",
    "    else:\n",
    "        vdj_data_combined = None\n",
    "    \n",
    "    # Combine gene expression data\n",
    "    if gene_adatas:\n",
    "        # Make observation names unique before concatenation\n",
    "        for i, adata in enumerate(gene_adatas):\n",
    "            adata.obs_names = [f\"{obs_name}_{i}\" for obs_name in adata.obs_names]\n",
    "        \n",
    "        combined_gene_adata = ad.concat(gene_adatas, join='outer', merge='same')\n",
    "        print(f\"Combined gene expression data: {combined_gene_adata.shape[0]} cells, {combined_gene_adata.shape[1]} genes\")\n",
    "        \n",
    "        if combined_gene_adata.var_names.duplicated().any():\n",
    "            print(f\"Found {combined_gene_adata.var_names.duplicated().sum()} duplicate gene names\")\n",
    "            combined_gene_adata.var_names = combined_gene_adata.var_names + '_' + combined_gene_adata.var['id']\n",
    "            if combined_gene_adata.var_names.duplicated().any():\n",
    "                combined_gene_adata.var_names_make_unique()\n",
    "    else:\n",
    "        combined_gene_adata = None\n",
    "    \n",
    "    # Combine protein data\n",
    "    if protein_adatas:\n",
    "        for i, adata in enumerate(protein_adatas):\n",
    "            adata.obs_names = [f\"{obs_name}_{i}\" for obs_name in adata.obs_names]\n",
    "        \n",
    "        combined_protein_adata = ad.concat(protein_adatas, join='outer', merge='same')\n",
    "        print(f\"Combined protein data: {combined_protein_adata.shape[0]} cells, {combined_protein_adata.shape[1]} proteins\")\n",
    "        \n",
    "        if combined_protein_adata.var_names.duplicated().any():\n",
    "            print(f\"Found {combined_protein_adata.var_names.duplicated().sum()} duplicate protein names\")\n",
    "            combined_protein_adata.var_names_make_unique()\n",
    "    else:\n",
    "        combined_protein_adata = None\n",
    "    \n",
    "    # Add VDJ data to gene expression and protein data\n",
    "    if vdj_data_combined is not None:\n",
    "        for col in ['chain', 'v_gene', 'j_gene', 'c_gene', 'cdr3', 'raw_clonotype_id']:\n",
    "            vdj_data_combined[col] = vdj_data_combined[col].fillna('').astype(str)\n",
    "        \n",
    "        vdj_summary = vdj_data_combined.groupby('barcode').agg({\n",
    "            'chain': lambda x: ','.join(sorted(set(x))) if all(x != '') else np.nan,\n",
    "            'v_gene': lambda x: ','.join(sorted(set(x))) if all(x != '') else np.nan,\n",
    "            'j_gene': lambda x: ','.join(sorted(set(x))) if all(x != '') else np.nan,\n",
    "            'c_gene': lambda x: ','.join(sorted(set(x))) if all(x != '') else np.nan,\n",
    "            'cdr3': lambda x: ','.join(sorted(set(x))) if all(x != '') else np.nan,\n",
    "            'productive': lambda x: all(x) if not pd.isna(x).any() else np.nan,\n",
    "            'full_length': lambda x: all(x) if not pd.isna(x).any() else np.nan,\n",
    "            'raw_clonotype_id': lambda x: ','.join(sorted(set(x))) if all(x != '') else np.nan\n",
    "        }).reset_index()\n",
    "        \n",
    "        vdj_dict = vdj_summary.set_index('barcode').to_dict(orient='index')\n",
    "        \n",
    "        if combined_gene_adata is not None:\n",
    "            vdj_cols = ['chain', 'v_gene', 'j_gene', 'c_gene', 'cdr3', 'productive', 'full_length', 'raw_clonotype_id']\n",
    "            for col in vdj_cols:\n",
    "                combined_gene_adata.obs[f'vdj_{col}'] = [vdj_dict.get(bc, {}).get(col, np.nan) for bc in combined_gene_adata.obs.index]\n",
    "        \n",
    "        if combined_protein_adata is not None:\n",
    "            vdj_cols = ['chain', 'v_gene', 'j_gene', 'c_gene', 'cdr3', 'productive', 'full_length', 'raw_clonotype_id']\n",
    "            for col in vdj_cols:\n",
    "                combined_protein_adata.obs[f'vdj_{col}'] = [vdj_dict.get(bc, {}).get(col, np.nan) for bc in combined_protein_adata.obs.index]\n",
    "    \n",
    "    # Filter to keep only paired data (cells present in both gene and protein data)\n",
    "    if combined_gene_adata is not None and combined_protein_adata is not None:\n",
    "        gene_barcodes = combined_gene_adata.obs['sample'].astype(str) + '_' + combined_gene_adata.obs.index.str.split('_').str[0]\n",
    "        protein_barcodes = combined_protein_adata.obs['sample'].astype(str) + '_' + combined_protein_adata.obs.index.str.split('_').str[0]\n",
    "        \n",
    "        gene_barcode_map = {bc: idx for idx, bc in zip(combined_gene_adata.obs.index, gene_barcodes)}\n",
    "        protein_barcode_map = {bc: idx for idx, bc in zip(combined_protein_adata.obs.index, protein_barcodes)}\n",
    "        \n",
    "        common_original_barcodes = set(gene_barcode_map.keys()) & set(protein_barcode_map.keys())\n",
    "        print(f\"Found {len(common_original_barcodes)} cells with both gene expression and protein data\")\n",
    "        \n",
    "        if common_original_barcodes:\n",
    "            gene_common_barcodes = [gene_barcode_map[bc] for bc in common_original_barcodes]\n",
    "            protein_common_barcodes = [protein_barcode_map[bc] for bc in common_original_barcodes]\n",
    "            \n",
    "            combined_gene_adata = combined_gene_adata[gene_common_barcodes].copy()\n",
    "            combined_protein_adata = combined_protein_adata[protein_common_barcodes].copy()\n",
    "            \n",
    "            combined_gene_adata.obs['original_barcode'] = [bc.split('_')[0] for bc in combined_gene_adata.obs.index]\n",
    "            combined_protein_adata.obs['original_barcode'] = [bc.split('_')[0] for bc in combined_protein_adata.obs.index]\n",
    "    \n",
    "    return combined_gene_adata, combined_protein_adata\n",
    "\n",
    "def main(data_dir: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Main function to download, process, and save the dataset.\n",
    "    \"\"\"\n",
    "    if data_dir is None:\n",
    "        data_dir = os.getcwd()\n",
    "    \n",
    "    # Download files if needed\n",
    "    download_files(data_dir)\n",
    "    \n",
    "    # Process the dataset\n",
    "    gene_adata, protein_adata = process_dataset(data_dir)\n",
    "    \n",
    "    # Save the processed data\n",
    "    output_dir = os.path.join(data_dir, \"processed\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if gene_adata is not None:\n",
    "        gene_output_file = os.path.join(output_dir, f\"{ACCESSION}_gene_expression_harmonized.h5ad\")\n",
    "        gene_adata.write_h5ad(gene_output_file, compression=\"gzip\")\n",
    "        print(f\"Saved gene expression data to {gene_output_file}\")\n",
    "    \n",
    "    if protein_adata is not None:\n",
    "        protein_output_file = os.path.join(output_dir, f\"{ACCESSION}_protein_expression_harmonized.h5ad\")\n",
    "        protein_adata.write_h5ad(protein_output_file, compression=\"gzip\")\n",
    "        print(f\"Saved protein expression data to {protein_output_file}\")\n",
    "\n",
    "# Run the main function directly (for use in a Jupyter Notebook cell)\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a838de-845c-4b24-ad68-772ecf4637f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
