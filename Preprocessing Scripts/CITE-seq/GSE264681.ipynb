{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b61713b-4c06-4222-8853-da54ecde594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import gzip\n",
    "import shutil\n",
    "import urllib.request\n",
    "import tempfile\n",
    "import subprocess\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "from scipy import sparse\n",
    "from scipy.io import mmread\n",
    "\n",
    "\n",
    "\n",
    "# List of files to download\n",
    "FILES = [\n",
    "    \"GSE264681_Arrayed_NALM6_D1_filtered_feature_bc_matrix.tar.gz\",\n",
    "    \"GSE264681_Arrayed_NALM6_D1_feature_reference_CARBC.csv.gz\",\n",
    "    \"GSE264681_Arrayed_NALM6_D2_filtered_feature_bc_matrix.tar.gz\",\n",
    "    \"GSE264681_Arrayed_NALM6_D2_feature_reference_CARBC.csv.gz\",\n",
    "    \"GSE264681_CD4_Spike-In_filtered_feature_bc_matrix.tar.gz\",\n",
    "    \"GSE264681_CD4_Spike-In_feature_reference_CARBC.csv.gz\",\n",
    "    \"GSE264681_In_Vivo_A375_filtered_feature_bc_matrix.tar.gz\",\n",
    "    \"GSE264681_In_Vivo_A375_feature_reference_CITEseq_CARBC.csv.gz\",\n",
    "    \"GSE264681_Pooled_A375_filtered_feature_bc_matrix.tar.gz\",\n",
    "    \"GSE264681_Pooled_A375_feature_reference_CARBC.csv.gz\",\n",
    "    \"GSE264681_Pooled_NALM6_filtered_feature_bc_matrix.tar.gz\",\n",
    "    \"GSE264681_Pooled_NALM6_feature_reference_CARBC.csv.gz\",\n",
    "    \"GSE264681_Resting_filtered_feature_bc_matrix.tar.gz\",\n",
    "    \"GSE264681_Resting_feature_reference_CARBC.csv.gz\"\n",
    "]\n",
    "\n",
    "# Sample information (not strictly necessary for the script, but here for reference)\n",
    "SAMPLES = [\n",
    "    \"Arrayed_NALM6_D1\",\n",
    "    \"Arrayed_NALM6_D2\",\n",
    "    \"CD4_Spike-In\",\n",
    "    \"In_Vivo_A375\",\n",
    "    \"Pooled_A375\",\n",
    "    \"Pooled_NALM6\",\n",
    "    \"Resting\"\n",
    "]\n",
    "\n",
    "# CAR architecture dictionary\n",
    "CAR_ARCHITECTURES = {\n",
    "    'CAR1': 'CD28-CD3z',\n",
    "    'CAR2': 'CD28-CD28-CD3z',\n",
    "    'CAR3': '41BB-CD3z',\n",
    "    'CAR4': '41BB-41BB-CD3z',\n",
    "    'CAR5': 'CD28-41BB-CD3z',\n",
    "    'CAR6': '41BB-CD28-CD3z',\n",
    "    'CAR7': 'ICOS-CD3z',\n",
    "    'CAR8': 'ICOS-ICOS-CD3z',\n",
    "    'CAR9': 'CD28-ICOS-CD3z',\n",
    "    'CAR10': 'ICOS-CD28-CD3z',\n",
    "    'CAR11': '41BB-ICOS-CD3z',\n",
    "    'CAR12': 'ICOS-41BB-CD3z',\n",
    "    'CAR13': 'OX40-CD3z',\n",
    "    'CAR14': 'OX40-OX40-CD3z',\n",
    "    'CAR15': 'CD28-OX40-CD3z',\n",
    "    'CAR16': 'OX40-CD28-CD3z',\n",
    "    'CAR17': '41BB-OX40-CD3z',\n",
    "    'CAR18': 'OX40-41BB-CD3z',\n",
    "    'CAR19': 'ICOS-OX40-CD3z',\n",
    "    'CAR20': 'OX40-ICOS-CD3z',\n",
    "    'CAR21': 'CD27-CD3z',\n",
    "    'CAR22': 'CD27-CD27-CD3z',\n",
    "    'CAR23': 'CD28-CD27-CD3z',\n",
    "    'CAR24': 'CD27-CD28-CD3z',\n",
    "    'CAR25': '41BB-CD27-CD3z',\n",
    "    'CAR26': 'CD27-41BB-CD3z',\n",
    "    'CAR27': 'ICOS-CD27-CD3z',\n",
    "    'CAR28': 'CD27-ICOS-CD3z',\n",
    "    'CAR29': 'OX40-CD27-CD3z',\n",
    "    'CAR30': 'CD27-OX40-CD3z',\n",
    "    'CAR31': 'CD28-41BB-ICOS-OX40-CD27-CD3z',\n",
    "    'CAR32': 'CD28-41BB-ICOS-OX40-CD27-CD3z-CD3z',\n",
    "    'CAR33': 'CD28-41BB-ICOS-OX40-CD27-CD3z-CD3z-CD3z',\n",
    "    'CAR34': 'CD28-ICOS-4-1BB-OX40-CD27-CD3z-CD3z',\n",
    "    'CAR35': 'CD3z',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def download_files(data_dir):\n",
    "    \"\"\"Download dataset files if they don't exist.\"\"\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    for file in FILES:\n",
    "        file_path = os.path.join(data_dir, file)\n",
    "        if not os.path.exists(file_path):\n",
    "            # Use the GEO download URL directly - this is more reliable than FTP\n",
    "            url = f\"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE264681&format=file&file={file}\"\n",
    "            print(f\"Downloading {file}...\")\n",
    "            \n",
    "            # Use curl with --location to follow redirects\n",
    "            cmd = f\"curl -L -s -o {file_path} '{url}'\"\n",
    "            try:\n",
    "                result = os.system(cmd)\n",
    "                if result == 0 and os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
    "                    print(f\"Downloaded {file}\")\n",
    "                else:\n",
    "                    # If curl fails or file is empty, try wget\n",
    "                    print(f\"Curl failed, trying wget for {file}...\")\n",
    "                    cmd = f\"wget -q -O {file_path} '{url}'\"\n",
    "                    result = os.system(cmd)\n",
    "                    if result == 0 and os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
    "                        print(f\"Downloaded {file} using wget\")\n",
    "                    else:\n",
    "                        # If wget also fails, try Python's urllib as last resort\n",
    "                        print(f\"Wget failed, trying urllib for {file}...\")\n",
    "                        try:\n",
    "                            # Add headers to mimic a browser request\n",
    "                            headers = {\n",
    "                                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "                                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "                                'Accept-Language': 'en-US,en;q=0.5',\n",
    "                                'Connection': 'keep-alive',\n",
    "                                'Upgrade-Insecure-Requests': '1',\n",
    "                            }\n",
    "                            req = urllib.request.Request(url, headers=headers)\n",
    "                            with urllib.request.urlopen(req, timeout=60) as response:\n",
    "                                with open(file_path, 'wb') as out_file:\n",
    "                                    shutil.copyfileobj(response, out_file)\n",
    "                            if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
    "                                print(f\"Downloaded {file} using urllib\")\n",
    "                            else:\n",
    "                                print(f\"Downloaded file {file} is empty\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"All download methods failed for {file}: {e}\")\n",
    "                            print(f\"Please download {file} manually from GEO and place it in {data_dir}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during download of {file}: {e}\")\n",
    "                print(f\"Please download {file} manually from GEO and place it in {data_dir}\")\n",
    "        else:\n",
    "            print(f\"File {file} already exists, skipping download\")\n",
    "\n",
    "def extract_tar_gz(file_path, extract_dir):\n",
    "    \"\"\"Extract tar.gz file to specified directory.\"\"\"\n",
    "    if not os.path.exists(os.path.join(extract_dir, \"filtered_feature_bc_matrix\")):\n",
    "        os.makedirs(extract_dir, exist_ok=True)\n",
    "        print(f\"Extracting {file_path} to {extract_dir}...\")\n",
    "        \n",
    "        # Check if the file exists and has content\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File {file_path} does not exist\")\n",
    "        \n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            raise ValueError(f\"File {file_path} is empty\")\n",
    "        \n",
    "        # Try different extraction methods\n",
    "        try:\n",
    "            # First try Python's built-in extraction\n",
    "            shutil.unpack_archive(file_path, extract_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with shutil.unpack_archive: {e}\")\n",
    "            print(\"Trying tar command...\")\n",
    "            \n",
    "            # Try using tar command\n",
    "            cmd = f\"tar -xzf {file_path} -C {extract_dir}\"\n",
    "            result = os.system(cmd)\n",
    "            \n",
    "            if result != 0:\n",
    "                raise RuntimeError(f\"Failed to extract {file_path} using both Python and tar command\")\n",
    "        \n",
    "        # Verify extraction was successful\n",
    "        if not os.path.exists(os.path.join(extract_dir, \"filtered_feature_bc_matrix\")):\n",
    "            # Sometimes the directory structure might be different\n",
    "            # Look for any directory that might contain the matrix files\n",
    "            matrix_dirs = []\n",
    "            for root, dirs, files in os.walk(extract_dir):\n",
    "                if \"matrix.mtx.gz\" in files or \"matrix.mtx\" in files:\n",
    "                    matrix_dirs.append(root)\n",
    "            \n",
    "            if matrix_dirs:\n",
    "                # If we found a directory with matrix files, create a symlink\n",
    "                matrix_dir = matrix_dirs[0]\n",
    "                target_dir = os.path.join(extract_dir, \"filtered_feature_bc_matrix\")\n",
    "                os.symlink(matrix_dir, target_dir)\n",
    "                print(f\"Created symlink from {matrix_dir} to {target_dir}\")\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Could not find matrix files in extracted directory {extract_dir}\")\n",
    "        \n",
    "        print(f\"Extracted {file_path}\")\n",
    "    else:\n",
    "        print(f\"Directory {extract_dir}/filtered_feature_bc_matrix already exists, skipping extraction\")\n",
    "\n",
    "def read_10x_mtx(matrix_dir):\n",
    "    \"\"\"Read 10x matrix files and return AnnData object.\"\"\"\n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(matrix_dir):\n",
    "        raise FileNotFoundError(f\"Matrix directory {matrix_dir} does not exist\")\n",
    "    \n",
    "    # Check if required files exist\n",
    "    required_files = ['matrix.mtx.gz', 'features.tsv.gz', 'barcodes.tsv.gz']\n",
    "    missing_files = [f for f in required_files if not os.path.exists(os.path.join(matrix_dir, f))]\n",
    "    \n",
    "    # Check for uncompressed versions if compressed ones are missing\n",
    "    for missing_file in list(missing_files):\n",
    "        uncompressed = missing_file[:-3]  # Remove .gz extension\n",
    "        if os.path.exists(os.path.join(matrix_dir, uncompressed)):\n",
    "            missing_files.remove(missing_file)\n",
    "            print(f\"Found uncompressed version of {missing_file}: {uncompressed}\")\n",
    "    \n",
    "    if missing_files:\n",
    "        raise FileNotFoundError(f\"Missing required files in {matrix_dir}: {', '.join(missing_files)}\")\n",
    "    \n",
    "    try:\n",
    "        # Use scanpy's read_10x_mtx function\n",
    "        print(f\"Reading 10x matrix from {matrix_dir}...\")\n",
    "        adata = sc.read_10x_mtx(matrix_dir, var_names='gene_symbols', cache=True)\n",
    "        \n",
    "        # Read features to get feature types\n",
    "        features_file = os.path.join(matrix_dir, 'features.tsv.gz')\n",
    "        if not os.path.exists(features_file):\n",
    "            features_file = os.path.join(matrix_dir, 'features.tsv')  # Try uncompressed version\n",
    "        \n",
    "        try:\n",
    "            if features_file.endswith('.gz'):\n",
    "                with gzip.open(features_file, 'rt') as f:\n",
    "                    features = [line.strip().split('\\t') for line in f]\n",
    "            else:\n",
    "                with open(features_file, 'rt') as f:\n",
    "                    features = [line.strip().split('\\t') for line in f]\n",
    "            \n",
    "            # Create a DataFrame with feature information\n",
    "            feature_df = pd.DataFrame(features, columns=['id', 'name', 'feature_type'])\n",
    "            \n",
    "            # Add feature_type column to adata.var\n",
    "            adata.var['feature_type'] = 'Unknown'  # Default value\n",
    "            \n",
    "            # Map feature types to var_names\n",
    "            feature_type_dict = dict(zip(feature_df['name'], feature_df['feature_type']))\n",
    "            for var_name in adata.var_names:\n",
    "                if var_name in feature_type_dict:\n",
    "                    adata.var.loc[var_name, 'feature_type'] = feature_type_dict[var_name]\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error reading feature types: {e}\")\n",
    "            print(\"Setting all features to 'Gene Expression' by default\")\n",
    "            adata.var['feature_type'] = 'Gene Expression'\n",
    "        \n",
    "        return adata\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading with scanpy: {e}\")\n",
    "        \n",
    "        # Manual reading as fallback\n",
    "        try:\n",
    "            print(\"Trying manual reading...\")\n",
    "            matrix_file = os.path.join(matrix_dir, 'matrix.mtx.gz')\n",
    "            features_file = os.path.join(matrix_dir, 'features.tsv.gz')\n",
    "            barcodes_file = os.path.join(matrix_dir, 'barcodes.tsv.gz')\n",
    "            \n",
    "            # Check for uncompressed versions\n",
    "            if not os.path.exists(matrix_file):\n",
    "                matrix_file = os.path.join(matrix_dir, 'matrix.mtx')\n",
    "            if not os.path.exists(features_file):\n",
    "                features_file = os.path.join(matrix_dir, 'features.tsv')\n",
    "            if not os.path.exists(barcodes_file):\n",
    "                barcodes_file = os.path.join(matrix_dir, 'barcodes.tsv')\n",
    "            \n",
    "            # Read the matrix\n",
    "            print(f\"Reading matrix from {matrix_file}...\")\n",
    "            X = mmread(matrix_file).T.tocsr()\n",
    "            \n",
    "            # Read features (genes/proteins)\n",
    "            print(f\"Reading features from {features_file}...\")\n",
    "            if features_file.endswith('.gz'):\n",
    "                with gzip.open(features_file, 'rt') as f:\n",
    "                    features = [line.strip().split('\\t') for line in f]\n",
    "            else:\n",
    "                with open(features_file, 'rt') as f:\n",
    "                    features = [line.strip().split('\\t') for line in f]\n",
    "            \n",
    "            feature_ids = [f[0] for f in features]\n",
    "            feature_names = [f[1] for f in features]\n",
    "            \n",
    "            # Handle case where feature_types might be missing\n",
    "            if len(features[0]) >= 3:\n",
    "                feature_types = [f[2] for f in features]\n",
    "            else:\n",
    "                print(\"Warning: Feature types not found in features file, assuming all are 'Gene Expression'\")\n",
    "                feature_types = ['Gene Expression'] * len(feature_names)\n",
    "            \n",
    "            # Read cell barcodes\n",
    "            print(f\"Reading barcodes from {barcodes_file}...\")\n",
    "            if barcodes_file.endswith('.gz'):\n",
    "                with gzip.open(barcodes_file, 'rt') as f:\n",
    "                    barcodes = [line.strip() for line in f]\n",
    "            else:\n",
    "                with open(barcodes_file, 'rt') as f:\n",
    "                    barcodes = [line.strip() for line in f]\n",
    "            \n",
    "            # Create feature metadata\n",
    "            var = pd.DataFrame({\n",
    "                'feature_id': feature_ids,\n",
    "                'feature_type': feature_types\n",
    "            }, index=feature_names)\n",
    "            \n",
    "            # Create AnnData object\n",
    "            print(f\"Creating AnnData object with {X.shape[0]} cells and {X.shape[1]} features...\")\n",
    "            adata = ad.AnnData(X, obs=pd.DataFrame(index=barcodes), var=var)\n",
    "            return adata\n",
    "        except Exception as e2:\n",
    "            print(f\"Error with manual reading: {e2}\")\n",
    "            raise\n",
    "\n",
    "def read_feature_reference(file_path):\n",
    "    \"\"\"Read feature reference file and return DataFrame.\"\"\"\n",
    "    try:\n",
    "        if file_path.endswith('.gz'):\n",
    "            with gzip.open(file_path, 'rt') as f:\n",
    "                df = pd.read_csv(f)\n",
    "        else:\n",
    "            df = pd.read_csv(file_path)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading feature reference file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_sample(sample_name, data_dir, feature_ref_file):\n",
    "    \"\"\"Process a single sample and return gene and protein AnnData objects.\"\"\"\n",
    "    # Check if files exist\n",
    "    matrix_file = f\"GSE264681_{sample_name}_filtered_feature_bc_matrix.tar.gz\"\n",
    "    matrix_file_path = os.path.join(data_dir, matrix_file)\n",
    "    feature_ref_path = os.path.join(data_dir, feature_ref_file)\n",
    "    \n",
    "    if not os.path.exists(matrix_file_path):\n",
    "        print(f\"Warning: Matrix file {matrix_file_path} not found, skipping sample {sample_name}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Extract the matrix files\n",
    "    extract_dir = os.path.join(data_dir, sample_name)\n",
    "    try:\n",
    "        extract_tar_gz(matrix_file_path, extract_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting {matrix_file_path}: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Read the matrix data\n",
    "    matrix_dir = os.path.join(extract_dir, \"filtered_feature_bc_matrix\")\n",
    "    if not os.path.exists(matrix_dir):\n",
    "        print(f\"Warning: Matrix directory {matrix_dir} not found, skipping sample {sample_name}\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        adata = read_10x_mtx(matrix_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading matrix data for {sample_name}: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Read the feature reference file if it exists\n",
    "    if os.path.exists(feature_ref_path):\n",
    "        try:\n",
    "            feature_ref = read_feature_reference(feature_ref_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading feature reference file {feature_ref_path}: {e}\")\n",
    "            feature_ref = None\n",
    "    else:\n",
    "        print(f\"Warning: Feature reference file {feature_ref_path} not found\")\n",
    "        feature_ref = None\n",
    "    \n",
    "    # Add sample information to obs\n",
    "    adata.obs['sample'] = sample_name\n",
    "    \n",
    "    # Check if feature_type column exists\n",
    "    if 'feature_type' not in adata.var.columns:\n",
    "        print(f\"Warning: 'feature_type' column not found in {sample_name} data. Adding it...\")\n",
    "        adata.var['feature_type'] = 'Gene Expression'  # Default to gene expression\n",
    "        \n",
    "        # For In_Vivo_A375, we know it has CITE-seq data; attempt a naive classification\n",
    "        if sample_name == \"In_Vivo_A375\":\n",
    "            import re\n",
    "            protein_pattern = re.compile(r'^(CD|HLA|IgG|Ig[ADGME]|IL|TNF|IFN|CCR|CXCR)')\n",
    "            protein_mask = [bool(protein_pattern.match(name)) for name in adata.var_names]\n",
    "            if sum(protein_mask) > 0:\n",
    "                print(f\"Identified {sum(protein_mask)} potential protein features based on naming patterns\")\n",
    "                adata.var.loc[protein_mask, 'feature_type'] = 'Antibody Capture'\n",
    "    \n",
    "    # Split gene expression and protein expression\n",
    "    gene_mask = adata.var['feature_type'] == 'Gene Expression'\n",
    "    protein_mask = adata.var['feature_type'] == 'Antibody Capture'\n",
    "    \n",
    "    print(f\"Gene features: {sum(gene_mask)}, Protein features: {sum(protein_mask)}\")\n",
    "    \n",
    "    if sum(gene_mask) > 0:\n",
    "        gene_expr = adata[:, gene_mask].copy()\n",
    "    else:\n",
    "        gene_expr = ad.AnnData(\n",
    "            X=sparse.csr_matrix((adata.n_obs, 0)),\n",
    "            obs=adata.obs.copy()\n",
    "        )\n",
    "    \n",
    "    if sum(protein_mask) > 0:\n",
    "        protein_expr = adata[:, protein_mask].copy()\n",
    "    else:\n",
    "        protein_expr = ad.AnnData(\n",
    "            X=sparse.csr_matrix((adata.n_obs, 0)),\n",
    "            obs=adata.obs.copy()\n",
    "        )\n",
    "    \n",
    "    # Add metadata\n",
    "    for adata_obj in [gene_expr, protein_expr]:\n",
    "        if adata_obj.n_obs == 0:\n",
    "            continue\n",
    "            \n",
    "        # Add organism information\n",
    "        adata_obj.obs['organism'] = 'Homo sapiens'\n",
    "        \n",
    "        # Add cell type information based on sample name\n",
    "        if 'CD4' in sample_name:\n",
    "            adata_obj.obs['cell_type'] = 'CD4+ T Cells'\n",
    "        elif 'NALM6' in sample_name or 'A375' in sample_name or 'Resting' in sample_name:\n",
    "            adata_obj.obs['cell_type'] = 'T Cells'\n",
    "        else:\n",
    "            adata_obj.obs['cell_type'] = 'T Cells'\n",
    "        \n",
    "        # Add CRISPR type information\n",
    "        adata_obj.obs['crispr_type'] = 'None'  # No CRISPR in this dataset\n",
    "        \n",
    "        # Add cancer type information\n",
    "        if 'NALM6' in sample_name:\n",
    "            adata_obj.obs['cancer_type'] = 'B-cell acute lymphoblastic leukemia'\n",
    "        elif 'A375' in sample_name:\n",
    "            adata_obj.obs['cancer_type'] = 'Melanoma'\n",
    "        else:\n",
    "            adata_obj.obs['cancer_type'] = 'Non-Cancer'\n",
    "        \n",
    "        # Add condition information\n",
    "        if 'Resting' in sample_name:\n",
    "            adata_obj.obs['condition'] = 'Resting'\n",
    "        elif 'In_Vivo' in sample_name:\n",
    "            adata_obj.obs['condition'] = 'In vivo'\n",
    "        elif 'Arrayed' in sample_name:\n",
    "            adata_obj.obs['condition'] = 'Arrayed co-culture'\n",
    "        elif 'Pooled' in sample_name:\n",
    "            adata_obj.obs['condition'] = 'Pooled co-culture'\n",
    "        elif 'CD4_Spike-In' in sample_name:\n",
    "            adata_obj.obs['condition'] = 'CD4 spike-in'\n",
    "        else:\n",
    "            adata_obj.obs['condition'] = 'Unknown'\n",
    "            \n",
    "        # Add perturbation_name (default to Unknown)\n",
    "        adata_obj.obs['perturbation_name'] = 'Unknown'\n",
    "    \n",
    "    # Add CAR barcode information if available\n",
    "    if feature_ref is not None:\n",
    "        # Map cell barcodes to CAR barcodes\n",
    "        if 'cell_barcode' in feature_ref.columns and 'CAR_barcode' in feature_ref.columns:\n",
    "            # Create a dictionary mapping cell barcodes to CAR barcodes\n",
    "            cell_to_car = dict(zip(feature_ref['cell_barcode'], feature_ref['CAR_barcode']))\n",
    "            \n",
    "            # Add CAR barcode information to obs\n",
    "            for adata_obj in [gene_expr, protein_expr]:\n",
    "                if adata_obj.n_obs == 0:\n",
    "                    continue\n",
    "                \n",
    "                adata_obj.obs['CAR_barcode'] = 'Unknown'\n",
    "                for cell_barcode in adata_obj.obs_names:\n",
    "                    if cell_barcode in cell_to_car:\n",
    "                        adata_obj.obs.loc[cell_barcode, 'CAR_barcode'] = cell_to_car[cell_barcode]\n",
    "                \n",
    "                # Add CAR architecture information\n",
    "                adata_obj.obs['perturbation_name'] = 'Unknown'\n",
    "                for car_id, car_arch in CAR_ARCHITECTURES.items():\n",
    "                    # Create a mask for cells with this CAR\n",
    "                    car_mask = adata_obj.obs['CAR_barcode'].str.contains(car_id, regex=False)\n",
    "                    if car_mask.any():\n",
    "                        adata_obj.obs.loc[car_mask, 'perturbation_name'] = car_arch\n",
    "                \n",
    "                # Add CAR component flags\n",
    "                adata_obj.obs['has_CD28'] = adata_obj.obs['perturbation_name'].str.contains('CD28', regex=False)\n",
    "                adata_obj.obs['has_41BB'] = adata_obj.obs['perturbation_name'].str.contains('41BB', regex=False) | adata_obj.obs['perturbation_name'].str.contains('4-1BB', regex=False)\n",
    "                adata_obj.obs['has_ICOS'] = adata_obj.obs['perturbation_name'].str.contains('ICOS', regex=False)\n",
    "                adata_obj.obs['has_OX40'] = adata_obj.obs['perturbation_name'].str.contains('OX40', regex=False)\n",
    "                adata_obj.obs['has_CD27'] = adata_obj.obs['perturbation_name'].str.contains('CD27', regex=False)\n",
    "                adata_obj.obs['has_CD3z'] = adata_obj.obs['perturbation_name'].str.contains('CD3z', regex=False)\n",
    "                \n",
    "                # Convert boolean columns to string for consistency\n",
    "                for col in ['has_CD28', 'has_41BB', 'has_ICOS', 'has_OX40', 'has_CD27', 'has_CD3z']:\n",
    "                    adata_obj.obs[col] = adata_obj.obs[col].astype(str)\n",
    "    \n",
    "    return gene_expr, protein_expr\n",
    "\n",
    "def harmonize_data(data_dir):\n",
    "    \"\"\"Process all samples and harmonize the data.\"\"\"\n",
    "    # Define sample to feature reference file mapping\n",
    "    samples = {\n",
    "        \"Arrayed_NALM6_D1\": \"GSE264681_Arrayed_NALM6_D1_feature_reference_CARBC.csv.gz\",\n",
    "        \"Arrayed_NALM6_D2\": \"GSE264681_Arrayed_NALM6_D2_feature_reference_CARBC.csv.gz\",\n",
    "        \"CD4_Spike-In\": \"GSE264681_CD4_Spike-In_feature_reference_CARBC.csv.gz\",\n",
    "        \"In_Vivo_A375\": \"GSE264681_In_Vivo_A375_feature_reference_CITEseq_CARBC.csv.gz\",\n",
    "        \"Pooled_A375\": \"GSE264681_Pooled_A375_feature_reference_CARBC.csv.gz\",\n",
    "        \"Pooled_NALM6\": \"GSE264681_Pooled_NALM6_feature_reference_CARBC.csv.gz\",\n",
    "        \"Resting\": \"GSE264681_Resting_feature_reference_CARBC.csv.gz\"\n",
    "    }\n",
    "    \n",
    "    # Process each sample\n",
    "    gene_adatas = []\n",
    "    protein_adatas = []\n",
    "    \n",
    "    for sample_name, feature_ref_file in samples.items():\n",
    "        print(f\"Processing sample: {sample_name}\")\n",
    "        gene_adata, protein_adata = process_sample(sample_name, data_dir, feature_ref_file)\n",
    "        \n",
    "        if gene_adata is not None and gene_adata.n_obs > 0:\n",
    "            print(f\"Adding gene expression data for {sample_name}: {gene_adata.n_obs} cells, {gene_adata.n_vars} genes\")\n",
    "            # Make cell barcodes unique by adding sample name\n",
    "            gene_adata.obs_names = [f\"{sample_name}_{bc}\" for bc in gene_adata.obs_names]\n",
    "            gene_adatas.append(gene_adata)\n",
    "        else:\n",
    "            print(f\"Skipping gene expression data for {sample_name}: no data or errors occurred\")\n",
    "        \n",
    "        if protein_adata is not None and protein_adata.n_obs > 0:\n",
    "            print(f\"Adding protein expression data for {sample_name}: {protein_adata.n_obs} cells, {protein_adata.n_vars} proteins\")\n",
    "            # Make cell barcodes unique by adding sample name\n",
    "            protein_adata.obs_names = [f\"{sample_name}_{bc}\" for bc in protein_adata.obs_names]\n",
    "            protein_adatas.append(protein_adata)\n",
    "        else:\n",
    "            print(f\"Skipping protein expression data for {sample_name}: no data or errors occurred\")\n",
    "    \n",
    "    # Combine all gene expression data\n",
    "    combined_gene = None\n",
    "    if gene_adatas:\n",
    "        try:\n",
    "            # Check for duplicate var_names\n",
    "            for adata in gene_adatas:\n",
    "                adata.var_names_make_unique()\n",
    "            \n",
    "            # Concatenate all gene expression data\n",
    "            combined_gene = ad.concat(gene_adatas, join='outer', merge='same')\n",
    "            print(f\"Combined gene expression data: {combined_gene.n_obs} cells, {combined_gene.n_vars} genes\")\n",
    "            \n",
    "            # Ensure var_names are unique\n",
    "            combined_gene.var_names_make_unique()\n",
    "        except Exception as e:\n",
    "            print(f\"Error combining gene expression data: {e}\")\n",
    "    else:\n",
    "        print(\"No gene expression data to combine\")\n",
    "    \n",
    "    # Combine all protein expression data\n",
    "    combined_protein = None\n",
    "    if protein_adatas:\n",
    "        try:\n",
    "            # Check for duplicate var_names\n",
    "            for adata in protein_adatas:\n",
    "                adata.var_names_make_unique()\n",
    "            \n",
    "            # Concatenate all protein expression data\n",
    "            combined_protein = ad.concat(protein_adatas, join='outer', merge='same')\n",
    "            print(f\"Combined protein expression data: {combined_protein.n_obs} cells, {combined_protein.n_vars} proteins\")\n",
    "            \n",
    "            # Ensure var_names are unique\n",
    "            combined_protein.var_names_make_unique()\n",
    "        except Exception as e:\n",
    "            print(f\"Error combining protein expression data: {e}\")\n",
    "    else:\n",
    "        print(\"No protein expression data to combine\")\n",
    "    \n",
    "    # Keep only paired cells (cells that exist in both gene and protein datasets)\n",
    "    if combined_gene is not None and combined_protein is not None:\n",
    "        common_cells = list(set(combined_gene.obs_names).intersection(set(combined_protein.obs_names)))\n",
    "        if common_cells:\n",
    "            print(f\"Found {len(common_cells)} cells in both gene and protein expression datasets\")\n",
    "            combined_gene = combined_gene[common_cells].copy()\n",
    "            combined_protein = combined_protein[common_cells].copy()\n",
    "            \n",
    "            # Ensure the cell order is the same in both datasets\n",
    "            combined_protein = combined_protein[combined_gene.obs_names].copy()\n",
    "        else:\n",
    "            print(\"Warning: No common cells found between gene and protein expression datasets\")\n",
    "    \n",
    "    return combined_gene, combined_protein\n",
    "\n",
    "def process_and_save_all_data(data_dir):\n",
    "    \"\"\"Main function to process the dataset.\"\"\"\n",
    "    # Download files if they don't exist\n",
    "    download_files(data_dir)\n",
    "    \n",
    "    # Check if we have at least some files to process\n",
    "    available_files = [f for f in FILES if os.path.exists(os.path.join(data_dir, f))]\n",
    "    if not available_files:\n",
    "        print(\"\\nNo files were successfully downloaded. Please check your internet connection.\")\n",
    "        print(\"You can download the files manually from GEO:\")\n",
    "        print(\"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE264681\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nSuccessfully downloaded or found {len(available_files)} out of {len(FILES)} files.\")\n",
    "    print(\"Proceeding with available data...\")\n",
    "    \n",
    "    # Process and harmonize the data\n",
    "    gene_adata, protein_adata = harmonize_data(data_dir)\n",
    "    \n",
    "    # Check if we have any data to save\n",
    "    if gene_adata is None and protein_adata is None:\n",
    "        print(\"\\nNo data was successfully processed. Please check the error messages above.\")\n",
    "        print(\"You may need to download some files manually from GEO:\")\n",
    "        print(\"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE264681\")\n",
    "        return\n",
    "    \n",
    "    # Save the harmonized data\n",
    "    if gene_adata is not None:\n",
    "        gene_adata.write_h5ad(os.path.join(data_dir, 'gene_expression.h5ad'))\n",
    "        print(f\"Saved gene expression data to {os.path.join(data_dir, 'gene_expression.h5ad')}\")\n",
    "        print(f\"Gene expression data shape: {gene_adata.shape}\")\n",
    "    \n",
    "    if protein_adata is not None:\n",
    "        protein_adata.write_h5ad(os.path.join(data_dir, 'protein_expression.h5ad'))\n",
    "        print(f\"Saved protein expression data to {os.path.join(data_dir, 'protein_expression.h5ad')}\")\n",
    "        print(f\"Protein expression data shape: {protein_adata.shape}\")\n",
    "    \n",
    "    print(\"\\nProcessing complete!\")\n",
    "\n",
    "# Example usage in Jupyter\n",
    "# Adjust 'data_dir' to the folder where you want data to be downloaded and processed\n",
    "data_dir = \"./GSE264681_data\"\n",
    "\n",
    "# Run the main processing function\n",
    "process_and_save_all_data(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a838de-845c-4b24-ad68-772ecf4637f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
