{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b61713b-4c06-4222-8853-da54ecde594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% [code]\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import gzip\n",
    "import time\n",
    "import shutil\n",
    "import tarfile\n",
    "import logging\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "from scipy import sparse, io\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Constants\n",
    "GEO_ACCESSION = \"GSE279914\"\n",
    "GEO_URL = f\"https://ftp.ncbi.nlm.nih.gov/geo/series/GSE279nnn/{GEO_ACCESSION}/suppl/{GEO_ACCESSION}_RAW.tar\"\n",
    "\n",
    "def setup_logger():\n",
    "    \"\"\"Configure logging for the script.\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        handlers=[logging.StreamHandler(sys.stdout)]\n",
    "    )\n",
    "    return logging.getLogger(\"GSE279914_processor\")\n",
    "\n",
    "def download_data(data_dir: Path, logger) -> Path:\n",
    "    \"\"\"\n",
    "    Download the dataset from GEO if not already present.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory where data will be stored\n",
    "        logger: Logger object\n",
    "        \n",
    "    Returns:\n",
    "        Path to the downloaded tar file\n",
    "    \"\"\"\n",
    "    tar_path = data_dir / f\"{GEO_ACCESSION}_RAW.tar\"\n",
    "    \n",
    "    if not tar_path.exists():\n",
    "        logger.info(f\"Downloading {GEO_ACCESSION} dataset from GEO...\")\n",
    "        data_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "            }\n",
    "            req = urllib.request.Request(GEO_URL, headers=headers)\n",
    "            \n",
    "            with urllib.request.urlopen(req) as response:\n",
    "                total_size = int(response.info().get('Content-Length', 0))\n",
    "                block_size = 1024 * 1024  # 1MB chunks\n",
    "                \n",
    "                with open(tar_path, 'wb') as out_file:\n",
    "                    downloaded = 0\n",
    "                    for chunk in tqdm(\n",
    "                        iter(lambda: response.read(block_size), b''),\n",
    "                        total=total_size // block_size + 1,\n",
    "                        unit='MB',\n",
    "                        desc=\"Downloading\"\n",
    "                    ):\n",
    "                        out_file.write(chunk)\n",
    "                        downloaded += len(chunk)\n",
    "            \n",
    "            logger.info(f\"Download complete: {tar_path}\")\n",
    "        except urllib.error.HTTPError as e:\n",
    "            if e.code == 403:\n",
    "                logger.error(\"Access forbidden (HTTP 403). The dataset may require authentication or is not publicly available.\")\n",
    "                logger.info(\"Please download the dataset manually from GEO and place it in the data directory.\")\n",
    "                logger.info(f\"Download URL: {GEO_URL}\")\n",
    "                logger.info(f\"Expected file path: {tar_path}\")\n",
    "                \n",
    "                try:\n",
    "                    from IPython.display import display, HTML\n",
    "                    display(HTML(f\"\"\"\n",
    "                    <div style=\"background-color: #ffffcc; padding: 10px; border: 1px solid #ffcc00; border-radius: 5px;\">\n",
    "                        <h3>Manual Download Required</h3>\n",
    "                        <p>The dataset cannot be downloaded automatically due to access restrictions.</p>\n",
    "                        <p>Please download the dataset manually from GEO:</p>\n",
    "                        <ol>\n",
    "                            <li>Go to <a href=\"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc={GEO_ACCESSION}\" target=\"_blank\">GEO {GEO_ACCESSION}</a></li>\n",
    "                            <li>Download the RAW.tar file</li>\n",
    "                            <li>Place it at: <code>{tar_path}</code></li>\n",
    "                        </ol>\n",
    "                    </div>\n",
    "                    \"\"\"))\n",
    "                except ImportError:\n",
    "                    pass\n",
    "                \n",
    "                while not tar_path.exists():\n",
    "                    logger.info(\"Waiting for manual download... Press Ctrl+C to cancel.\")\n",
    "                    try:\n",
    "                        for _ in tqdm(range(60), desc=\"Waiting\", unit=\"s\"):\n",
    "                            time.sleep(1)\n",
    "                    except KeyboardInterrupt:\n",
    "                        logger.error(\"Process canceled by user.\")\n",
    "                        sys.exit(1)\n",
    "                \n",
    "                logger.info(f\"Found manually downloaded file: {tar_path}\")\n",
    "            else:\n",
    "                raise\n",
    "    else:\n",
    "        logger.info(f\"Using existing download: {tar_path}\")\n",
    "    \n",
    "    return tar_path\n",
    "\n",
    "def extract_data(tar_path: Path, data_dir: Path, logger) -> Path:\n",
    "    \"\"\"\n",
    "    Extract the downloaded tar file.\n",
    "    \n",
    "    Args:\n",
    "        tar_path: Path to the downloaded tar file\n",
    "        data_dir: Directory where data will be extracted\n",
    "        logger: Logger object\n",
    "        \n",
    "    Returns:\n",
    "        Path to the extracted directory\n",
    "    \"\"\"\n",
    "    extract_dir = data_dir / \"extracted\"\n",
    "    \n",
    "    if not extract_dir.exists() or not any(extract_dir.iterdir()):\n",
    "        logger.info(f\"Extracting {tar_path} to {extract_dir}...\")\n",
    "        extract_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with tarfile.open(tar_path, 'r') as tar:\n",
    "            members = tar.getmembers()\n",
    "            for member in tqdm(members, desc=\"Extracting files\"):\n",
    "                tar.extract(member, path=extract_dir)\n",
    "        \n",
    "        logger.info(f\"Extraction complete: {extract_dir}\")\n",
    "    else:\n",
    "        logger.info(f\"Using existing extracted data: {extract_dir}\")\n",
    "    \n",
    "    return extract_dir\n",
    "\n",
    "def find_10x_data_files(extract_dir: Path, logger) -> List[Dict[str, Path]]:\n",
    "    \"\"\"\n",
    "    Find all sets of 10X Genomics data files.\n",
    "    \n",
    "    Args:\n",
    "        extract_dir: Directory containing extracted data\n",
    "        logger: Logger object\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing paths to matrix, features, and barcodes files\n",
    "    \"\"\"\n",
    "    matrix_files = list(extract_dir.glob(\"**/*matrix.mtx.gz\"))\n",
    "    \n",
    "    data_sets = []\n",
    "    for matrix_file in matrix_files:\n",
    "        base_name = matrix_file.name.replace('matrix.mtx.gz', '')\n",
    "        base_dir = matrix_file.parent\n",
    "        \n",
    "        features_file = base_dir / f\"{base_name}features.tsv.gz\"\n",
    "        barcodes_file = base_dir / f\"{base_name}barcodes.tsv.gz\"\n",
    "        \n",
    "        if features_file.exists() and barcodes_file.exists():\n",
    "            sample_id = matrix_file.name.split('_')[0]\n",
    "            \n",
    "            data_sets.append({\n",
    "                'sample_id': sample_id,\n",
    "                'matrix_file': matrix_file,\n",
    "                'features_file': features_file,\n",
    "                'barcodes_file': barcodes_file\n",
    "            })\n",
    "    \n",
    "    logger.info(f\"Found {len(data_sets)} 10X data sets\")\n",
    "    return data_sets\n",
    "\n",
    "def split_gene_and_protein_data(adata: ad.AnnData, logger) -> Tuple[ad.AnnData, ad.AnnData]:\n",
    "    \"\"\"\n",
    "    Split an AnnData object into gene expression and protein data.\n",
    "    \n",
    "    Args:\n",
    "        adata: AnnData object containing both gene expression and protein data\n",
    "        logger: Logger object\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (gene_adata, protein_adata)\n",
    "    \"\"\"\n",
    "    if 'feature_types' not in adata.var:\n",
    "        protein_pattern = '^(Hash|CD|HLA|IgG)'\n",
    "        adata.var['feature_types'] = 'Gene Expression'\n",
    "        adata.var.loc[adata.var_names.str.match(protein_pattern), 'feature_types'] = 'Antibody Capture'\n",
    "    \n",
    "    logger.info(f\"Feature types: {adata.var['feature_types'].unique()}\")\n",
    "    logger.info(f\"Number of Gene Expression features: {(adata.var['feature_types'] == 'Gene Expression').sum()}\")\n",
    "    logger.info(f\"Number of Antibody Capture features: {(adata.var['feature_types'] == 'Antibody Capture').sum()}\")\n",
    "    \n",
    "    gene_data = adata[:, adata.var['feature_types'] == 'Gene Expression'].copy()\n",
    "    protein_data = adata[:, adata.var['feature_types'] == 'Antibody Capture'].copy()\n",
    "    \n",
    "    logger.info(f\"Gene data shape: {gene_data.shape}\")\n",
    "    logger.info(f\"Protein data shape: {protein_data.shape}\")\n",
    "    \n",
    "    if protein_data.shape[1] == 0:\n",
    "        logger.warning(\"No protein features found in the data\")\n",
    "    \n",
    "    return gene_data, protein_data\n",
    "\n",
    "def process_10x_data(data_dir: Path, logger) -> Tuple[ad.AnnData, ad.AnnData]:\n",
    "    \"\"\"\n",
    "    Process 10X Genomics data and split into gene expression and protein data.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing extracted data\n",
    "        logger: Logger object\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (gene_adata, protein_adata)\n",
    "    \"\"\"\n",
    "    extract_dir = data_dir / \"extracted\"\n",
    "    data_sets = find_10x_data_files(extract_dir, logger)\n",
    "    \n",
    "    gene_adatas = []\n",
    "    protein_adatas = []\n",
    "    \n",
    "    for data_set in tqdm(data_sets, desc=\"Processing 10X data\"):\n",
    "        sample_id = data_set['sample_id']\n",
    "        matrix_file = data_set['matrix_file']\n",
    "        features_file = data_set['features_file']\n",
    "        barcodes_file = data_set['barcodes_file']\n",
    "        \n",
    "        temp_dir = data_dir / \"temp\" / sample_id\n",
    "        temp_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        shutil.copy(matrix_file, temp_dir / \"matrix.mtx.gz\")\n",
    "        shutil.copy(features_file, temp_dir / \"features.tsv.gz\")\n",
    "        shutil.copy(barcodes_file, temp_dir / \"barcodes.tsv.gz\")\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"Reading data for {sample_id} using direct approach\")\n",
    "            \n",
    "            mtx_file = temp_dir / \"matrix.mtx.gz\"\n",
    "            with gzip.open(mtx_file, 'rb') as f:\n",
    "                X = io.mmread(f).T.tocsr()\n",
    "            \n",
    "            features_file = temp_dir / \"features.tsv.gz\"\n",
    "            features = pd.read_csv(features_file, sep='\\t', header=None, names=['id', 'name', 'feature_type'])\n",
    "            \n",
    "            barcodes_file = temp_dir / \"barcodes.tsv.gz\"\n",
    "            barcodes = pd.read_csv(barcodes_file, sep='\\t', header=None)[0].values\n",
    "            \n",
    "            adata = ad.AnnData(X=X, obs=pd.DataFrame(index=barcodes), var=pd.DataFrame(index=features['name'].values))\n",
    "            adata.var['feature_types'] = features['feature_type'].values\n",
    "            \n",
    "            logger.info(f\"Feature types in {sample_id}: {adata.var['feature_types'].unique()}\")\n",
    "            logger.info(f\"Number of Gene Expression features: {(adata.var['feature_types'] == 'Gene Expression').sum()}\")\n",
    "            logger.info(f\"Number of Antibody Capture features: {(adata.var['feature_types'] == 'Antibody Capture').sum()}\")\n",
    "            \n",
    "            if adata.var_names.duplicated().any():\n",
    "                logger.warning(f\"Found duplicate var names in {sample_id}, making them unique\")\n",
    "                adata.var_names_make_unique()\n",
    "            \n",
    "            adata.obs['sample_id'] = sample_id\n",
    "            \n",
    "            gene_data, protein_data = split_gene_and_protein_data(adata, logger)\n",
    "            \n",
    "            gene_data.obs['organism'] = 'Homo sapiens'\n",
    "            gene_data.obs['cell_type'] = 'bone marrow cells'\n",
    "            gene_data.obs['cancer_type'] = 'Mixed Cell Lines' if 'CellLineMix' in sample_id else 'Non-Cancer'\n",
    "            gene_data.obs['condition'] = 'diagnosis'\n",
    "            gene_data.obs['perturbation_name'] = 'None'\n",
    "            gene_data.obs['disease'] = 'myelodysplastic syndrome'\n",
    "            gene_data.obs['tissue'] = 'bone marrow'\n",
    "            gene_data.obs['crispr_type'] = 'None'\n",
    "            gene_data.obs['donor'] = sample_id\n",
    "            gene_data.obs['is_cell_line'] = 'CellLineMix' in sample_id\n",
    "            gene_data.obs['lane'] = matrix_file.name.split('_')[1].replace('matrix.mtx.gz', '')\n",
    "            gene_data.obs['batch'] = 'batch1'\n",
    "            gene_data.obs['title'] = f\"GSE279914_{sample_id}\"\n",
    "            \n",
    "            protein_data.obs = gene_data.obs.copy()\n",
    "            \n",
    "            gene_adatas.append(gene_data)\n",
    "            protein_adatas.append(protein_data)\n",
    "            \n",
    "            logger.info(f\"Processed sample {sample_id}: {gene_data.shape} genes, {protein_data.shape} proteins\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing data from {sample_id}: {e}\")\n",
    "        \n",
    "        shutil.rmtree(temp_dir)\n",
    "    \n",
    "    if gene_adatas:\n",
    "        gene_adata = ad.concat(gene_adatas, join='outer', label='sample_id', index_unique='-')\n",
    "        logger.info(f\"Combined gene expression data: {gene_adata.shape}\")\n",
    "    else:\n",
    "        gene_adata = None\n",
    "        logger.warning(\"No gene expression data found\")\n",
    "    \n",
    "    if protein_adatas:\n",
    "        protein_adata = ad.concat(protein_adatas, join='outer', label='sample_id', index_unique='-')\n",
    "        logger.info(f\"Combined protein data: {protein_adata.shape}\")\n",
    "    else:\n",
    "        protein_adata = None\n",
    "        logger.warning(\"No protein data found\")\n",
    "    \n",
    "    return gene_adata, protein_adata\n",
    "\n",
    "def check_for_duplicate_genes(adata: ad.AnnData, logger) -> ad.AnnData:\n",
    "    \"\"\"\n",
    "    Check for and handle duplicate gene names in var_names.\n",
    "    \n",
    "    Args:\n",
    "        adata: AnnData object to check\n",
    "        logger: Logger object\n",
    "        \n",
    "    Returns:\n",
    "        AnnData object with unique var_names\n",
    "    \"\"\"\n",
    "    duplicates = adata.var_names.duplicated()\n",
    "    n_duplicates = duplicates.sum()\n",
    "    \n",
    "    if n_duplicates > 0:\n",
    "        logger.warning(f\"Found {n_duplicates} duplicate gene names\")\n",
    "        adata_unique = adata.copy()\n",
    "        dup_genes = adata.var_names[duplicates].tolist()\n",
    "        logger.info(f\"Duplicate genes: {dup_genes[:10]}...\")\n",
    "        adata_unique.var_names_make_unique()\n",
    "        return adata_unique\n",
    "    else:\n",
    "        logger.info(\"No duplicate gene names found\")\n",
    "        return adata\n",
    "\n",
    "def create_paired_datasets(gene_adata: ad.AnnData, protein_adata: ad.AnnData, logger) -> Tuple[ad.AnnData, ad.AnnData]:\n",
    "    \"\"\"\n",
    "    Create paired datasets where gene and protein data share the same cells.\n",
    "    \n",
    "    Args:\n",
    "        gene_adata: AnnData object containing gene expression data\n",
    "        protein_adata: AnnData object containing protein data\n",
    "        logger: Logger object\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (paired_gene_adata, paired_protein_adata)\n",
    "    \"\"\"\n",
    "    if gene_adata is None or protein_adata is None:\n",
    "        logger.warning(\"Cannot create paired datasets: missing gene or protein data\")\n",
    "        return None, None\n",
    "    \n",
    "    common_barcodes = np.intersect1d(gene_adata.obs_names, protein_adata.obs_names)\n",
    "    logger.info(f\"Found {len(common_barcodes)} cells with both gene and protein data\")\n",
    "    \n",
    "    if len(common_barcodes) == 0:\n",
    "        logger.warning(\"No common cells found between gene and protein data\")\n",
    "        return None, None\n",
    "    \n",
    "    paired_gene_adata = gene_adata[common_barcodes].copy()\n",
    "    paired_protein_adata = protein_adata[common_barcodes].copy()\n",
    "    paired_protein_adata = paired_protein_adata[paired_gene_adata.obs_names].copy()\n",
    "    \n",
    "    return paired_gene_adata, paired_protein_adata\n",
    "\n",
    "def process_data(data_dir: Path, logger) -> None:\n",
    "    \"\"\"\n",
    "    Process the GSE279914 dataset.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory where data will be stored\n",
    "        logger: Logger object\n",
    "    \"\"\"\n",
    "    tar_path = download_data(data_dir, logger)\n",
    "    extract_data(tar_path, data_dir, logger)\n",
    "    \n",
    "    gene_adata, protein_adata = process_10x_data(data_dir, logger)\n",
    "    \n",
    "    if gene_adata is not None:\n",
    "        gene_adata = check_for_duplicate_genes(gene_adata, logger)\n",
    "    \n",
    "    paired_gene_adata, paired_protein_adata = create_paired_datasets(gene_adata, protein_adata, logger)\n",
    "    \n",
    "    # --- QC Filtering Step ---\n",
    "    if paired_gene_adata is not None and paired_protein_adata is not None:\n",
    "        logger.info(\"Performing QC filtering on paired gene expression data...\")\n",
    "        logger.info(f\"Number of cells before QC filtering: {paired_gene_adata.n_obs}\")\n",
    "        sc.pp.calculate_qc_metrics(paired_gene_adata, inplace=True)\n",
    "        qc_threshold = 200\n",
    "        paired_gene_adata_qc = paired_gene_adata[paired_gene_adata.obs['n_genes_by_counts'] >= qc_threshold].copy()\n",
    "        logger.info(f\"Number of cells after QC filtering: {paired_gene_adata_qc.n_obs}\")\n",
    "        \n",
    "        # Update the paired protein data to keep only cells present after QC filtering\n",
    "        paired_protein_adata_qc = paired_protein_adata[paired_gene_adata_qc.obs_names].copy()\n",
    "        paired_gene_adata, paired_protein_adata = paired_gene_adata_qc, paired_protein_adata_qc\n",
    "    # --- End QC Filtering Step ---\n",
    "    \n",
    "    # --- Clean Protein var_names: Remove suffix after '-' ---\n",
    "    if paired_protein_adata is not None:\n",
    "        logger.info(\"Cleaning protein var_names: removing suffix\")\n",
    "        # For each protein name, keep only the part before the hyphen\n",
    "        cleaned_var_names = [name.split('-')[0] for name in paired_protein_adata.var_names]\n",
    "        paired_protein_adata.var_names = cleaned_var_names\n",
    "    # --- End Cleaning Protein var_names ---\n",
    "    \n",
    "    output_dir = data_dir / GEO_ACCESSION\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if gene_adata is not None:\n",
    "        gene_adata.write_h5ad(output_dir / f\"{GEO_ACCESSION}_gene_expression.h5ad\")\n",
    "        logger.info(f\"Saved all gene expression data: {gene_adata.shape}\")\n",
    "    \n",
    "    if protein_adata is not None:\n",
    "        protein_adata.write_h5ad(output_dir / f\"{GEO_ACCESSION}_protein_expression.h5ad\")\n",
    "        logger.info(f\"Saved all protein expression data: {protein_adata.shape}\")\n",
    "    \n",
    "    if paired_gene_adata is not None and paired_protein_adata is not None:\n",
    "        paired_gene_adata.write_h5ad(output_dir / f\"{GEO_ACCESSION}_paired_gene_expression.h5ad\")\n",
    "        paired_protein_adata.write_h5ad(output_dir / f\"{GEO_ACCESSION}_paired_protein_expression.h5ad\")\n",
    "        logger.info(f\"Saved paired gene expression data: {paired_gene_adata.shape}\")\n",
    "        logger.info(f\"Saved paired protein expression data: {paired_protein_adata.shape}\")\n",
    "\n",
    "#%% [code]\n",
    "def run_processing(data_dir: Union[str, Path] = None):\n",
    "    \"\"\"\n",
    "    Run the GSE279914 processing pipeline.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Optional directory to store data. Defaults to \"./GSE279914\".\n",
    "    \"\"\"\n",
    "    logger = setup_logger()\n",
    "    \n",
    "    if data_dir is None:\n",
    "        data_dir = Path(\"./GSE279914\")\n",
    "    elif isinstance(data_dir, str):\n",
    "        data_dir = Path(data_dir)\n",
    "    \n",
    "    logger.info(f\"Processing {GEO_ACCESSION} dataset\")\n",
    "    logger.info(f\"Data directory: {data_dir}\")\n",
    "    \n",
    "    process_data(data_dir, logger)\n",
    "    logger.info(\"Processing complete\")\n",
    "\n",
    "# Run the processing pipeline\n",
    "run_processing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a838de-845c-4b24-ad68-772ecf4637f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
