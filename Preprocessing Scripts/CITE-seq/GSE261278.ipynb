{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b61713b-4c06-4222-8853-da54ecde594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "from scipy import sparse\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import re\n",
    "\n",
    "# Constants\n",
    "GEO_ACCESSION = \"GSE261278\"\n",
    "BASE_URL = f\"https://ftp.ncbi.nlm.nih.gov/geo/series/GSE261nnn/{GEO_ACCESSION}/suppl/\"\n",
    "SAMPLE_IDS = [\"D534\", \"D559\", \"D561\", \"D564\", \"D568\", \"D574\", \"D616\", \"FLD032FLD040FLD015FLD041FLD029FLD030\"]\n",
    "\n",
    "def simplify_protein_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Simplify a protein name by removing parentheses and extra tokens.\n",
    "    For example, \"anti-human_CD101_(BB27)\" becomes \"CD101\".\n",
    "    \"\"\"\n",
    "    # Remove any parentheses and content inside them\n",
    "    name = re.sub(r'\\s*\\(.*?\\)', '', name)\n",
    "    \n",
    "    # If the name starts with \"anti-\", drop everything before (and including) the first underscore.\n",
    "    if name.startswith(\"anti-\"):\n",
    "        parts = name.split('_', 1)\n",
    "        if len(parts) > 1:\n",
    "            name = parts[1]\n",
    "    \n",
    "    # Split the remaining string by underscores.\n",
    "    tokens = name.split('_')\n",
    "    # Return the first token that starts with \"CD\" or \"Ig\"\n",
    "    for token in tokens:\n",
    "        token_clean = token.rstrip(',').strip()\n",
    "        if token_clean.startswith(\"CD\") or token_clean.startswith(\"Ig\"):\n",
    "            return token_clean\n",
    "    # If no such token, return the cleaned name.\n",
    "    return name.strip()\n",
    "\n",
    "def download_file(url: str, destination: str) -> None:\n",
    "    \"\"\"\n",
    "    Download a file from a URL to a destination path using a custom user-agent.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(destination):\n",
    "        print(f\"Downloading {url} to {destination}\")\n",
    "        try:\n",
    "            req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            with urllib.request.urlopen(req) as response, open(destination, 'wb') as out_file:\n",
    "                out_file.write(response.read())\n",
    "            print(f\"Downloaded {destination}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {url}: {e}\")\n",
    "    else:\n",
    "        print(f\"File {destination} already exists, skipping download\")\n",
    "\n",
    "def download_dataset(data_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Download all dataset files if they don't exist.\n",
    "    \"\"\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    # Define all files to download\n",
    "    files_to_download = []\n",
    "    \n",
    "    # Add GEX and ADT matrices for each sample\n",
    "    for sample_id in SAMPLE_IDS:\n",
    "        files_to_download.extend([\n",
    "            (f\"{BASE_URL}{GEO_ACCESSION}_{sample_id}.GEX.matrix.txt.gz\", \n",
    "             os.path.join(data_dir, f\"{GEO_ACCESSION}_{sample_id}.GEX.matrix.txt.gz\")),\n",
    "            (f\"{BASE_URL}{GEO_ACCESSION}_{sample_id}.ADT.matrix.txt.gz\", \n",
    "             os.path.join(data_dir, f\"{GEO_ACCESSION}_{sample_id}.ADT.matrix.txt.gz\"))\n",
    "        ])\n",
    "    \n",
    "    # Download files in parallel\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        for url, destination in files_to_download:\n",
    "            executor.submit(download_file, url, destination)\n",
    "\n",
    "def read_matrix_file(file_path: str) -> Tuple[sparse.csr_matrix, pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Read a matrix file and return a sparse matrix, gene/protein info, and cell barcodes.\n",
    "    \"\"\"\n",
    "    print(f\"Reading {file_path}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # First pass: get dimensions and headers\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        header = f.readline().strip().split('\\t')\n",
    "        cell_barcodes = header[2:]  # Skip 'accession' and 'gene' columns\n",
    "        gene_count = sum(1 for _ in f)\n",
    "    \n",
    "    print(f\"  Found {gene_count} genes/proteins and {len(cell_barcodes)} cells\")\n",
    "    \n",
    "    # Second pass: read data efficiently into sparse matrix\n",
    "    data = []\n",
    "    row_indices = []\n",
    "    col_indices = []\n",
    "    gene_ids = []\n",
    "    gene_names = []\n",
    "    \n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        # Skip header\n",
    "        f.readline()\n",
    "        \n",
    "        for row_idx, line in enumerate(f):\n",
    "            parts = line.strip().split('\\t')\n",
    "            gene_id = parts[0]\n",
    "            gene_name = parts[1]\n",
    "            values = parts[2:]\n",
    "            \n",
    "            gene_ids.append(gene_id)\n",
    "            gene_names.append(gene_name)\n",
    "            \n",
    "            # Add non-zero values to sparse matrix\n",
    "            for col_idx, val in enumerate(values):\n",
    "                try:\n",
    "                    val_float = float(val)\n",
    "                    if val_float != 0:\n",
    "                        data.append(val_float)\n",
    "                        row_indices.append(row_idx)\n",
    "                        col_indices.append(col_idx)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    \n",
    "    matrix = sparse.csr_matrix(\n",
    "        (data, (row_indices, col_indices)), \n",
    "        shape=(len(gene_names), len(cell_barcodes)),\n",
    "        dtype=np.float32\n",
    "    )\n",
    "    \n",
    "    var_df = pd.DataFrame({\n",
    "        'gene_id': gene_ids,\n",
    "        'gene_name': gene_names\n",
    "    }, index=gene_names)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"  Processed {file_path} in {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    return matrix, var_df, cell_barcodes\n",
    "\n",
    "def parse_cell_metadata(cell_barcode: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Parse metadata from a cell barcode.\n",
    "    Format: BARCODE-14.HTOX.SAMPLEID.TISSUE.CELLTYPE.CONDITION\n",
    "    \"\"\"\n",
    "    parts = cell_barcode.split('.')\n",
    "    if len(parts) >= 5:\n",
    "        barcode = parts[0]\n",
    "        hto = parts[1] if len(parts) > 1 else \"\"\n",
    "        sample_id = parts[2] if len(parts) > 2 else \"\"\n",
    "        tissue = parts[3] if len(parts) > 3 else \"\"\n",
    "        cell_type = parts[4] if len(parts) > 4 else \"\"\n",
    "        condition = parts[5] if len(parts) > 5 else \"unstimulated\"\n",
    "    else:\n",
    "        return {\n",
    "            \"barcode\": cell_barcode,\n",
    "            \"hto\": \"\",\n",
    "            \"sample_id\": \"\",\n",
    "            \"tissue\": \"\",\n",
    "            \"cell_type\": \"\",\n",
    "            \"condition\": \"unstimulated\"\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        \"barcode\": barcode,\n",
    "        \"hto\": hto,\n",
    "        \"sample_id\": sample_id,\n",
    "        \"tissue\": tissue,\n",
    "        \"cell_type\": cell_type,\n",
    "        \"condition\": condition\n",
    "    }\n",
    "\n",
    "def create_harmonized_metadata(cell_barcodes: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create harmonized metadata from cell barcodes.\n",
    "    \"\"\"\n",
    "    metadata_list = []\n",
    "    \n",
    "    for barcode in cell_barcodes:\n",
    "        metadata = parse_cell_metadata(barcode)\n",
    "        organism = \"Homo sapiens\"  # All samples in this dataset are human\n",
    "        cell_type = metadata[\"cell_type\"]\n",
    "        condition = metadata[\"condition\"]\n",
    "        tissue = metadata[\"tissue\"]\n",
    "        crispr_type = \"None\"         # No CRISPR data in this dataset\n",
    "        cancer_type = \"Non-Cancer\"     # No cancer data in this dataset\n",
    "        perturbation_name = \"None\"     # No perturbation data in this dataset\n",
    "        \n",
    "        if condition in [\"sars\", \"flu\"]:\n",
    "            perturbation_name = \"SARS-CoV-2\" if condition == \"sars\" else \"Influenza\"\n",
    "        \n",
    "        harmonized_metadata = {\n",
    "            \"barcode\": metadata[\"barcode\"],\n",
    "            \"original_barcode\": barcode,\n",
    "            \"hto\": metadata[\"hto\"],\n",
    "            \"sample_id\": metadata[\"sample_id\"],\n",
    "            \"tissue\": tissue,\n",
    "            \"cell_type\": cell_type,\n",
    "            \"condition\": condition,\n",
    "            \"organism\": organism,\n",
    "            \"crispr_type\": crispr_type,\n",
    "            \"cancer_type\": cancer_type,\n",
    "            \"perturbation_name\": perturbation_name\n",
    "        }\n",
    "        metadata_list.append(harmonized_metadata)\n",
    "    \n",
    "    return pd.DataFrame(metadata_list)\n",
    "\n",
    "def process_sample(sample_id: str, data_dir: str) -> Tuple[ad.AnnData, ad.AnnData]:\n",
    "    \"\"\"\n",
    "    Process a single sample and return AnnData objects for GEX and ADT data.\n",
    "    \"\"\"\n",
    "    gex_path = os.path.join(data_dir, f\"{GEO_ACCESSION}_{sample_id}.GEX.matrix.txt.gz\")\n",
    "    adt_path = os.path.join(data_dir, f\"{GEO_ACCESSION}_{sample_id}.ADT.matrix.txt.gz\")\n",
    "    \n",
    "    # Read GEX data\n",
    "    gex_matrix, gex_var, gex_barcodes = read_matrix_file(gex_path)\n",
    "    \n",
    "    # Read ADT data\n",
    "    adt_matrix, adt_var, adt_barcodes = read_matrix_file(adt_path)\n",
    "    \n",
    "    # Find common barcodes between GEX and ADT\n",
    "    common_barcodes = list(set(gex_barcodes).intersection(adt_barcodes))\n",
    "    print(f\"Sample {sample_id}: {len(common_barcodes)} common barcodes between GEX and ADT\")\n",
    "    \n",
    "    gex_indices = [gex_barcodes.index(bc) for bc in common_barcodes]\n",
    "    adt_indices = [adt_barcodes.index(bc) for bc in common_barcodes]\n",
    "    \n",
    "    gex_matrix_filtered = gex_matrix[:, gex_indices]\n",
    "    adt_matrix_filtered = adt_matrix[:, adt_indices]\n",
    "    \n",
    "    metadata = create_harmonized_metadata(common_barcodes)\n",
    "    \n",
    "    gex_adata = ad.AnnData(\n",
    "        X=gex_matrix_filtered.T,  # Transpose to have cells as rows\n",
    "        obs=pd.DataFrame(index=common_barcodes),\n",
    "        var=pd.DataFrame(index=gex_var.index)\n",
    "    )\n",
    "    \n",
    "    adt_adata = ad.AnnData(\n",
    "        X=adt_matrix_filtered.T,  # Transpose to have cells as rows\n",
    "        obs=pd.DataFrame(index=common_barcodes),\n",
    "        var=pd.DataFrame(index=adt_var.index)\n",
    "    )\n",
    "    \n",
    "    # Add harmonized metadata to obs\n",
    "    metadata_indexed = metadata.set_index('original_barcode')\n",
    "    for col in metadata_indexed.columns:\n",
    "        gex_adata.obs[col] = metadata_indexed[col]\n",
    "        adt_adata.obs[col] = metadata_indexed[col]\n",
    "    \n",
    "    # Add gene/protein information to var\n",
    "    for col in gex_var.columns:\n",
    "        gex_adata.var[col] = gex_var[col].values\n",
    "    for col in adt_var.columns:\n",
    "        adt_adata.var[col] = adt_var[col].values\n",
    "\n",
    "    # Simplify protein names in ADT var_names\n",
    "    adt_adata.var.index = [simplify_protein_name(name) for name in adt_adata.var.index]\n",
    "    \n",
    "    # Additional metadata in uns\n",
    "    gex_adata.uns[\"sample_id\"] = sample_id\n",
    "    gex_adata.uns[\"data_type\"] = \"gene_expression\"\n",
    "    gex_adata.uns[\"geo_accession\"] = GEO_ACCESSION\n",
    "    \n",
    "    adt_adata.uns[\"sample_id\"] = sample_id\n",
    "    adt_adata.uns[\"data_type\"] = \"protein_expression\"\n",
    "    adt_adata.uns[\"geo_accession\"] = GEO_ACCESSION\n",
    "    \n",
    "    # Check for duplicate gene/protein names and make unique if needed\n",
    "    if gex_adata.var_names.duplicated().any():\n",
    "        print(f\"Warning: Found {gex_adata.var_names.duplicated().sum()} duplicate gene names in GEX data\")\n",
    "        gex_adata.var_names_make_unique()\n",
    "    \n",
    "    if adt_adata.var_names.duplicated().any():\n",
    "        print(f\"Warning: Found {adt_adata.var_names.duplicated().sum()} duplicate protein names in ADT data\")\n",
    "        adt_adata.var_names_make_unique()\n",
    "    \n",
    "    return gex_adata, adt_adata\n",
    "\n",
    "def process_all_samples(data_dir: str) -> Tuple[ad.AnnData, ad.AnnData]:\n",
    "    \"\"\"\n",
    "    Process all samples and combine them into a single AnnData object.\n",
    "    \"\"\"\n",
    "    gex_adatas = []\n",
    "    adt_adatas = []\n",
    "    \n",
    "    for sample_id in SAMPLE_IDS:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            gex_adata, adt_adata = process_sample(sample_id, data_dir)\n",
    "            gex_adatas.append(gex_adata)\n",
    "            adt_adatas.append(adt_adata)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Processed sample {sample_id} in {elapsed_time:.2f} seconds\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {sample_id}: {e}\")\n",
    "    \n",
    "    print(\"Combining all samples...\")\n",
    "    combined_gex = ad.concat(gex_adatas, join=\"outer\", merge=\"same\")\n",
    "    combined_adt = ad.concat(adt_adatas, join=\"outer\", merge=\"same\")\n",
    "    \n",
    "    if combined_gex.var_names.duplicated().any():\n",
    "        print(f\"Warning: Found {combined_gex.var_names.duplicated().sum()} duplicate gene names in combined GEX data\")\n",
    "        combined_gex.var_names_make_unique()\n",
    "    \n",
    "    if combined_adt.var_names.duplicated().any():\n",
    "        print(f\"Warning: Found {combined_adt.var_names.duplicated().sum()} duplicate protein names in combined ADT data\")\n",
    "        combined_adt.var_names_make_unique()\n",
    "    \n",
    "    return combined_gex, combined_adt\n",
    "\n",
    "def main(data_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Main function to process the GSE261278 dataset.\n",
    "    \"\"\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    # Download dataset files if needed\n",
    "    download_dataset(data_dir)\n",
    "    \n",
    "    # Process all samples\n",
    "    start_time = time.time()\n",
    "    gex_adata, adt_adata = process_all_samples(data_dir)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Processed all samples in {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    # Save processed data\n",
    "    output_dir = os.path.join(data_dir, \"processed\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    gex_output_path = os.path.join(output_dir, f\"{GEO_ACCESSION}_gene_expression.h5ad\")\n",
    "    adt_output_path = os.path.join(output_dir, f\"{GEO_ACCESSION}_protein_expression.h5ad\")\n",
    "    \n",
    "    print(f\"Saving gene expression data to {gex_output_path}\")\n",
    "    gex_adata.write(gex_output_path, compression=\"gzip\")\n",
    "    \n",
    "    print(f\"Saving protein expression data to {adt_output_path}\")\n",
    "    adt_adata.write(adt_output_path, compression=\"gzip\")\n",
    "    \n",
    "    print(\"Processing complete!\")\n",
    "    print(f\"Gene expression data shape: {gex_adata.shape}\")\n",
    "    print(f\"Protein expression data shape: {adt_adata.shape}\")\n",
    "    \n",
    "    # Print metadata summary\n",
    "    print(\"\\nMetadata summary:\")\n",
    "    for field in [\"organism\", \"cell_type\", \"condition\", \"perturbation_name\", \"tissue\"]:\n",
    "        unique_values = gex_adata.obs[field].unique()\n",
    "        preview = unique_values[:5]\n",
    "        print(f\"{field}: {preview}{'...' if len(unique_values) > 5 else ''}\")\n",
    "\n",
    "# For Jupyter Notebook, set the data directory explicitly\n",
    "data_dir = os.path.join(os.getcwd(), GEO_ACCESSION)\n",
    "main(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a838de-845c-4b24-ad68-772ecf4637f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
