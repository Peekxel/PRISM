{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b61713b-4c06-4222-8853-da54ecde594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import shutil\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "from scipy import sparse\n",
    "from scipy.io import mmread\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re  # Needed for regex in simplify_protein_name\n",
    "\n",
    "# Constants\n",
    "GEO_ACCESSION = \"GSE269140\"\n",
    "BASE_URL = f\"https://ftp.ncbi.nlm.nih.gov/geo/series/GSE269nnn/{GEO_ACCESSION}/suppl\"\n",
    "FILE_PATTERNS = [\n",
    "    f\"{GEO_ACCESSION}_feature_reference.csv.gz\",\n",
    "    f\"{GEO_ACCESSION}_Pool1-lane1_barcodes.tsv.gz\",\n",
    "    f\"{GEO_ACCESSION}_Pool1-lane1_features.tsv.gz\",\n",
    "    f\"{GEO_ACCESSION}_Pool1-lane1_matrix.mtx.gz\",\n",
    "    f\"{GEO_ACCESSION}_Pool1-lane2_barcodes.tsv.gz\",\n",
    "    f\"{GEO_ACCESSION}_Pool1-lane2_features.tsv.gz\",\n",
    "    f\"{GEO_ACCESSION}_Pool1-lane2_matrix.mtx.gz\",\n",
    "    f\"{GEO_ACCESSION}_Pool1-lane3_barcodes.tsv.gz\",\n",
    "    f\"{GEO_ACCESSION}_Pool1-lane3_features.tsv.gz\",\n",
    "    f\"{GEO_ACCESSION}_Pool1-lane3_matrix.mtx.gz\",\n",
    "    f\"{GEO_ACCESSION}_Pool1-lane4_barcodes.tsv.gz\",\n",
    "    f\"{GEO_ACCESSION}_Pool1-lane4_features.tsv.gz\",\n",
    "    f\"{GEO_ACCESSION}_Pool1-lane4_matrix.mtx.gz\",\n",
    "    f\"{GEO_ACCESSION}_Pool1-lane5_barcodes.tsv.gz\",\n",
    "    f\"{GEO_ACCESSION}_Pool1-lane5_features.tsv.gz\",\n",
    "    f\"{GEO_ACCESSION}_Pool1-lane5_matrix.mtx.gz\",\n",
    "    f\"{GEO_ACCESSION}_Pool2-lane1_barcodes.tsv.gz\",\n",
    "    f\"{GEO_ACCESSION}_Pool2-lane1_features.tsv.gz\",\n",
    "    f\"{GEO_ACCESSION}_Pool2-lane1_matrix.mtx.gz\",\n",
    "    f\"{GEO_ACCESSION}_Pool2-lane2_barcodes.tsv.gz\",\n",
    "    f\"{GEO_ACCESSION}_Pool2-lane2_features.tsv.gz\",\n",
    "    f\"{GEO_ACCESSION}_Pool2-lane2_matrix.mtx.gz\",\n",
    "    f\"{GEO_ACCESSION}_Pool2-lane3_barcodes.tsv.gz\",\n",
    "    f\"{GEO_ACCESSION}_Pool2-lane3_features.tsv.gz\",\n",
    "    f\"{GEO_ACCESSION}_Pool2-lane3_matrix.mtx.gz\",\n",
    "    f\"{GEO_ACCESSION}_Pool2-lane4_barcodes.tsv.gz\",\n",
    "    f\"{GEO_ACCESSION}_Pool2-lane4_features.tsv.gz\",\n",
    "    f\"{GEO_ACCESSION}_Pool2-lane4_matrix.mtx.gz\",\n",
    "]\n",
    "\n",
    "def simplify_protein_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove 'TotalSeqCXXXX_' and 'anti-human'/'anti-mouse', etc. prefixes\n",
    "    and keep only the main marker name (e.g. 'CD19', 'CD4', etc.).\n",
    "    \"\"\"\n",
    "    # 1) Remove 'TotalSeqC' plus digits + underscore\n",
    "    name = re.sub(r'^TotalSeqC\\d+_+', '', name)\n",
    "    \n",
    "    # 2) Remove leading 'anti-' text (including 'anti-human', 'anti-mouse/human', etc.)\n",
    "    #    followed by optional underscores or spaces.\n",
    "    name = re.sub(r'^anti-[^_ ]+[_ ]*', '', name)\n",
    "    \n",
    "    # 3) Clean up repeated underscores/spaces\n",
    "    name = name.strip().replace('_', ' ')\n",
    "    \n",
    "    return name.strip()\n",
    "\n",
    "def download_file(url, output_path, max_retries=3):\n",
    "    \"\"\"\n",
    "    Download a file from a URL with retry logic\n",
    "    \n",
    "    Args:\n",
    "        url: URL to download from\n",
    "        output_path: Path to save the file\n",
    "        max_retries: Maximum number of retry attempts\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if download was successful, False otherwise\n",
    "    \"\"\"\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"File already exists: {output_path}\")\n",
    "        return True\n",
    "    \n",
    "    print(f\"Downloading {url} to {output_path}\")\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            response = requests.get(url, stream=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            block_size = 1024  # 1 KiB\n",
    "            \n",
    "            with open(output_path, 'wb') as f, tqdm(\n",
    "                desc=f\"Attempt {attempt}/{max_retries}\",\n",
    "                total=total_size,\n",
    "                unit='B',\n",
    "                unit_scale=True,\n",
    "                unit_divisor=1024,\n",
    "            ) as bar:\n",
    "                for data in response.iter_content(block_size):\n",
    "                    bar.update(len(data))\n",
    "                    f.write(data)\n",
    "            \n",
    "            return True\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading file (attempt {attempt}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries:\n",
    "                print(f\"Retrying in 5 seconds...\")\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                print(f\"Failed to download {url} after {max_retries} attempts\")\n",
    "                return False\n",
    "\n",
    "def download_dataset(data_dir):\n",
    "    \"\"\"\n",
    "    Download all files for the dataset\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory to save the files\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of downloaded files by pool and lane\n",
    "    \"\"\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    data_files = {}\n",
    "    \n",
    "    for file_pattern in FILE_PATTERNS:\n",
    "        url = f\"{BASE_URL}/{file_pattern}\"\n",
    "        output_path = os.path.join(data_dir, file_pattern)\n",
    "        \n",
    "        success = download_file(url, output_path)\n",
    "        \n",
    "        # Organize files by pool and lane\n",
    "        if success and \"Pool\" in file_pattern:\n",
    "            parts = file_pattern.split(\"_\")\n",
    "            pool_lane = parts[1]  # e.g., \"Pool1-lane1\"\n",
    "            file_type = parts[2].split(\".\")[0]  # e.g., \"barcodes\", \"features\", \"matrix\"\n",
    "            \n",
    "            if pool_lane not in data_files:\n",
    "                data_files[pool_lane] = {}\n",
    "            \n",
    "            data_files[pool_lane][file_type] = output_path\n",
    "    \n",
    "    return data_files\n",
    "\n",
    "def get_existing_files(data_dir):\n",
    "    \"\"\"\n",
    "    Get existing files in the data directory\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing the files\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of existing files by pool and lane\n",
    "    \"\"\"\n",
    "    data_files = {}\n",
    "    \n",
    "    for file_pattern in FILE_PATTERNS:\n",
    "        file_path = os.path.join(data_dir, file_pattern)\n",
    "        \n",
    "        if os.path.exists(file_path) and \"Pool\" in file_pattern:\n",
    "            parts = file_pattern.split(\"_\")\n",
    "            pool_lane = parts[1]  # e.g., \"Pool1-lane1\"\n",
    "            file_type = parts[2].split(\".\")[0]  # e.g., \"barcodes\", \"features\", \"matrix\"\n",
    "            \n",
    "            if pool_lane not in data_files:\n",
    "                data_files[pool_lane] = {}\n",
    "            \n",
    "            data_files[pool_lane][file_type] = file_path\n",
    "    \n",
    "    return data_files\n",
    "\n",
    "def read_10x_mtx(matrix_file, features_file, barcodes_file):\n",
    "    \"\"\"\n",
    "    Read 10X Genomics matrix format files\n",
    "    \n",
    "    Args:\n",
    "        matrix_file: Path to the matrix.mtx.gz file\n",
    "        features_file: Path to the features.tsv.gz file\n",
    "        barcodes_file: Path to the barcodes.tsv.gz file\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (matrix, features_df, barcodes)\n",
    "    \"\"\"\n",
    "    # Check if all files exist\n",
    "    for file_path in [matrix_file, features_file, barcodes_file]:\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    # Read matrix\n",
    "    try:\n",
    "        with gzip.open(matrix_file, 'rb') as f:\n",
    "            X = mmread(f).T.tocsr()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading matrix file {matrix_file}: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Read features\n",
    "    feature_ids = []\n",
    "    feature_names = []\n",
    "    feature_types = []\n",
    "    \n",
    "    with gzip.open(features_file, 'rt') as f:\n",
    "        for line in f:\n",
    "            fields = line.strip().split('\\t')\n",
    "            feature_ids.append(fields[0])\n",
    "            feature_names.append(fields[1])\n",
    "            if len(fields) > 2:\n",
    "                feature_types.append(fields[2])\n",
    "            else:\n",
    "                feature_types.append(\"Gene Expression\")\n",
    "    \n",
    "    features_df = pd.DataFrame({\n",
    "        'gene_ids': feature_ids,\n",
    "        'feature_names': feature_names,\n",
    "        'feature_types': feature_types\n",
    "    })\n",
    "    \n",
    "    # Read barcodes\n",
    "    barcodes = []\n",
    "    with gzip.open(barcodes_file, 'rt') as f:\n",
    "        for line in f:\n",
    "            barcodes.append(line.strip())\n",
    "    \n",
    "    # Validate dimensions\n",
    "    if X.shape[0] != len(barcodes) or X.shape[1] != len(features_df):\n",
    "        raise ValueError(\n",
    "            f\"Dimension mismatch: Matrix shape {X.shape}, \"\n",
    "            f\"barcodes length {len(barcodes)}, \"\n",
    "            f\"features length {len(features_df)}\"\n",
    "        )\n",
    "    \n",
    "    return X, features_df, barcodes\n",
    "\n",
    "def process_lane(pool_lane, files, data_dir):\n",
    "    \"\"\"\n",
    "    Process a single lane of data\n",
    "    \n",
    "    Args:\n",
    "        pool_lane: Pool and lane identifier (e.g., \"Pool1-lane1\")\n",
    "        files: Dictionary of files for this lane\n",
    "        data_dir: Data directory\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (gene_adata, protein_adata) AnnData objects for gene and protein expression\n",
    "    \"\"\"\n",
    "    print(f\"Processing {pool_lane}\")\n",
    "    \n",
    "    try:\n",
    "        # Check if all required files are present\n",
    "        required_files = ['barcodes', 'features', 'matrix']\n",
    "        for req_file in required_files:\n",
    "            if req_file not in files:\n",
    "                raise ValueError(f\"Missing {req_file} file for {pool_lane}\")\n",
    "        \n",
    "        # Read the 10X matrix files\n",
    "        X, features_df, barcodes = read_10x_mtx(\n",
    "            files['matrix'], \n",
    "            files['features'], \n",
    "            files['barcodes']\n",
    "        )\n",
    "        \n",
    "        # Split gene expression and protein data\n",
    "        gene_mask = features_df['feature_types'] == 'Gene Expression'\n",
    "        protein_mask = features_df['feature_types'] == 'Antibody Capture'\n",
    "        \n",
    "        # Create AnnData objects\n",
    "        gene_adata = ad.AnnData(\n",
    "            X=X[:, gene_mask],\n",
    "            obs=pd.DataFrame(index=barcodes),\n",
    "            var=features_df[gene_mask].reset_index(drop=True)\n",
    "        )\n",
    "        \n",
    "        protein_adata = ad.AnnData(\n",
    "            X=X[:, protein_mask],\n",
    "            obs=pd.DataFrame(index=barcodes),\n",
    "            var=features_df[protein_mask].reset_index(drop=True)\n",
    "        )\n",
    "        \n",
    "        # Add metadata\n",
    "        pool = pool_lane.split('-')[0].replace('Pool', '')\n",
    "        lane = pool_lane.split('-')[1].replace('lane', '')\n",
    "        \n",
    "        # Add batch information\n",
    "        gene_adata.obs['batch'] = f\"{pool_lane.lower()}\"\n",
    "        protein_adata.obs['batch'] = f\"{pool_lane.lower()}\"\n",
    "        \n",
    "        # Add pool and lane information\n",
    "        gene_adata.obs['pool'] = pool\n",
    "        gene_adata.obs['lane'] = lane\n",
    "        protein_adata.obs['pool'] = pool\n",
    "        protein_adata.obs['lane'] = lane\n",
    "        \n",
    "        # Add harmonization metadata\n",
    "        for adata in [gene_adata, protein_adata]:\n",
    "            adata.obs['organism'] = 'Homo sapiens'\n",
    "            adata.obs['cell_type'] = 'T Cells'\n",
    "            adata.obs['cell_subtype'] = 'CD8+ T cells'\n",
    "            adata.obs['crispr_type'] = 'None'\n",
    "            adata.obs['cancer_type'] = 'Non-Cancer'\n",
    "            adata.obs['condition'] = 'SStim'\n",
    "            adata.obs['perturbation_name'] = 'ROR1-CAR'\n",
    "            adata.obs['geo_accession'] = GEO_ACCESSION\n",
    "            adata.obs['study_title'] = (\n",
    "                'Engineering potent chimeric antigen receptor T cells by programming '\n",
    "                'signaling during T-cell activation [CITE-seq]'\n",
    "            )\n",
    "        \n",
    "        # Set var_names to gene symbols for gene expression data\n",
    "        gene_adata.var_names = gene_adata.var['feature_names'].values\n",
    "        gene_adata.var.drop(columns=['feature_names'], inplace=True)\n",
    "        \n",
    "        # Set var_names to protein names for protein data\n",
    "        protein_adata.var_names = protein_adata.var['feature_names'].values\n",
    "        protein_adata.var.drop(columns=['feature_names'], inplace=True)\n",
    "        \n",
    "        # ---- NEW: Simplify the protein var_names by removing TotalSeqC/anti-human prefixes\n",
    "        simplified_protein_names = [simplify_protein_name(n) for n in protein_adata.var_names]\n",
    "        protein_adata.var_names = simplified_protein_names\n",
    "        \n",
    "        # IMPORTANT FIX: Make sure var names are unique right now\n",
    "        gene_adata.var_names_make_unique()\n",
    "        protein_adata.var_names_make_unique()\n",
    "        \n",
    "        return gene_adata, protein_adata\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pool_lane}: {e}\")\n",
    "        print(\"Skipping this lane and continuing with others.\")\n",
    "        return None, None\n",
    "\n",
    "def merge_lanes(data_files, data_dir):\n",
    "    \"\"\"\n",
    "    Merge data from multiple lanes\n",
    "    \n",
    "    Args:\n",
    "        data_files: Dictionary of files by pool and lane\n",
    "        data_dir: Data directory\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (gene_adata, protein_adata) Merged AnnData objects\n",
    "    \"\"\"\n",
    "    gene_adatas = []\n",
    "    protein_adatas = []\n",
    "    \n",
    "    for pool_lane, files in data_files.items():\n",
    "        gene_adata, protein_adata = process_lane(pool_lane, files, data_dir)\n",
    "        \n",
    "        if gene_adata is not None and protein_adata is not None:\n",
    "            # Add pool-lane identifier to cell barcodes to make them unique\n",
    "            gene_adata.obs_names = [f\"{bc}-{pool_lane.lower()}\" for bc in gene_adata.obs_names]\n",
    "            protein_adata.obs_names = [f\"{bc}-{pool_lane.lower()}\" for bc in protein_adata.obs_names]\n",
    "            \n",
    "            gene_adatas.append(gene_adata)\n",
    "            protein_adatas.append(protein_adata)\n",
    "    \n",
    "    if not gene_adatas or not protein_adatas:\n",
    "        raise ValueError(\"No valid data found in any pool/lane combination.\")\n",
    "    \n",
    "    print(\"Merging gene expression data...\")\n",
    "    merged_gene_adata = ad.concat(gene_adatas, join='outer')\n",
    "    \n",
    "    print(\"Merging protein expression data...\")\n",
    "    merged_protein_adata = ad.concat(protein_adatas, join='outer')\n",
    "    \n",
    "    # Make sure var_names are unique after merging\n",
    "    merged_gene_adata.var_names_make_unique()\n",
    "    merged_protein_adata.var_names_make_unique()\n",
    "    \n",
    "    # Keep only cells that have both gene expression and protein data\n",
    "    common_cells = list(set(merged_gene_adata.obs_names) & set(merged_protein_adata.obs_names))\n",
    "    print(f\"Found {len(common_cells)} cells with both gene expression and protein data.\")\n",
    "    \n",
    "    merged_gene_adata = merged_gene_adata[common_cells]\n",
    "    merged_protein_adata = merged_protein_adata[common_cells]\n",
    "    \n",
    "    return merged_gene_adata, merged_protein_adata\n",
    "\n",
    "def run_pipeline(data_dir=None):\n",
    "    \"\"\"\n",
    "    Run the full pipeline in a Jupyter-compatible manner.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory in which to download and process data.\n",
    "                  If None, uses the current working directory.\n",
    "    \"\"\"\n",
    "    if data_dir is None:\n",
    "        data_dir = os.getcwd()\n",
    "    \n",
    "    print(f\"Processing {GEO_ACCESSION} dataset\")\n",
    "    print(f\"Data directory: {data_dir}\")\n",
    "    \n",
    "    try:\n",
    "        # Download dataset\n",
    "        data_files = download_dataset(data_dir)\n",
    "        \n",
    "        # Process and merge lanes\n",
    "        gene_adata, protein_adata = merge_lanes(data_files, data_dir)\n",
    "        \n",
    "        # Save the harmonized data\n",
    "        gene_output_path = os.path.join(data_dir, f\"{GEO_ACCESSION}_gene_expression.h5ad\")\n",
    "        protein_output_path = os.path.join(data_dir, f\"{GEO_ACCESSION}_protein_expression.h5ad\")\n",
    "        \n",
    "        print(f\"Saving gene expression data to {gene_output_path}\")\n",
    "        gene_adata.write(gene_output_path)\n",
    "        \n",
    "        print(f\"Saving protein expression data to {protein_output_path}\")\n",
    "        protein_adata.write(protein_output_path)\n",
    "        \n",
    "        print(\"Processing complete!\")\n",
    "        print(f\"Gene expression data shape: {gene_adata.shape}\")\n",
    "        print(f\"Protein expression data shape: {protein_adata.shape}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing dataset: {e}\")\n",
    "        \n",
    "        # Try using existing files if the download fails\n",
    "        print(\"Attempting to process using existing files...\")\n",
    "        try:\n",
    "            existing_files = get_existing_files(data_dir)\n",
    "            gene_adata, protein_adata = merge_lanes(existing_files, data_dir)\n",
    "            \n",
    "            gene_output_path = os.path.join(data_dir, f\"{GEO_ACCESSION}_gene_expression.h5ad\")\n",
    "            protein_output_path = os.path.join(data_dir, f\"{GEO_ACCESSION}_protein_expression.h5ad\")\n",
    "            \n",
    "            print(f\"Saving gene expression data to {gene_output_path}\")\n",
    "            gene_adata.write(gene_output_path)\n",
    "            \n",
    "            print(f\"Saving protein expression data to {protein_output_path}\")\n",
    "            protein_adata.write(protein_output_path)\n",
    "            \n",
    "            print(\"Processing complete!\")\n",
    "            print(f\"Gene expression data shape: {gene_adata.shape}\")\n",
    "            print(f\"Protein expression data shape: {protein_adata.shape}\")\n",
    "        \n",
    "        except Exception as e2:\n",
    "            print(f\"Error processing existing files: {e2}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "# USAGE in Jupyter:\n",
    "# 1) Paste this entire code block in a cell\n",
    "# 2) Run the cell\n",
    "# 3) Call run_pipeline(\"/path/to/data_dir\") in another cell\n",
    "#    or just run_pipeline() to use the current directory.\n",
    "\n",
    "\n",
    "# --- USAGE EXAMPLE IN JUPYTER NOTEBOOK ---\n",
    "# In your Jupyter Notebook, after running the above cell, simply call:\n",
    "run_pipeline(\"/content/GSE269140\")\n",
    "#\n",
    "# If you prefer to use the current directory, do:\n",
    "# run_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a838de-845c-4b24-ad68-772ecf4637f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
