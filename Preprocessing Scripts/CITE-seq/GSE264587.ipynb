{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b61713b-4c06-4222-8853-da54ecde594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import anndata as ad\n",
    "from scipy import sparse\n",
    "from scipy.io import mmread\n",
    "import h5py\n",
    "import re\n",
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "import subprocess\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler('process_GSE264587.log')\n",
    "    ]\n",
    ")\n",
    "\n",
    "def get_main_protein_name(name):\n",
    "    \"\"\"\n",
    "    1. Strip known species/prefix markers: \n",
    "       - 'anti-human_'\n",
    "       - 'Mouse_'\n",
    "       - 'Rat_'\n",
    "       - 'Armenian_Hamster_'\n",
    "       - 'human_'\n",
    "       (repeat if multiple occur)\n",
    "    2. Keep only the substring up to the first underscore.\n",
    "\n",
    "    Example:\n",
    "      \"anti-human_CD274_B7-H1_PD-L1\" -> \"CD274\"\n",
    "      \"Mouse_IgG1_isotype_Ctrl\" -> \"IgG1\"\n",
    "    \"\"\"\n",
    "    result = re.sub(r'^anti-[^_]+_', '', name)\n",
    "    \n",
    "    # Remove repeated species prefixes (\"Mouse_\", \"Rat_\", etc.) if any remain\n",
    "    while True:\n",
    "        new_result = re.sub(r'^(?:[Mm]ouse|[Rr]at|Armenian_Hamster|human)(?:_[^_]+)?_', '', result)\n",
    "        if new_result == result:\n",
    "            break\n",
    "        result = new_result\n",
    "    \n",
    "    # Keep only substring before first underscore\n",
    "    if \"_\" in result:\n",
    "        result = result.split(\"_\", 1)[0]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def download_dataset(data_dir):\n",
    "    \"\"\"\n",
    "    Download the GSE264587 dataset if not already present\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dir : str\n",
    "        Directory to store the downloaded data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Path to the directory containing the extracted files\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "        \n",
    "        # Define the URL and target file\n",
    "        url = \"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE264587&format=file\"\n",
    "        tar_file = os.path.join(data_dir, \"GSE264587_RAW.tar\")\n",
    "        \n",
    "        # Approx >600MB if already complete\n",
    "        if os.path.exists(tar_file) and os.path.getsize(tar_file) > 600000000:\n",
    "            logging.info(f\"File {tar_file} already exists and appears complete. Skipping download.\")\n",
    "        else:\n",
    "            logging.info(f\"Downloading GSE264587_RAW.tar to {tar_file}...\")\n",
    "            \n",
    "            # Use subprocess to call wget\n",
    "            try:\n",
    "                cmd = [\n",
    "                    \"wget\", \n",
    "                    \"--continue\",\n",
    "                    \"--tries=5\",\n",
    "                    \"--timeout=60\",\n",
    "                    \"--waitretry=60\",\n",
    "                    \"--no-verbose\",\n",
    "                    \"-O\", tar_file,\n",
    "                    url\n",
    "                ]\n",
    "                logging.info(f\"Running command: {' '.join(cmd)}\")\n",
    "                subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "                logging.info(\"Download complete.\")\n",
    "                \n",
    "            except subprocess.CalledProcessError as e:\n",
    "                logging.error(f\"Error downloading file with wget: {e}\")\n",
    "                logging.info(\"Trying alternative download method with urllib...\")\n",
    "                \n",
    "                try:\n",
    "                    urllib.request.urlretrieve(url, tar_file)\n",
    "                    logging.info(\"Download complete with urllib.\")\n",
    "                except Exception as e2:\n",
    "                    logging.error(f\"Error downloading file with urllib: {e2}\")\n",
    "                    raise\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Unexpected error during download: {e}\")\n",
    "                raise\n",
    "        \n",
    "        # Verify\n",
    "        if not os.path.exists(tar_file):\n",
    "            logging.error(f\"Download failed: {tar_file} does not exist\")\n",
    "            raise FileNotFoundError(f\"Downloaded file {tar_file} not found\")\n",
    "        \n",
    "        size_mb = os.path.getsize(tar_file)/(1024*1024)\n",
    "        logging.info(f\"Downloaded file size: {size_mb:.2f} MB\")\n",
    "        if size_mb < 10:  # < 10 MB means likely incomplete\n",
    "            logging.error(f\"Downloaded file is too small ({size_mb} MB).\")\n",
    "            raise ValueError(\"Downloaded file is too small. Possibly incomplete.\")\n",
    "        \n",
    "        # Extract\n",
    "        extracted_dir = os.path.join(data_dir, \"extracted\")\n",
    "        if not os.path.exists(extracted_dir) or len(os.listdir(extracted_dir)) == 0:\n",
    "            os.makedirs(extracted_dir, exist_ok=True)\n",
    "            logging.info(f\"Extracting {tar_file} to {extracted_dir}...\")\n",
    "            try:\n",
    "                # Attempt system tar\n",
    "                cmd = [\"tar\", \"-xf\", tar_file, \"-C\", extracted_dir]\n",
    "                logging.info(f\"Running command: {' '.join(cmd)}\")\n",
    "                subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "                logging.info(\"Extraction complete using tar command.\")\n",
    "            except (subprocess.CalledProcessError, FileNotFoundError) as e:\n",
    "                logging.warning(f\"Extraction with tar command failed: {e}.\\n Trying Python tarfile...\")\n",
    "                with tarfile.open(tar_file, 'r') as tar:\n",
    "                    tar.extractall(path=extracted_dir)\n",
    "                logging.info(\"Extraction complete using Python tarfile.\")\n",
    "        else:\n",
    "            logging.info(f\"Files already extracted in {extracted_dir}. Skipping extraction.\")\n",
    "        \n",
    "        # Check extraction\n",
    "        if not os.path.exists(extracted_dir) or len(os.listdir(extracted_dir)) == 0:\n",
    "            logging.error(f\"No files found in {extracted_dir} after extraction.\")\n",
    "            raise FileNotFoundError(f\"No files found in {extracted_dir}\")\n",
    "        \n",
    "        extracted_files = os.listdir(extracted_dir)\n",
    "        logging.info(f\"Extracted {len(extracted_files)} files: {extracted_files[:5]}...\")\n",
    "        return extracted_dir\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to download or extract dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_sample_data(extracted_dir, sample_id):\n",
    "    \"\"\"\n",
    "    Load data for a specific sample from the extracted directory\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    extracted_dir : str\n",
    "        Directory containing the extracted files\n",
    "    sample_id : str\n",
    "        Sample ID (e.g., 'T086')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (gene_data, protein_data) - AnnData objects for gene and protein expression\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Gather sample files\n",
    "        sample_files = [f for f in os.listdir(extracted_dir) if sample_id in f]\n",
    "        if not sample_files:\n",
    "            logging.warning(f\"No files found for sample {sample_id}\")\n",
    "            return None, None\n",
    "        \n",
    "        mtx_files = [f for f in sample_files if f.endswith('matrix.mtx.gz')]\n",
    "        feature_files = [f for f in sample_files if f.endswith('features.tsv.gz')]\n",
    "        barcode_files = [f for f in sample_files if f.endswith('barcodes.tsv.gz')]\n",
    "        \n",
    "        if not (mtx_files and feature_files and barcode_files):\n",
    "            logging.warning(f\"Missing required matrix/features/barcodes files for {sample_id}\")\n",
    "            return None, None\n",
    "        \n",
    "        mtx_path = os.path.join(extracted_dir, mtx_files[0])\n",
    "        feature_path = os.path.join(extracted_dir, feature_files[0])\n",
    "        barcode_path = os.path.join(extracted_dir, barcode_files[0])\n",
    "        \n",
    "        logging.info(f\"Loading {sample_id} from {mtx_path}\")\n",
    "        \n",
    "        # Features\n",
    "        features_df = pd.read_csv(feature_path, sep='\\t', header=None, names=['id','name','feature_type'])\n",
    "        \n",
    "        # Ensure feature names are unique right away\n",
    "        features_df['name'] = features_df['name'].astype(str)\n",
    "        features_df['name'] = pd.Series(\n",
    "            f\"{name}_{i}\" if i > 0 else name\n",
    "            for name, i in zip(features_df['name'], features_df.groupby('name').cumcount())\n",
    "        )\n",
    "        \n",
    "        # Barcodes\n",
    "        barcodes = pd.read_csv(barcode_path, sep='\\t', header=None, names=['barcode'])\n",
    "        # Make unique cell barcodes by prepending sample ID\n",
    "        barcodes['barcode'] = f\"{sample_id}_\" + barcodes['barcode'].astype(str)\n",
    "        \n",
    "        # Subset features\n",
    "        gene_mask = (features_df['feature_type'] == 'Gene Expression')\n",
    "        protein_mask = (features_df['feature_type'] == 'Antibody Capture')\n",
    "        \n",
    "        gene_features = features_df[gene_mask]\n",
    "        protein_features = features_df[protein_mask]\n",
    "        \n",
    "        logging.info(f\"{sample_id}: {len(gene_features)} gene features; {len(protein_features)} protein features.\")\n",
    "        \n",
    "        # Matrix\n",
    "        from scipy.io import mmread\n",
    "        matrix = mmread(mtx_path).tocsc()  # shape = [features, cells]\n",
    "        \n",
    "        # Safety checks\n",
    "        if matrix.shape[0] != len(features_df):\n",
    "            logging.error(f\"Matrix rows ({matrix.shape[0]}) != features ({len(features_df)})\")\n",
    "            return None, None\n",
    "        if matrix.shape[1] != len(barcodes):\n",
    "            logging.error(f\"Matrix cols ({matrix.shape[1]}) != barcodes ({len(barcodes)})\")\n",
    "            return None, None\n",
    "        \n",
    "        # Split gene vs protein\n",
    "        gene_idx = gene_features.index.tolist()\n",
    "        prot_idx = protein_features.index.tolist()\n",
    "        \n",
    "        gene_matrix = matrix[gene_idx, :].T  # shape = [cells, genes]\n",
    "        prot_matrix = matrix[prot_idx, :].T  # shape = [cells, proteins]\n",
    "        \n",
    "        logging.info(f\"{sample_id} gene matrix: {gene_matrix.shape}; protein matrix: {prot_matrix.shape}\")\n",
    "        \n",
    "        # Construct AnnData\n",
    "        gene_data = ad.AnnData(\n",
    "            X=gene_matrix,\n",
    "            obs=pd.DataFrame(index=barcodes['barcode']),\n",
    "            var=pd.DataFrame(\n",
    "                data={\n",
    "                    'feature_id': gene_features['id'].values,\n",
    "                    'feature_type': gene_features['feature_type'].values\n",
    "                },\n",
    "                index=gene_features['name'].values\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        protein_data = ad.AnnData(\n",
    "            X=prot_matrix,\n",
    "            obs=pd.DataFrame(index=barcodes['barcode']),\n",
    "            var=pd.DataFrame(\n",
    "                data={\n",
    "                    'feature_id': protein_features['id'].values,\n",
    "                    'feature_type': protein_features['feature_type'].values\n",
    "                },\n",
    "                index=protein_features['name'].values\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # If a feature_reference file exists, you could parse it here\n",
    "        feature_ref = [f for f in sample_files if 'feature_reference' in f]\n",
    "        \n",
    "        # Clean protein var_names (remove prefix, etc.)\n",
    "        if protein_data.n_vars > 0:\n",
    "            original_names = protein_data.var_names.tolist()\n",
    "            stripped_names = [get_main_protein_name(n) for n in original_names]\n",
    "            protein_data.var_names = stripped_names\n",
    "        \n",
    "        # >>>>>>> MAKE OBS/VAR NAMES UNIQUE PER SAMPLE <<<<<<<\n",
    "        gene_data.var_names_make_unique()\n",
    "        gene_data.obs_names_make_unique()   # Should be unique already but just in case\n",
    "        protein_data.var_names_make_unique()\n",
    "        protein_data.obs_names_make_unique()\n",
    "        \n",
    "        logging.info(f\"Sample {sample_id} loaded: gene_data {gene_data.shape}, protein_data {protein_data.shape}\")\n",
    "        return gene_data, protein_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data for {sample_id}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def add_metadata(adata, sample_id, sample_metadata):\n",
    "    \"\"\"\n",
    "    Add standardized metadata to AnnData object\n",
    "    \"\"\"\n",
    "    if adata is None:\n",
    "        return None\n",
    "    \n",
    "    # sample ID\n",
    "    adata.obs['sample_id'] = sample_id\n",
    "    \n",
    "    # Standard fields\n",
    "    fields = {\n",
    "        'organism': 'Homo sapiens',\n",
    "        'cell_type': 'Thymus cells',\n",
    "        'crispr_type': 'None',\n",
    "        'cancer_type': 'Non-Cancer',\n",
    "        'condition': 'Normal',\n",
    "        'perturbation_name': 'None'\n",
    "    }\n",
    "    for k,v in fields.items():\n",
    "        adata.obs[k] = v\n",
    "    \n",
    "    # Add from sample_metadata\n",
    "    if 'age' in sample_metadata:\n",
    "        adata.obs['age'] = sample_metadata['age']\n",
    "    if 'sex' in sample_metadata:\n",
    "        adata.obs['sex'] = sample_metadata['sex']\n",
    "    \n",
    "    return adata\n",
    "\n",
    "def get_sample_metadata(sample_id):\n",
    "    \"\"\"\n",
    "    Return a dictionary of relevant metadata given the sample ID\n",
    "    \"\"\"\n",
    "    meta = {\n",
    "        'T086': {'sex': 'Male', 'age': '33 months'},\n",
    "        'T087': {'sex': 'Female', 'age': '4 months'},\n",
    "        'T096': {'sex': 'Male', 'age': '5 months'},\n",
    "        'T097': {'sex': 'Female', 'age': '4 months'},\n",
    "        'T098': {'sex': 'Male', 'age': '4 months'},\n",
    "        'T099': {'sex': 'Female', 'age': '4 months'},\n",
    "        'T100': {'sex': 'Male', 'age': '4 months'}\n",
    "    }\n",
    "    return meta.get(sample_id, {})\n",
    "\n",
    "def process_dataset(data_dir):\n",
    "    \"\"\"\n",
    "    Download, extract, and load the GSE264587 data\n",
    "    returning (combined_gene_data, combined_protein_data).\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    logging.info(f\"Processing GSE264587 in {data_dir}...\")\n",
    "    \n",
    "    # 1. Download/extract\n",
    "    extracted_dir = download_dataset(data_dir)\n",
    "    \n",
    "    # 2. Find sample IDs in extracted_dir\n",
    "    files = os.listdir(extracted_dir)\n",
    "    sample_pattern = r'GSM\\d+_([A-Za-z0-9]+)_'\n",
    "    sample_ids = set()\n",
    "    for f in files:\n",
    "        m = re.search(sample_pattern, f)\n",
    "        if m:\n",
    "            sample_ids.add(m.group(1))\n",
    "    \n",
    "    if not sample_ids:\n",
    "        # fallback pattern\n",
    "        alt_pattern = r'T\\d+'\n",
    "        for f in files:\n",
    "            m = re.search(alt_pattern, f)\n",
    "            if m:\n",
    "                sample_ids.add(m.group(0))\n",
    "    \n",
    "    sample_ids = sorted(sample_ids)\n",
    "    logging.info(f\"Found {len(sample_ids)} samples: {sample_ids}\")\n",
    "    \n",
    "    # 3. Load data for each sample\n",
    "    all_gene_data = []\n",
    "    all_protein_data = []\n",
    "    \n",
    "    for sid in sample_ids:\n",
    "        g_dat, p_dat = load_sample_data(extracted_dir, sid)\n",
    "        if g_dat is not None:\n",
    "            add_metadata(g_dat, sid, get_sample_metadata(sid))\n",
    "            all_gene_data.append(g_dat)\n",
    "        if p_dat is not None:\n",
    "            add_metadata(p_dat, sid, get_sample_metadata(sid))\n",
    "            all_protein_data.append(p_dat)\n",
    "    \n",
    "    # >>>>>>> BEFORE CONCAT, ENSURE EACH SAMPLE'S NAMES ARE UNIQUE <<<<<<<\n",
    "    # (We already did it in load_sample_data, but let's be extra safe.)\n",
    "    for gd in all_gene_data:\n",
    "        gd.var_names_make_unique()\n",
    "        gd.obs_names_make_unique()\n",
    "    for pd in all_protein_data:\n",
    "        pd.var_names_make_unique()\n",
    "        pd.obs_names_make_unique()\n",
    "    \n",
    "    # 4. Concatenate gene data\n",
    "    if all_gene_data:\n",
    "        combined_gene_data = ad.concat(\n",
    "            all_gene_data, \n",
    "            join='outer',  # union of gene sets\n",
    "            merge='same',  # only merge obs/var fields with same name\n",
    "            label='sample_id'\n",
    "        )\n",
    "        logging.info(f\"Combined gene data: {combined_gene_data.shape}\")\n",
    "        \n",
    "        # check duplicates after concat\n",
    "        if combined_gene_data.var_names.duplicated().any():\n",
    "            logging.warning(\"Duplicate gene var_names found – making them unique.\")\n",
    "            combined_gene_data.var_names_make_unique()\n",
    "    else:\n",
    "        combined_gene_data = None\n",
    "        logging.warning(\"No gene data found.\")\n",
    "    \n",
    "    # 5. Concatenate protein data\n",
    "    if all_protein_data:\n",
    "        combined_protein_data = ad.concat(\n",
    "            all_protein_data,\n",
    "            join='outer',\n",
    "            merge='same',\n",
    "            label='sample_id'\n",
    "        )\n",
    "        logging.info(f\"Combined protein data: {combined_protein_data.shape}\")\n",
    "        \n",
    "        # check duplicates\n",
    "        if combined_protein_data.var_names.duplicated().any():\n",
    "            logging.warning(\"Duplicate protein var_names found – making them unique.\")\n",
    "            combined_protein_data.var_names_make_unique()\n",
    "    else:\n",
    "        combined_protein_data = None\n",
    "        logging.warning(\"No protein data found.\")\n",
    "    \n",
    "    # 6. Filter to cells present in both gene and protein data (if both exist)\n",
    "    if combined_gene_data is not None and combined_protein_data is not None:\n",
    "        common_barcodes = list(set(combined_gene_data.obs_names).intersection(combined_protein_data.obs_names))\n",
    "        logging.info(f\"{len(common_barcodes)} cells present in BOTH gene/protein data.\")\n",
    "        combined_gene_data = combined_gene_data[common_barcodes].copy()\n",
    "        combined_protein_data = combined_protein_data[common_barcodes].copy()\n",
    "    \n",
    "    # 7. Save results\n",
    "    if combined_gene_data is not None:\n",
    "        gene_out = os.path.join(data_dir, \"GSE264587_gene_expression.h5ad\")\n",
    "        combined_gene_data.write_h5ad(gene_out, compression=\"gzip\")\n",
    "        logging.info(f\"Saved gene data: {gene_out}\")\n",
    "    \n",
    "    if combined_protein_data is not None:\n",
    "        protein_out = os.path.join(data_dir, \"GSE264587_protein_expression.h5ad\")\n",
    "        combined_protein_data.write_h5ad(protein_out, compression=\"gzip\")\n",
    "        logging.info(f\"Saved protein data: {protein_out}\")\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    logging.info(f\"Done. Elapsed time = {elapsed:.1f} s\")\n",
    "    return combined_gene_data, combined_protein_data\n",
    "\n",
    "# ---------------------------\n",
    "# Jupyter main cell\n",
    "# ---------------------------\n",
    "data_dir = \"GSE264587_\"  # Adjust path if needed\n",
    "gene_data, protein_data = process_dataset(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a838de-845c-4b24-ad68-772ecf4637f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
