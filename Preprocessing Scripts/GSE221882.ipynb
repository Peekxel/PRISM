{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a30c09d-2469-48af-96d5-7c0cae4d0517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import urllib.request\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Try to import anndata, install if not available\n",
    "try:\n",
    "    import anndata as ad\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    print(\"Installing anndata package...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"anndata\"])\n",
    "    import anndata as ad\n",
    "\n",
    "# Constants\n",
    "DATASET_ID = \"GSE221882\"\n",
    "DATASET_URL = f\"https://ftp.ncbi.nlm.nih.gov/geo/series/GSE221nnn/{DATASET_ID}/suppl/{DATASET_ID}_RAW.tar\"\n",
    "FILE_PATTERNS = [\n",
    "    \"GSM6908651_Clay_ALI_w10_filtered_feature_bc_matrix.h5\",\n",
    "    \"GSM6908652_Arch_correction_ALI_w10_filtered_feature_bc_matrix.h5\"\n",
    "]\n",
    "\n",
    "def download_dataset(data_dir):\n",
    "    \"\"\"Download the dataset if not already present.\"\"\"\n",
    "    tar_path = os.path.join(data_dir, f\"{DATASET_ID}_RAW.tar\")\n",
    "    \n",
    "    # Check if the tar file already exists\n",
    "    if not os.path.exists(tar_path):\n",
    "        print(f\"Downloading {DATASET_ID} dataset...\")\n",
    "        urllib.request.urlretrieve(DATASET_URL, tar_path)\n",
    "        print(\"Download complete.\")\n",
    "    else:\n",
    "        print(f\"Found existing tar file: {tar_path}\")\n",
    "    \n",
    "    # Extract the tar file if the h5 files don't exist\n",
    "    files_exist = all(os.path.exists(os.path.join(data_dir, pattern)) for pattern in FILE_PATTERNS)\n",
    "    if not files_exist:\n",
    "        print(\"Extracting tar file...\")\n",
    "        with tarfile.open(tar_path, 'r') as tar:\n",
    "            tar.extractall(path=data_dir)\n",
    "        print(\"Extraction complete.\")\n",
    "    else:\n",
    "        print(\"All required files already extracted.\")\n",
    "    \n",
    "    # Verify files exist\n",
    "    missing_files = [pattern for pattern in FILE_PATTERNS \n",
    "                     if not os.path.exists(os.path.join(data_dir, pattern))]\n",
    "    if missing_files:\n",
    "        raise FileNotFoundError(f\"Missing files after extraction: {missing_files}\")\n",
    "    \n",
    "    return [os.path.join(data_dir, pattern) for pattern in FILE_PATTERNS]\n",
    "\n",
    "def load_10x_h5(file_path):\n",
    "    \"\"\"Load a 10X h5 file into an AnnData object.\"\"\"\n",
    "    print(f\"Loading {os.path.basename(file_path)}...\")\n",
    "    \n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        # Get the matrix data\n",
    "        data = f['matrix/data'][:]\n",
    "        indices = f['matrix/indices'][:]\n",
    "        indptr = f['matrix/indptr'][:]\n",
    "        shape = f['matrix/shape'][:]\n",
    "        \n",
    "        # Create a sparse matrix (CSC format)\n",
    "        matrix = sparse.csc_matrix((data, indices, indptr), shape=shape)\n",
    "        \n",
    "        # Get barcodes (cell IDs)\n",
    "        barcodes = [b.decode('utf-8') for b in f['matrix/barcodes'][:]]\n",
    "        \n",
    "        # Get features (genes)\n",
    "        gene_ids = [g.decode('utf-8') for g in f['matrix/features/id'][:]]\n",
    "        gene_names = [g.decode('utf-8') for g in f['matrix/features/name'][:]]\n",
    "        feature_types = [g.decode('utf-8') for g in f['matrix/features/feature_type'][:]]\n",
    "        \n",
    "        # Check for duplicate gene names and make them unique\n",
    "        gene_name_counts = {}\n",
    "        unique_gene_names = []\n",
    "        \n",
    "        for name in gene_names:\n",
    "            if name in gene_name_counts:\n",
    "                gene_name_counts[name] += 1\n",
    "                unique_gene_names.append(f\"{name}_{gene_name_counts[name]}\")\n",
    "            else:\n",
    "                gene_name_counts[name] = 0\n",
    "                unique_gene_names.append(name)\n",
    "        \n",
    "        # Create AnnData object (transpose matrix to cells x genes)\n",
    "        obs = pd.DataFrame(index=barcodes)\n",
    "        var = pd.DataFrame(index=unique_gene_names)\n",
    "        var['gene_ids'] = gene_ids\n",
    "        var['original_gene_names'] = gene_names\n",
    "        var['feature_types'] = feature_types\n",
    "        \n",
    "        # Create AnnData object\n",
    "        adata = ad.AnnData(X=matrix.transpose(), obs=obs, var=var)\n",
    "        \n",
    "        # Add file name as a metadata field\n",
    "        adata.uns['source_file'] = os.path.basename(file_path)\n",
    "        \n",
    "        print(f\"  Loaded {adata.n_obs} cells and {adata.n_vars} genes.\")\n",
    "        return adata\n",
    "\n",
    "def add_metadata(adata, file_name):\n",
    "    \"\"\"Add standardized metadata to the AnnData object.\"\"\"\n",
    "    # Extract sample information from filename\n",
    "    sample_name = os.path.basename(file_name).split('_filtered')[0]\n",
    "    \n",
    "    # Determine condition based on sample name\n",
    "    if 'Clay' in sample_name:\n",
    "        condition = 'Control'\n",
    "        perturbation_name = 'Non-targeting'\n",
    "        crispr_type = 'None'\n",
    "    elif 'Arch_correction' in sample_name:\n",
    "        condition = 'Test'\n",
    "        perturbation_name = 'CTNNB1'\n",
    "        crispr_type = 'CRISPR_correction'\n",
    "    else:\n",
    "        condition = 'Unknown'\n",
    "        perturbation_name = 'Unknown'\n",
    "        crispr_type = 'Unknown'\n",
    "    \n",
    "    # Add standardized metadata\n",
    "    adata.obs['sample'] = sample_name\n",
    "    adata.obs['organism'] = 'Homo sapiens'\n",
    "    adata.obs['cell_type'] = 'iPSC-derived cortical organoid'\n",
    "    adata.obs['condition'] = condition\n",
    "    adata.obs['perturbation_name'] = perturbation_name\n",
    "    adata.obs['crispr_type'] = crispr_type\n",
    "    adata.obs['cancer_type'] = 'Non-Cancer'\n",
    "    \n",
    "    return adata\n",
    "\n",
    "def harmonize_dataset(file_paths):\n",
    "    \"\"\"Load and harmonize the dataset.\"\"\"\n",
    "    # Process each file separately and save to individual temporary h5ad files\n",
    "    processed_files = []\n",
    "    \n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        temp_file = f\"temp_processed_{i}.h5ad\"\n",
    "        if not os.path.exists(temp_file):\n",
    "            print(f\"Processing {os.path.basename(file_path)}...\")\n",
    "            adata = load_10x_h5(file_path)\n",
    "            adata = add_metadata(adata, file_path)\n",
    "            \n",
    "            # Add batch information\n",
    "            adata.obs['batch'] = f'batch{i}'\n",
    "            adata.obs['source_file'] = os.path.basename(file_path)\n",
    "            \n",
    "            # Save to temporary file\n",
    "            adata.write_h5ad(temp_file)\n",
    "            print(f\"  Saved to {temp_file}\")\n",
    "        else:\n",
    "            print(f\"Found existing processed file: {temp_file}\")\n",
    "        \n",
    "        processed_files.append(temp_file)\n",
    "    \n",
    "    # Combine the processed files\n",
    "    print(\"Combining datasets...\")\n",
    "    \n",
    "    combined = ad.read_h5ad(processed_files[0])\n",
    "    print(f\"  Loaded first dataset: {combined.n_obs} cells, {combined.n_vars} genes\")\n",
    "    \n",
    "    for i in range(1, len(processed_files)):\n",
    "        print(f\"  Adding dataset {i+1}...\")\n",
    "        adata = ad.read_h5ad(processed_files[i])\n",
    "        print(f\"    Dataset {i+1}: {adata.n_obs} cells, {adata.n_vars} genes\")\n",
    "        \n",
    "        # Check if gene sets are the same; if not, find common genes\n",
    "        if not all(combined.var_names == adata.var_names):\n",
    "            print(\"    Gene sets differ. Finding common genes...\")\n",
    "            common_genes = set(combined.var_names).intersection(set(adata.var_names))\n",
    "            print(f\"    Found {len(common_genes)} common genes.\")\n",
    "            \n",
    "            combined = combined[:, list(common_genes)].copy()\n",
    "            adata = adata[:, list(common_genes)].copy()\n",
    "        \n",
    "        combined = ad.concat([combined, adata], join='outer', merge='same')\n",
    "        print(f\"    Combined dataset now: {combined.n_obs} cells, {combined.n_vars} genes\")\n",
    "    \n",
    "    # Ensure var_names are gene symbols\n",
    "    if 'gene_ids' in combined.var:\n",
    "        print(\"Ensuring var_names are based on gene symbols...\")\n",
    "        if combined.var_names[0] in combined.var['gene_ids'].values:\n",
    "            gene_id_to_symbol = {}\n",
    "            for file_path in file_paths:\n",
    "                with h5py.File(file_path, 'r') as f:\n",
    "                    gene_ids = [g.decode('utf-8') for g in f['matrix/features/id'][:]]\n",
    "                    gene_names = [g.decode('utf-8') for g in f['matrix/features/name'][:]]\n",
    "                    for gene_id, gene_name in zip(gene_ids, gene_names):\n",
    "                        gene_id_to_symbol[gene_id] = gene_name\n",
    "            \n",
    "            new_var = combined.var.copy()\n",
    "            new_var.index = [gene_id_to_symbol.get(gene_id, gene_id) \n",
    "                             for gene_id in combined.var_names]\n",
    "            \n",
    "            combined = ad.AnnData(X=combined.X, obs=combined.obs, var=new_var)\n",
    "    \n",
    "    print(f\"Final combined dataset: {combined.n_obs} cells, {combined.n_vars} genes\")\n",
    "    \n",
    "    # Clean up temporary files\n",
    "    for file in processed_files:\n",
    "        if os.path.exists(file):\n",
    "            os.remove(file)\n",
    "    \n",
    "    return combined\n",
    "\n",
    "# In Jupyter, we'll simply set data_dir to the current working directory\n",
    "data_dir = os.getcwd()\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Download and extract the dataset (if needed)\n",
    "file_paths = download_dataset(data_dir)\n",
    "\n",
    "# Harmonize the dataset\n",
    "combined = harmonize_dataset(file_paths)\n",
    "\n",
    "# Save the harmonized dataset\n",
    "output_file = os.path.join(data_dir, f\"{DATASET_ID}_harmonized.h5ad\")\n",
    "print(f\"Saving harmonized dataset to {output_file}...\")\n",
    "combined.write_h5ad(output_file)\n",
    "print(\"Processing complete.\")\n",
    "\n",
    "# Print summary of the harmonized dataset\n",
    "print(\"\\nHarmonized Dataset Summary:\")\n",
    "print(f\"  Number of cells: {combined.n_obs}\")\n",
    "print(f\"  Number of genes: {combined.n_vars}\")\n",
    "print(\"  Metadata fields:\")\n",
    "for field in combined.obs.columns:\n",
    "    print(f\"    - {field}\")\n",
    "\n",
    "print(\"\\nUnique values in categorical fields:\")\n",
    "for field in ['organism', 'cell_type', 'crispr_type', 'cancer_type', 'condition', 'perturbation_name']:\n",
    "    if field in combined.obs.columns:\n",
    "        unique_values = combined.obs[field].unique()\n",
    "        print(f\"    - {field}: {unique_values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b8e670-7c91-4cb5-8aac-00102b8176b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "703ca59a-be42-409e-b477-a3a0fbea9dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3b38bd-1bdb-4f0a-801c-082ce9d71f87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
